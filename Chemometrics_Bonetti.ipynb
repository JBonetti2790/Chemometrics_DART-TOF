{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“package ‘dplyr’ was built under R version 4.0.2”\n",
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Warning message:\n",
      "“package ‘ggplot2’ was built under R version 4.0.2”\n",
      "Warning message in (function (kind = NULL, normal.kind = NULL, sample.kind = NULL) :\n",
      "“non-uniform 'Rounding' sampler used”\n",
      "Warning message:\n",
      "“package ‘ggpubr’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘randomForest’ was built under R version 4.0.2”\n",
      "randomForest 4.6-14\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "Warning message:\n",
      "“package ‘ggrepel’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘gridExtra’ was built under R version 4.0.2”\n",
      "\n",
      "Attaching package: ‘gridExtra’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:randomForest’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘MASS’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    select\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(dplyr) #for arrange/sort functions\n",
    "library(stringr) #string manipulation functions\n",
    "library(xlsx) #for writing to excel workbooks\n",
    "library(ggplot2) #plotting package\n",
    "library(ggpubr) #for arranging ggplots\n",
    "library(randomForest) #for performing randomforest \n",
    "library(ggrepel) #text and label functions for ggplot2\n",
    "library(gridExtra) #for arranging plots in grid format\n",
    "library(grid) #for creating and drawing rectangles around tables in ggplot2\n",
    "library(gtable) #other table functions for ggplot2\n",
    "library(MASS) #for lda function\n",
    "library(reshape2) #for correlation heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload appropriate Perl converted data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomer_all <- read.csv(\"~/Desktop/FA_all.csv\", header = TRUE) #read in perl converted data file \n",
    "old_data <- read.csv(\"~/Desktop/FMA_olddata.csv\", header = TRUE) #old FMA data\n",
    "old_old_data <- read.csv(\"~/Desktop/FMA_oldold.csv\", header = TRUE) #old old FMA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "type <- \"FA\"                           #isomer type\n",
    "normal = \"ioncurrent\"                 #type of normalization, will change for replicate t-tests\n",
    "ion_threshold <- 10                     #threshold in percent, 0.3/10 for FA/FMA, 1/10 for MMC\n",
    "max_threshold <- 50                     #Number of spectra which must reach percent threshold\n",
    "mean_threshold <- 120                   #mean threshold per voltage\n",
    "replace_zero <- 120                     #what to replace zero abundance with once final ions are selectd\n",
    "rep_num <- 20                           #for t-test analysis\n",
    "weeks <- 8                              #number of weeks\n",
    "card <- 32                              #number of cards in dataset (assumes triplicate data)\n",
    "card.names <- paste0(isomer_all[,2], \"-\", substr(isomer_all[,3],1,nchar(isomer_all[,3])-1)) #combines volume with Card # for Card Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select which analysis/analyses to perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_excel_data = FALSE #replace saved excel dataset\n",
    "sameday = FALSE      #Analyze data generated on the same day\n",
    "ttest = FALSE        #perform replicate unequal variance t-tests/create ROC curves\n",
    "anova = TRUE            #perform multi-way ANOVA\n",
    "week_plots = FALSE      #plot changing data over time\n",
    "growing_plots = FALSE   #plots changing data over time as a growing data set (Week 1, Weeks 1-2, Weeks 1-3, etc)\n",
    "PCA_LDA = FALSE         #perform PCA followed by LDA\n",
    "LDA = TRUE             #perform only LDA\n",
    "lda_threshold = TRUE   #calculate LDA error rates based on differing thresholds, rather than maximum posterior probabilitiy\n",
    "random_forest = FALSE   #perform random forest algorithm\n",
    "test_set = FALSE        #analyze using test vs training sets (80/20), also if sameday is true, use each card as a test set\n",
    "default_rf_confusion = FALSE  #Use the default random forest confusion matrix for plotting\n",
    "threshold_rf_confusion = FALSE  #Use thresholds for inconclusive/error rates for random forest\n",
    "test_old_data = FALSE       #Test old FMA data against full dataset (Random forest)\n",
    "test_oldold_data = FALSE    #Test very original FMA data (from different method, original insulator cap)\n",
    "correlation = FALSE         #plot correlation heatmap of variables\n",
    "rf_cv = FALSE          #cross-validation analysis of random forest, plot error rate based on reducing variables\n",
    "compare_methods = FALSE   #compare and plot importance of variables based on different methods\n",
    "compare_plot = FALSE   #plot success/inconclusive/error rates for t-test/LDA/RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_dataset <- paste(getwd(), \"/Documents/UvA/Pub1_data/\", type, \"_\", ion_threshold,\"_reduced.xlsx\", sep=\"\") #name for saved .xlsx file of dataset\n",
    "\n",
    "if(!sameday){\n",
    "    file_ttest <- paste(getwd(), \"/Documents/UvA/Pub1_excel/\", type,\"_t-test_\",ion_threshold,\".xlsx\", sep=\"\") #name for saved .xlsx file of replicate t-test results\n",
    "    roc_curves <- paste(getwd(), \"/Documents/UvA/Pub1_plots/ttests/\",type,\"-\",ion_threshold,\"_ROC.pdf\", sep=\"\") #name for saved individual ROC curves, not same day\n",
    "    average_roc_curve <- paste(getwd(), \"/Documents/UvA/Pub1_plots/ttests/average/\",type,\"-\",ion_threshold,\"_ROC_average.pdf\", sep=\"\") #name for saved average ROC curve\n",
    "    file_ttest_threshold <- paste(getwd(), \"/Documents/UvA/Pub1_excel/t-test_threshold.xlsx\", sep=\"\") #name for saved .xlsx file of t-test with error rates based on threshold\n",
    "    max_ttest <- paste(getwd(),\"/Documents/UvA/Pub1_excel/max_ttest.xlsx\", sep=\"\") #Save highest success and lowest error rate data with associated normalization of 20 replicates\n",
    "    lda_loocv_plot <- paste(getwd(), \"/Documents/UvA/Pub1_plots/LDA/loocv/\",type, \"-\", ion_threshold, \"-\", normal, \".pdf\",sep=\"\") #save LDA plot with loocv error rate\n",
    "    lda_test_plot <- paste(getwd(), \"/Documents/UvA/Pub1_plots/LDA/Testset/\",type, \"-\", ion_threshold, \"-\", normal, \"_testset.pdf\",sep=\"\") #save LDA plot with test set error rate\n",
    "    lda_file <- paste(getwd(), \"/Documents/UvA/Pub1_excel/LDA_threshold.xlsx\",sep=\"\") #excel datafile with success/inconclusive/error rate based on changing LDA posterior probability thresholds\n",
    "    lda_threshold_plot <- paste(getwd(), \"/Documents/UvA/Pub1_plots/LDA/loocv/\",type, \"-\", ion_threshold, \"-\", normal, \"_threshold.pdf\",sep=\"\") #lda plot without loocv listed on plot\n",
    "    lda_threshold_plot_test <- paste(getwd(), \"/Documents/UvA/Pub1_plots/LDA/Testset/\",type, \"-\", ion_threshold, \"-\", normal, \"_threshold.pdf\",sep=\"\") #lda plot of training set with test sets without error rate listed\n",
    "    compare_plot_file <- paste(getwd(), \"/Documents/UvA/Pub1_plots/Comparison/\",type,\"-\",ion_threshold,\"-\", normal,\"_comparison.pdf\", sep=\"\") #plot comparison of results for different methods\n",
    "    compare_plot_file_test <- paste(getwd(), \"/Documents/UvA/Pub1_plots/Comparison/\",type,\"-\",ion_threshold,\"-\", normal,\"_comparison_testset.pdf\", sep=\"\") #plot comparison of results for different methods\n",
    "}\n",
    "\n",
    "if(sameday){\n",
    "    file_ttest <- paste(getwd(), \"/Documents/UvA/Pub1_excel/\", type,\"_t-test_\",ion_threshold,\"_sameday.xlsx\", sep=\"\") #name for saved .xlsx file of replicate t-test results\n",
    "    roc_curves <- paste(getwd(), \"/Documents/UvA/Pub1_plots/ttests/\",type,\"-\",ion_threshold,\"_ROC_sameday.pdf\", sep=\"\") #name for saved individual ROC curves, same day\n",
    "    average_roc_curve <- paste(getwd(), \"/Documents/UvA/Pub1_plots/ttests/average/\",type,\"-\",ion_threshold,\"_ROC_sameday_average.pdf\", sep=\"\") #name for saved average ROC curves - same day\n",
    "    file_ttest_threshold <- paste(getwd(), \"/Documents/UvA/Pub1_excel/t-test_threshold_sameday.xlsx\", sep=\"\") #name for saved .xlsx file of t-test with error rates based on threshold, same day\n",
    "    max_ttest <- paste(getwd(),\"/Documents/UvA/Pub1_excel/max_ttest_sameday.xlsx\", sep=\"\") #Save highest success and lowest error rate data with associated normalization of 20 replicates, same day\n",
    "    lda_loocv_plot <- paste(getwd(), \"/Documents/UvA/Pub1_plots/LDA/Sameday/loocv/\",type, \"-\", ion_threshold, \"-\", normal, \".pdf\",sep=\"\") #save LDA plots with loocv error rate, same day\n",
    "    lda_test_plot <- paste(getwd(), \"/Documents/UvA/Pub1_plots/LDA/Sameday/Testset/\",type, \"-\", ion_threshold, \"-\", normal, \"_testset.pdf\",sep=\"\") #save LDA plot with test set error rate, same day\n",
    "    lda_file <- paste(getwd(), \"/Documents/UvA/Pub1_excel/LDA_threshold_sameday.xlsx\",sep=\"\") #excel datafile with success/inconclusive/error rate based on changing LDA posterior probability thresholds, same day\n",
    "    lda_threshold_plot <- paste(getwd(), \"/Documents/UvA/Pub1_plots/LDA/Sameday/loocv/\",type, \"-\", ion_threshold, \"-\", normal, \"_threshold.pdf\",sep=\"\") #lda plots without loocv listed on plots, same day\n",
    "    lda_threshold_plot_test <- paste(getwd(), \"/Documents/UvA/Pub1_plots/LDA/Sameday/Testset/\",type, \"-\", ion_threshold, \"-\", normal, \"_threshold.pdf\",sep=\"\") #lda plots of sameday training sets with test sets without error rate listed\n",
    "    compare_plot_file <- paste(getwd(), \"/Documents/UvA/Pub1_plots/Comparison/sameday/\",type,\"-\",ion_threshold,\"-\", normal,\"_comparison_sameday.pdf\", sep=\"\")  #plot comparison of results for different methods\n",
    "    compare_plot_file_test <- paste(getwd(), \"/Documents/UvA/Pub1_plots/Comparison/sameday/\",type,\"-\",ion_threshold,\"-\", normal,\"_comparison_sameday_testset.pdf\", sep=\"\")  #plot comparison of results for different methods\n",
    "}\n",
    "\n",
    "file_anova <- paste(getwd(), \"/Documents/UvA/Pub1_excel/ANOVA.xlsx\", sep=\"\") #name for saved .xlsx file of ANOVA\n",
    "file_weekly <- paste(getwd(), \"/Documents/UvA/Pub1_plots/Week-to-week/\",type,\"-\",ion_threshold,\"-\",normal,\".pdf\", sep=\"\") #name for week to week plots\n",
    "file_growing <- paste(getwd(), \"/Documents/UvA/Pub1_plots/Growing_dataset/\",type,\"-\",ion_threshold,\"-\",normal,\"_growing.pdf\", sep=\"\") #name for growing dataset plots\n",
    "rf_plot_name <- paste(getwd(), \"/Documents/UvA/Pub1_plots/RF/Importance/\",type,\"-\",ion_threshold,\"-\",normal,\"_RF_importance.pdf\", sep=\"\") #name for RF variable importance plots\n",
    "rf_plot_name_combined <- paste(getwd(), \"/Documents/UvA/Pub1_plots/RF/\",type,\"-\",ion_threshold,\"-\",normal,\"_RF_combined.pdf\", sep=\"\") #name for RF variable importance combined with top 6 ion distributions plots and error rate tables\n",
    "rf_sameday_file <- paste(getwd(), \"/Documents/UvA/Pub1_excel/RF_sameday.xlsx\", sep=\"\")  #random forest success/inconclusive/error for same day (oob and test set)\n",
    "rf_plot_name_combined_test <- paste(getwd(), \"/Documents/UvA/Pub1_plots/RF/Testset/\",type,\"-\",ion_threshold,\"-\",normal,\"_RF_testset.pdf\", sep=\"\") #name for RF variable importance combined with top 6 ion distributions plots and error rate tables, for 80/20 test set\n",
    "rf_olddata <- paste(getwd(), \"/Documents/UvA/Pub1_excel/RF_olddata.xlsx\", sep=\"\") #random forest success/inconclusive/error for old FMA data\n",
    "rf_oldold_data <- paste(getwd(), \"/Documents/UvA/Pub1_excel/RF_oldold_data.xlsx\", sep=\"\") #random forest success/inconclusive/error for original FMA data\n",
    "pca_file <- paste(getwd(), \"/Documents/UvA/Pub1_plots/PCA/\",type, \"-\", ion_threshold, \"-\", normal, \".pdf\",sep=\"\") #saved solely PCA plots\n",
    "pca_test_file <- paste(getwd(), \"/Documents/UvA/Pub1_plots/PCA/\",type, \"-\", ion_threshold, \"-\", normal, \"_testset.pdf\",sep=\"\") #saved training set PCA plots\n",
    "pcalda_loocv_plot <- paste(getwd(), \"/Documents/UvA/Pub1_plots/PCA_LDA/loocv/\",type, \"-\", ion_threshold, \"-\", normal, \".pdf\",sep=\"\") #saved PCA/LDA plots with loocv error rate\n",
    "pcalda_test_plot <- paste(getwd(), \"/Documents/UvA/Pub1_plots/PCA_LDA/Testset/\",type, \"-\", ion_threshold, \"-\", normal, \"_testset.pdf\",sep=\"\") #saved PCA/LDA plots of training set data with test set shown and with test set error rate\n",
    "pcalda_threshold_plot <- paste(getwd(), \"/Documents/UvA/Pub1_plots/PCA_LDA/loocv/\",type, \"-\", ion_threshold, \"-\", normal, \"_threshold.pdf\",sep=\"\") #saved full dataset PCA/LDA without loocv error rate shown\n",
    "pcalda_threshold_plot_test <- paste(getwd(), \"/Documents/UvA/Pub1_plots/PCA_LDA/Testset/\",type, \"-\", ion_threshold, \"-\", normal, \"_threshold.pdf\",sep=\"\") #saved PCA/LDA plots of training set data with test set shown, without test set error rate\n",
    "pcalda_file <- paste(getwd(), \"/Documents/UvA/Pub1_excel/PCA_LDA_threshold.xlsx\",sep=\"\") #success/inconclusive/error rate workbook for PCA/LDA\n",
    "corr_file_full <- paste(getwd(), \"/Documents/UvA/Pub1_plots/Correlation/\",type, \"-\", ion_threshold, \"-\", normal, \".pdf\",sep=\"\") #plot for correlation heatmap (only full dataset)\n",
    "corr_file_all <- paste(getwd(), \"/Documents/UvA/Pub1_plots/Correlation/\",type, \"-\", ion_threshold, \"-\", normal, \"_all.pdf\",sep=\"\") #plots for correlation heatmaps (full dataset, and isomer pairs)\n",
    "rfcv_file <- paste(getwd(), \"/Documents/UvA/Pub1_plots/RF/CV/\",type,\"-\",ion_threshold,\"-\",normal,\"_RF_combined.pdf\", sep=\"\") #plot of random forest cross-validation results showing error rate based on reducing variables\n",
    "comparison_file <-paste(getwd(), \"/Documents/UvA/Pub1_plots/Comparison/Axis-adjust/\",type,\"-\",ion_threshold,\"-\", normal,\"_comparison.pdf\", sep=\"\") #plots of comparing variable importance based on method used (axis-adjust - with y-axis for t-test adjusted to min=0/35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize <- function(data, normal){                        #create function to normalize data based on desired method\n",
    "    n_30 <- sum(grepl(\"30V\", colnames(data)))               #count how many 30V variables are retained\n",
    "    n_60 <- sum(grepl(\"60V\", colnames(data)))               #count how many 60V variables are retained\n",
    "    n_90 <- sum(grepl(\"90V\", colnames(data)))               #count how many 90V variables are retained\n",
    "    \n",
    "sum_30 <- vector(length=n_30)                               #create vector to store data to use for normalization\n",
    "sum_60 <- vector(length=n_60)                               #create vector to store data to use for normalization\n",
    "sum_90 <- vector(length=n_90)                               #create vector to store data to use for normalization\n",
    "\n",
    "my_data_normal <- data                                      #create copy of data matrix to store normalized data\n",
    "\n",
    "for (j in 1:nrow(data)) {                                   #for every row in the data matrix\n",
    "    if(normal == \"vectorlength\") {                          #if \"vectorlength\" is chosen, data is normalized to a unit vector per voltage\n",
    "      for (k in 1:n_30) {                                   #for each 30V m/z\n",
    "        sum_30[k] <- data[j,k]*data[j,k]                    #in sum_30 vector, insert square of abundance of the given sample at the given m/z\n",
    "      }\n",
    "      \n",
    "      for(k in 1:n_60){                                     #for each 60V m/z\n",
    "        sum_60[k] <- data[j,k+n_30]*data[j,k+n_30]          #in sum_60 vector, insert square of abundance of the given sample at the given m/z\n",
    "      }\n",
    "      \n",
    "      for(k in 1:n_90){                                     #for each 90V m/z\n",
    "        sum_90[k] <- data[j,k+n_30+n_60]*data[j,k+n_30+n_60]#in sum_90 vector, insert square of abundance of the given sample at the given m/z\n",
    "      }\n",
    "      \n",
    "      divisor_30 <- sqrt(sum(sum_30))                       #the 30V divisor for the jth sample is the square root of the sum of the squares that are stored in the sum_30 vector\n",
    "      divisor_60 <- sqrt(sum(sum_60))                       #the 60V divisor for the jth sample is the square root of the sum of the squares that are stored in the sum_60 vector\n",
    "      divisor_90 <- sqrt(sum(sum_90))                       #the 90V divisor for the jth sample is the square root of the sum of the squares that are stored in the sum_90 vector\n",
    "    }\n",
    "    \n",
    "    if (normal == \"ioncurrent\") {                           #if ioncurrent is selected, abundances are normalized by the total ion current of all selected m/z ions for the given voltage\n",
    "      for (k in 1:n_30) {                                   #for each 30V m/z\n",
    "        sum_30[k] <- data[j,k]                              #in sum_30 vector, insert the abundance for each sample at the given m/z\n",
    "      }\n",
    "      \n",
    "      for(k in 1:n_60){                                     #for each 60V m/z\n",
    "        sum_60[k] <- data[j,k+n_30]                         #in sum_60 vector, insert the abundance for each sample at the given m/z\n",
    "      }\n",
    "      \n",
    "      for(k in 1:n_90){                                     #for each 90V m/z\n",
    "        sum_90[k] <- data[j,k+n_30+n_60]                    #in sum_90 vector, insert the abundance for each sample at the given m/z\n",
    "      }\n",
    "      \n",
    "      divisor_30 <- sum(sum_30)                             #the 30V divisor for the jth sample is the sum of the abundances of all selected 30V \n",
    "      divisor_60 <- sum(sum_60)                             #the 60V divisor for the jth sample is the sum of the abundances of all selected 60V\n",
    "      divisor_90 <- sum(sum_90)                             #the 90V divisor for the jth sample is the sum of the abundances of all selected 90V\n",
    "    }\n",
    "    \n",
    "    for (k in 1:n_30){                                      #for each 30V m/z\n",
    "      my_data_normal[j,k] <-data[j,k]/divisor_30            #divide abundance by stored 30V divisor and store in my_data_normal\n",
    "    }\n",
    "    \n",
    "    for(k in 1:n_60){                                       #for each 60V m/z\n",
    "      my_data_normal[j,k+n_30] <- data[j,k+n_30]/divisor_60 #divide abundance by stored 60V divisor and store in my_data_normal\n",
    "    }\n",
    "    \n",
    "    for(k in 1:n_90){                                       #for each 90V m/z\n",
    "      my_data_normal[j,k+n_30+n_60] <- data[j,k+n_30+n_60]/divisor_90 #divide abundance by stored 90V divisor and store in my_data_normal\n",
    "    }\n",
    "  }\n",
    "\n",
    "return(my_data_normal)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin to clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>864</li><li>3596</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 864\n",
       "\\item 3596\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 864\n",
       "2. 3596\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  864 3596"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "isomer_data <- isomer_all[,-(1:5)]                                   #remove date, volume, card #, voltage, Label (Isomer ID)\n",
    "colnames(isomer_data) <- str_replace(colnames(isomer_data),\"X.\", \"\") #remove X. from Colnames (introduced by < in header)\n",
    "number_of_spectra <- nrow(isomer_data)/3                             #number of spectra per isomer\n",
    "isomer_data[is.na(isomer_data)] <- 0                                 #replace any NAs with 0\n",
    "dim(isomer_data)  #print initial dimensions of data, number of rows should be number of analyzed samples * 3 (3 voltages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all-zero columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>864</li><li>300</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 864\n",
       "\\item 300\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 864\n",
       "2. 300\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 864 300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detect_zero <- function(data) {                        #create a function to detect 0's in each m/z bins\n",
    "    return(length(which(data==0)))                     #count how many indices in a given data vector are equal to 0\n",
    "}\n",
    "\n",
    "zero_vector <- apply(isomer_data, MARGIN = 2, FUN = detect_zero) #apply the detect_zero function to all columns in isomer_data\n",
    "\n",
    "removed_ions <- which(zero_vector == nrow(isomer_data))#ID which bins have no signal in any spectra\n",
    "isomer_data <- isomer_data[,-removed_ions]             #remove bins with all zero signal\n",
    "dim(isomer_data)                                       #show dimensions of reduced isomer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector to identify max abundance in each spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>4444.7849478</li><li>12912.2181653</li><li>21490.7210119</li><li>4150.5309416</li><li>10835.0296068</li><li>13572.5229848</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4444.7849478\n",
       "\\item 12912.2181653\n",
       "\\item 21490.7210119\n",
       "\\item 4150.5309416\n",
       "\\item 10835.0296068\n",
       "\\item 13572.5229848\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4444.7849478\n",
       "2. 12912.2181653\n",
       "3. 21490.7210119\n",
       "4. 4150.5309416\n",
       "5. 10835.0296068\n",
       "6. 13572.5229848\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  4444.785 12912.218 21490.721  4150.531 10835.030 13572.523"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_times_percent <- function(X, ion_threshold){      #Create a function to identify the maximum abundance in a single voltage spectrum and multiply by the ion threshold percentage. This gives a threshold over which a given number of spectra in a m/z bin must be over in order to be retained \n",
    "  max(X) * (ion_threshold/100)       \n",
    "}\n",
    "threshold_max <- apply(isomer_data, MARGIN = 1, FUN = max_times_percent, ion_threshold = ion_threshold) #store the thresholds in a vector to later test how many spectra in a m/z bin reach their respective thresholds \n",
    "\n",
    "head(threshold_max) #show the first six threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector to determine how many spectra reach the abundance threshold\n",
    "#### Reduce data to only ions for which the number of spectra over the abundance threshold is over the set \"max threshold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'83.050'</li><li>'109.050'</li><li>'137.100'</li><li>'154.125'</li><li>'155.125'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item '83.050'\n",
       "\\item '109.050'\n",
       "\\item '137.100'\n",
       "\\item '154.125'\n",
       "\\item '155.125'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. '83.050'\n",
       "2. '109.050'\n",
       "3. '137.100'\n",
       "4. '154.125'\n",
       "5. '155.125'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"83.050\"  \"109.050\" \"137.100\" \"154.125\" \"155.125\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum_if_greater <- function(X, threshold_max){   #create a function count how many spectra in a given m/z bin are over their respective spectrum thresholds\n",
    "    sum(X>threshold_max)\n",
    "}\n",
    "\n",
    "threshold_vector <- apply(isomer_data, MARGIN = 2, FUN = sum_if_greater, threshold_max = threshold_max) #create a vector containing the number of spectra in each m/z bin that meet their threshold\n",
    "\n",
    "selected_ions <- which(threshold_vector>max_threshold)  #create a vector of ions whose number of spectra over the abundance threshold is more than the set number required\n",
    "colnames(isomer_data[selected_ions]) #print the names of the selected ions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add voltage and label information to data and separate based on voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 7</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Voltage</th><th scope=col>Label</th><th scope=col>83.050</th><th scope=col>109.050</th><th scope=col>137.100</th><th scope=col>154.125</th><th scope=col>155.125</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>30</td><td>2</td><td>0</td><td> 6657.861</td><td>13051.642</td><td>44447.85</td><td>4425.142</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>30</td><td>2</td><td>0</td><td> 5460.045</td><td>11854.513</td><td>41505.31</td><td>4334.198</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>30</td><td>2</td><td>0</td><td> 3231.863</td><td> 7119.597</td><td>36323.79</td><td>3667.519</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>30</td><td>2</td><td>0</td><td> 4708.092</td><td>10210.246</td><td>42320.23</td><td>4279.409</td></tr>\n",
       "\t<tr><th scope=row>13</th><td>30</td><td>2</td><td>0</td><td> 9603.635</td><td>20178.159</td><td>72752.30</td><td>6668.887</td></tr>\n",
       "\t<tr><th scope=row>16</th><td>30</td><td>2</td><td>0</td><td>12592.793</td><td>24562.279</td><td>85645.82</td><td>7994.639</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 7\n",
       "\\begin{tabular}{r|lllllll}\n",
       "  & Voltage & Label & 83.050 & 109.050 & 137.100 & 154.125 & 155.125\\\\\n",
       "  & <int> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 30 & 2 & 0 &  6657.861 & 13051.642 & 44447.85 & 4425.142\\\\\n",
       "\t4 & 30 & 2 & 0 &  5460.045 & 11854.513 & 41505.31 & 4334.198\\\\\n",
       "\t7 & 30 & 2 & 0 &  3231.863 &  7119.597 & 36323.79 & 3667.519\\\\\n",
       "\t10 & 30 & 2 & 0 &  4708.092 & 10210.246 & 42320.23 & 4279.409\\\\\n",
       "\t13 & 30 & 2 & 0 &  9603.635 & 20178.159 & 72752.30 & 6668.887\\\\\n",
       "\t16 & 30 & 2 & 0 & 12592.793 & 24562.279 & 85645.82 & 7994.639\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 7\n",
       "\n",
       "| <!--/--> | Voltage &lt;int&gt; | Label &lt;int&gt; | 83.050 &lt;dbl&gt; | 109.050 &lt;dbl&gt; | 137.100 &lt;dbl&gt; | 154.125 &lt;dbl&gt; | 155.125 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|\n",
       "| 1 | 30 | 2 | 0 |  6657.861 | 13051.642 | 44447.85 | 4425.142 |\n",
       "| 4 | 30 | 2 | 0 |  5460.045 | 11854.513 | 41505.31 | 4334.198 |\n",
       "| 7 | 30 | 2 | 0 |  3231.863 |  7119.597 | 36323.79 | 3667.519 |\n",
       "| 10 | 30 | 2 | 0 |  4708.092 | 10210.246 | 42320.23 | 4279.409 |\n",
       "| 13 | 30 | 2 | 0 |  9603.635 | 20178.159 | 72752.30 | 6668.887 |\n",
       "| 16 | 30 | 2 | 0 | 12592.793 | 24562.279 | 85645.82 | 7994.639 |\n",
       "\n"
      ],
      "text/plain": [
       "   Voltage Label 83.050 109.050   137.100   154.125  155.125 \n",
       "1  30      2     0       6657.861 13051.642 44447.85 4425.142\n",
       "4  30      2     0       5460.045 11854.513 41505.31 4334.198\n",
       "7  30      2     0       3231.863  7119.597 36323.79 3667.519\n",
       "10 30      2     0       4708.092 10210.246 42320.23 4279.409\n",
       "13 30      2     0       9603.635 20178.159 72752.30 6668.887\n",
       "16 30      2     0      12592.793 24562.279 85645.82 7994.639"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "isomer_reduced <- cbind(isomer_all[,4:5],isomer_data[,selected_ions])  #combine the selected ions with Isomer label and voltage info\n",
    "isomer_30 <- isomer_reduced[which(isomer_reduced[,1]==30),]            #separate out the 30V spectra\n",
    "isomer_60 <- isomer_reduced[which(isomer_reduced[,1]==60),]            #separate out the 60V spectra\n",
    "isomer_90 <- isomer_reduced[which(isomer_reduced[,1]==90),]            #separate out the 90V spectra\n",
    "head(isomer_30)      #show the first six rows of the 30V data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce 30V data \n",
    "- Based on ions whose average abundance in voltage is over mean threshold\n",
    "- Remove remaining ions where over 2/3 have 0 signal\n",
    "- Replace remaining 0 abundance with centroiding threshold from instrument\n",
    "- Remove fringe data (ions whose bin is less than 1 mass unit higher than main ion value\n",
    "- Remove likely isotopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 5</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Voltage</th><th scope=col>Label</th><th scope=col>109.050</th><th scope=col>137.100</th><th scope=col>154.125</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>30</td><td>2</td><td> 6657.861</td><td>13051.642</td><td>44447.85</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>30</td><td>2</td><td> 5460.045</td><td>11854.513</td><td>41505.31</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>30</td><td>2</td><td> 3231.863</td><td> 7119.597</td><td>36323.79</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>30</td><td>2</td><td> 4708.092</td><td>10210.246</td><td>42320.23</td></tr>\n",
       "\t<tr><th scope=row>13</th><td>30</td><td>2</td><td> 9603.635</td><td>20178.159</td><td>72752.30</td></tr>\n",
       "\t<tr><th scope=row>16</th><td>30</td><td>2</td><td>12592.793</td><td>24562.279</td><td>85645.82</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 5\n",
       "\\begin{tabular}{r|lllll}\n",
       "  & Voltage & Label & 109.050 & 137.100 & 154.125\\\\\n",
       "  & <int> & <int> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 30 & 2 &  6657.861 & 13051.642 & 44447.85\\\\\n",
       "\t4 & 30 & 2 &  5460.045 & 11854.513 & 41505.31\\\\\n",
       "\t7 & 30 & 2 &  3231.863 &  7119.597 & 36323.79\\\\\n",
       "\t10 & 30 & 2 &  4708.092 & 10210.246 & 42320.23\\\\\n",
       "\t13 & 30 & 2 &  9603.635 & 20178.159 & 72752.30\\\\\n",
       "\t16 & 30 & 2 & 12592.793 & 24562.279 & 85645.82\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 5\n",
       "\n",
       "| <!--/--> | Voltage &lt;int&gt; | Label &lt;int&gt; | 109.050 &lt;dbl&gt; | 137.100 &lt;dbl&gt; | 154.125 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 30 | 2 |  6657.861 | 13051.642 | 44447.85 |\n",
       "| 4 | 30 | 2 |  5460.045 | 11854.513 | 41505.31 |\n",
       "| 7 | 30 | 2 |  3231.863 |  7119.597 | 36323.79 |\n",
       "| 10 | 30 | 2 |  4708.092 | 10210.246 | 42320.23 |\n",
       "| 13 | 30 | 2 |  9603.635 | 20178.159 | 72752.30 |\n",
       "| 16 | 30 | 2 | 12592.793 | 24562.279 | 85645.82 |\n",
       "\n"
      ],
      "text/plain": [
       "   Voltage Label 109.050   137.100   154.125 \n",
       "1  30      2      6657.861 13051.642 44447.85\n",
       "4  30      2      5460.045 11854.513 41505.31\n",
       "7  30      2      3231.863  7119.597 36323.79\n",
       "10 30      2      4708.092 10210.246 42320.23\n",
       "13 30      2      9603.635 20178.159 72752.30\n",
       "16 30      2     12592.793 24562.279 85645.82"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_ions_2 <- which(colMeans(isomer_30[,-(1:2)])>mean_threshold)+2 #determine ions whose mean abundance is over a threshold\n",
    "isomer_30_temp <- isomer_30[,selected_ions_2]     #temporarily store only those ions over the mean abundance threshold\n",
    "removed_ions_2 <- which(colSums(isomer_30_temp==0)>(2*number_of_spectra/3)) #determine remaining ions where over 2/3 have zero signal\n",
    "\n",
    "if(length(removed_ions_2)!=0){              #if there are any ions that don't meet the 2/3 threshold\n",
    "  isomer_30_temp <- cbind(isomer_30[,1:2],isomer_30_temp[,-removed_ions_2])   #replace isomer_30_temp with label and voltage info and 30V data with ions removed that don't reach the 2/3 threshold\n",
    "} else {\n",
    "  isomer_30_temp <- cbind(isomer_30[,1:2],isomer_30_temp) #if there are no ions to be removed, combine isomer_30_temp with label and voltage info\n",
    "}\n",
    "\n",
    "isomer_30_temp[isomer_30_temp==0] <- replace_zero #replace zero with centroiding threshold from instrument\n",
    "\n",
    "to_remove_30 <- vector()                      #determine ions whose bin is less than 1 mass unit greater (fringe)\n",
    "for (i in 1:(ncol(isomer_30_temp)-3)){        #for 1 to number of columns of isomer_30_temp-3 (to account for first two columns being label/voltage info)\n",
    "  to_remove_30 <- append(to_remove_30,which(as.integer(colnames(isomer_30_temp[,-(1:2)])[-(1:i)]) <= as.integer(colnames(isomer_30_temp[,-(1:2)])[i])+0.9)+i+2)\n",
    "} #find column names that, when rounded down to the nearest integer, are less than 0.9 amu greater than the column(s) preceding them\n",
    "\n",
    "if(length(to_remove_30)!=0){  #if there are any fringe ions \n",
    "  isomer_30_temp <- isomer_30_temp[,-to_remove_30] #remove fringe ions\n",
    "}\n",
    "\n",
    "to_remove_30a<-vector()    #determine ions which are 1-2 mass units greater (possible isotopes) \n",
    "for (i in 1:(ncol(isomer_30_temp)-3)){   #for 1 to number of columns of isomer_30_temp-3 (to account for first two columns being label/voltage\n",
    "  to_remove_30a <- append(to_remove_30a,which(as.integer(colnames(isomer_30_temp[,-(1:2)])[-(1:i)]) <= as.integer(colnames(isomer_30_temp[,-(1:2)])[i])+1)+i+2)\n",
    "} #find column names that, when rounded down to the nearest integer, are less than 1 amu greater than the column(s) preceding them\n",
    "\n",
    "for(i in 1:length(to_remove_30a)){ #if ions determined above are not suspected isotopes, remove them from to remove vector\n",
    "  if(mean(isomer_30_temp[,to_remove_30a[i]]/isomer_30_temp[,to_remove_30a[i]-1],na.rm = TRUE) < 0.011 | mean(isomer_30_temp[,to_remove_30a[i]]/isomer_30_temp[,to_remove_30a[i]-1],na.rm = TRUE) > (as.integer(colnames(isomer_30_temp)[to_remove_30a[i]])/1000)) {#if the mean proportion of the suspected isotope to the preceding ion abundance is less than 0.011 or more than the (as integer) bin divided by 1000 ex: 109 checks form 10.9%)\n",
    "    to_remove_30a[i] <- NA\n",
    "  }\n",
    "}\n",
    "to_remove_30a <- na.omit(to_remove_30a) #remove m/z values that do not appear consistent with isotopes\n",
    "\n",
    "isomer_30_final <- isomer_30_temp[,-to_remove_30a] #final 30V data, remove isotope m/z values\n",
    "\n",
    "head(isomer_30_final) #first six rows of remaining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60V data selection\n",
    "    Same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Voltage</th><th scope=col>Label</th><th scope=col>83.050</th><th scope=col>109.050</th><th scope=col>137.100</th><th scope=col>154.125</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2</th><td>60</td><td>2</td><td>259.0592</td><td>129122.18</td><td>32765.59</td><td>72403.18</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>60</td><td>2</td><td>231.6909</td><td>108350.30</td><td>18945.40</td><td>29371.14</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>60</td><td>2</td><td>254.9387</td><td>122481.22</td><td>23956.12</td><td>37096.90</td></tr>\n",
       "\t<tr><th scope=row>11</th><td>60</td><td>2</td><td>120.0000</td><td> 92041.57</td><td>16620.21</td><td>25216.46</td></tr>\n",
       "\t<tr><th scope=row>14</th><td>60</td><td>2</td><td>475.7090</td><td>196938.01</td><td>28681.14</td><td>35513.54</td></tr>\n",
       "\t<tr><th scope=row>17</th><td>60</td><td>2</td><td>571.9423</td><td>225310.52</td><td>27203.26</td><td>28970.47</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 6\n",
       "\\begin{tabular}{r|llllll}\n",
       "  & Voltage & Label & 83.050 & 109.050 & 137.100 & 154.125\\\\\n",
       "  & <int> & <int> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t2 & 60 & 2 & 259.0592 & 129122.18 & 32765.59 & 72403.18\\\\\n",
       "\t5 & 60 & 2 & 231.6909 & 108350.30 & 18945.40 & 29371.14\\\\\n",
       "\t8 & 60 & 2 & 254.9387 & 122481.22 & 23956.12 & 37096.90\\\\\n",
       "\t11 & 60 & 2 & 120.0000 &  92041.57 & 16620.21 & 25216.46\\\\\n",
       "\t14 & 60 & 2 & 475.7090 & 196938.01 & 28681.14 & 35513.54\\\\\n",
       "\t17 & 60 & 2 & 571.9423 & 225310.52 & 27203.26 & 28970.47\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 6\n",
       "\n",
       "| <!--/--> | Voltage &lt;int&gt; | Label &lt;int&gt; | 83.050 &lt;dbl&gt; | 109.050 &lt;dbl&gt; | 137.100 &lt;dbl&gt; | 154.125 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 2 | 60 | 2 | 259.0592 | 129122.18 | 32765.59 | 72403.18 |\n",
       "| 5 | 60 | 2 | 231.6909 | 108350.30 | 18945.40 | 29371.14 |\n",
       "| 8 | 60 | 2 | 254.9387 | 122481.22 | 23956.12 | 37096.90 |\n",
       "| 11 | 60 | 2 | 120.0000 |  92041.57 | 16620.21 | 25216.46 |\n",
       "| 14 | 60 | 2 | 475.7090 | 196938.01 | 28681.14 | 35513.54 |\n",
       "| 17 | 60 | 2 | 571.9423 | 225310.52 | 27203.26 | 28970.47 |\n",
       "\n"
      ],
      "text/plain": [
       "   Voltage Label 83.050   109.050   137.100  154.125 \n",
       "2  60      2     259.0592 129122.18 32765.59 72403.18\n",
       "5  60      2     231.6909 108350.30 18945.40 29371.14\n",
       "8  60      2     254.9387 122481.22 23956.12 37096.90\n",
       "11 60      2     120.0000  92041.57 16620.21 25216.46\n",
       "14 60      2     475.7090 196938.01 28681.14 35513.54\n",
       "17 60      2     571.9423 225310.52 27203.26 28970.47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_ions_3 <- which(colMeans(isomer_60[,-(1:2)])>mean_threshold)+2  #determine ions whose mean abundance is over a threshold\n",
    "isomer_60_temp <- isomer_60[,selected_ions_3]       #temporarily store only those ions over the mean abundance threshold\n",
    "removed_ions_3 <- which(colSums(isomer_60_temp==0)>(2*number_of_spectra/3))  #determine remaining ions where over 2/3 have zero signal\n",
    "\n",
    "if(length(removed_ions_3)!=0){                    #if there are any ions that don't meet the 2/3 threshold\n",
    "  isomer_60_temp <- cbind(isomer_60[,1:2],isomer_60_temp[,-removed_ions_3]) #replace isomer_60_temp with label and voltage info and 60V data with ions removed that don't reach the 2/3 threshold\n",
    "} else{\n",
    "  isomer_60_temp <- cbind(isomer_60[,1:2],isomer_60_temp)    #if there are no ions to be removed, combine isomer_60_temp with label and voltage info\n",
    "}\n",
    "\n",
    "isomer_60_temp[isomer_60_temp==0] <- replace_zero #replace zero with centroiding threshold from instrument\n",
    "\n",
    "to_remove_60 <- vector()       #determine ions whose bin is less than 1 mass unit greater (fringe)\n",
    "for (i in 1:(ncol(isomer_60_temp)-3)){       #for 1 to number of columns of isomer_30_temp-3 (to account for first two columns being label/voltage info)\n",
    "  to_remove_60 <- append(to_remove_60,which(as.integer(colnames(isomer_60_temp[,-(1:2)])[-(1:i)]) <= as.integer(colnames(isomer_60_temp[,-(1:2)])[i])+0.9)+i+2)\n",
    "} #find column names that, when rounded down to the nearest integer, are less than 0.9 amu greater than the column(s) preceding them\n",
    "\n",
    "if(length(to_remove_60)!=0){ #if there are any fringe ions \n",
    "  isomer_60_temp <- isomer_60_temp[,-to_remove_60] #remove fringe ions\n",
    "}\n",
    "\n",
    "to_remove_60a<-vector() #determine ions which are 1-2 mass units greater (possible isotopes)\n",
    "for (i in 1:(ncol(isomer_60_temp)-3)){ #for 1 to number of columns of isomer_30_temp-3 (to account for first two columns being label/voltage\n",
    "  to_remove_60a <- append(to_remove_60a,which(as.integer(colnames(isomer_60_temp[,-(1:2)])[-(1:i)]) <= as.integer(colnames(isomer_60_temp[,-(1:2)])[i])+1)+i+2)\n",
    "}#find column names that, when rounded down to the nearest integer, are less than 1 amu greater than the column(s) preceding them\n",
    "\n",
    "for(i in 1:length(to_remove_60a)){ #if ions determined above are not suspected isotopes, remove them from to remove vector\n",
    "  if(mean(isomer_60_temp[,to_remove_60a[i]]/isomer_60_temp[,to_remove_60a[i]-1],na.rm = TRUE) < 0.011 | mean(isomer_60_temp[,to_remove_60a[i]]/isomer_60_temp[,to_remove_60a[i]-1],na.rm = TRUE) > (as.integer(colnames(isomer_60_temp)[to_remove_60a[i]])/1000)) {#if the mean proportion of the suspected isotope to the preceding ion abundance is less than 0.011 or more than the (as integer) bin divided by 1000 ex: 109 checks form 10.9%)\n",
    "    to_remove_60a[i] <- NA\n",
    "  }\n",
    "}\n",
    "to_remove_60a <- na.omit(to_remove_60a) #remove m/z values that do not appear consistent with isotopes\n",
    "\n",
    "isomer_60_final <- isomer_60_temp[,-to_remove_60a]#final 60V data, remove isotope m/z values\n",
    "head(isomer_60_final) #first six rows of remaining variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90V data selection\n",
    "    Same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ions_4 <- which(colMeans(isomer_90[,-(1:2)])>mean_threshold)+2 #determine ions whose mean abundance is over a threshold\n",
    "isomer_90_temp <- isomer_90[,selected_ions_4]  #temporarily store only those ions over the mean abundance threshold\n",
    "removed_ions_4 <- which(colSums(isomer_90_temp==0)>(2*number_of_spectra/3))  #determine remaining ions where over 2/3 have zero signal\n",
    "\n",
    "if(length(removed_ions_4)!=0){   #if there are any ions that don't meet the 2/3 threshold\n",
    "  isomer_90_temp <- cbind(isomer_90[,1:2],isomer_90_temp[,-removed_ions_4]) #replace isomer_30_temp with label and voltage info and 30V data with ions removed that don't reach the 2/3 threshold\n",
    "} else{\n",
    "  isomer_90_temp <- cbind(isomer_90[,1:2],isomer_90_temp)  #if there are no ions to be removed, combine isomer_30_temp with label and voltage info\n",
    "}\n",
    "isomer_90_temp[isomer_90_temp==0] <- replace_zero #replace zero with centroiding threshold from instrument\n",
    "\n",
    "to_remove_90 <- vector()  #determine ions whose bin is less than 1 mass unit greater (fringe)\n",
    "for (i in 1:(ncol(isomer_90_temp)-3)){  #for 1 to number of columns of isomer_30_temp-3 (to account for first two columns being label/voltage info)\n",
    "  to_remove_90 <- append(to_remove_90,which(as.integer(colnames(isomer_90_temp[,-(1:2)])[-(1:i)]) <= as.integer(colnames(isomer_90_temp[,-(1:2)])[i])+0.9)+i+2)\n",
    "} #find column names that, when rounded down to the nearest integer, are less than 0.9 amu greater than the column(s) preceding them\n",
    "\n",
    "if(length(to_remove_90)!=0){ #if there are any fringe ions \n",
    "  isomer_90_temp <- isomer_90_temp[,-to_remove_90] #remove fringe ions\n",
    "}\n",
    "\n",
    "to_remove_90a<-vector() #determine ions which are 1-2 mass units greater (possible isotopes) \n",
    "for (i in 1:(ncol(isomer_90_temp)-3)){ #for 1 to number of columns of isomer_30_temp-3 (to account for first two columns being label/voltage\n",
    "  to_remove_90a <- append(to_remove_90a,which(as.integer(colnames(isomer_90_temp[,-(1:2)])[-(1:i)]) <= as.integer(colnames(isomer_90_temp[,-(1:2)])[i])+1)+i+2)\n",
    "} #find column names that, when rounded down to the nearest integer, are less than 1 amu greater than the column(s) preceding them\n",
    "\n",
    "for(i in 1:length(to_remove_90a)){ #if ions determined above are not suspected isotopes, remove them from to remove vector\n",
    "  if(mean(isomer_90_temp[,to_remove_90a[i]]/isomer_90_temp[,to_remove_90a[i]-1],na.rm = TRUE) < 0.011 | mean(isomer_90_temp[,to_remove_90a[i]]/isomer_90_temp[,to_remove_90a[i]-1],na.rm = TRUE) > (as.integer(colnames(isomer_90_temp)[to_remove_90a[i]])/1000)) {#if the mean proportion of the suspected isotope to the preceding ion abundance is less than 0.011 or more than the (as integer) bin divided by 1000 ex: 109 checks form 10.9%)\n",
    "    to_remove_90a[i] <- NA\n",
    "  }\n",
    "}\n",
    "to_remove_90a <- na.omit(to_remove_90a) #remove m/z values that do not appear consistent with isotopes\n",
    "\n",
    "isomer_90_final <- isomer_90_temp[,-to_remove_90a]  #final 90V data, remove isotope m/z values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all voltages\n",
    "- Save finished dataset if desired\n",
    "- Determine number of ions of each voltage included in final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"3 30V ions\"\n",
      "[1] \"4 60V ions\"\n",
      "[1] \"4 90V ions\"\n"
     ]
    }
   ],
   "source": [
    "all_isomers <- cbind(isomer_30_final[,-1], isomer_60_final[,-(1:2)], isomer_90_final[,-(1:2)]) #combine all three voltages, plus one column of label information\n",
    "\n",
    "if(print_excel_data) {   #if desired, print the final dataset to an excel file\n",
    "write.xlsx(all_isomers, file_dataset, append = FALSE, showNA = FALSE, row.names = FALSE)\n",
    "}\n",
    "\n",
    "n_30 <- ncol(isomer_30_final) - 2   #determine number of 30V m/z ions\n",
    "n_60 <- ncol(isomer_60_final) - 2   #determine number of 60V m/z ions\n",
    "n_90 <- ncol(isomer_90_final) - 2   #determine number of 90V m/z ions\n",
    "\n",
    "print(paste0(n_30, \" 30V ions\"))    #print number of 30V m/z ions\n",
    "print(paste0(n_60, \" 60V ions\"))    #print number of 60V m/z ions\n",
    "print(paste0(n_90, \" 90V ions\"))    #print number of 90V m/z ions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add voltage information to column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'Label'</li><li>'109.050 30V'</li><li>'137.100 30V'</li><li>'154.125 30V'</li><li>'83.050 60V'</li><li>'109.050 60V'</li><li>'137.100 60V'</li><li>'154.125 60V'</li><li>'83.050 90V'</li><li>'109.050 90V'</li><li>'137.100 90V'</li><li>'154.125 90V'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Label'\n",
       "\\item '109.050 30V'\n",
       "\\item '137.100 30V'\n",
       "\\item '154.125 30V'\n",
       "\\item '83.050 60V'\n",
       "\\item '109.050 60V'\n",
       "\\item '137.100 60V'\n",
       "\\item '154.125 60V'\n",
       "\\item '83.050 90V'\n",
       "\\item '109.050 90V'\n",
       "\\item '137.100 90V'\n",
       "\\item '154.125 90V'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Label'\n",
       "2. '109.050 30V'\n",
       "3. '137.100 30V'\n",
       "4. '154.125 30V'\n",
       "5. '83.050 60V'\n",
       "6. '109.050 60V'\n",
       "7. '137.100 60V'\n",
       "8. '154.125 60V'\n",
       "9. '83.050 90V'\n",
       "10. '109.050 90V'\n",
       "11. '137.100 90V'\n",
       "12. '154.125 90V'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"Label\"       \"109.050 30V\" \"137.100 30V\" \"154.125 30V\" \"83.050 60V\" \n",
       " [6] \"109.050 60V\" \"137.100 60V\" \"154.125 60V\" \"83.050 90V\"  \"109.050 90V\"\n",
       "[11] \"137.100 90V\" \"154.125 90V\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if(length(unique(colnames(all_isomers))) < n_30+n_60+n_90+1){    #if there are duplicate column names (voltage info, not yet added)\n",
    "for (i in 1:length(colnames(all_isomers))){                  #add voltage information to column names\n",
    "  if(i>1 & i<= (1+n_30)){ #if column number is between 1 and (1+ number of 30V variables)-inclusive\n",
    "    colnames(all_isomers)[i] <- paste(colnames(all_isomers)[i],\"30V\") #add \" 30V\" to column name\n",
    "  }\n",
    "  if (i>(n_30+1) & i<= (1+ n_30 + n_60)){ #if column number is between (1+ number of 30V variables) and (1+ number of 30 and 60V variables) - inclusive\n",
    "  colnames(all_isomers)[i] <- paste(colnames(all_isomers)[i],\"60V\") #add \" 60V\" to column name\n",
    "  }\n",
    "  if (i> (1 + n_30 + n_60)){  #if column is greater than 1+ number of 30 and 60V variables\n",
    "    colnames(all_isomers)[i] <- paste(colnames(all_isomers)[i],\"90V\") #add \" 90V\" to column name\n",
    "  }\n",
    "}\n",
    "}\n",
    "\n",
    "colnames(all_isomers) #print column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectors with pertinent information:\n",
    "- Week number\n",
    "- Volume\n",
    "- Card number in a given week\n",
    "#### Assumes data to be arranged by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Week <- vector(length = nrow(all_isomers))   #create a vector with space for week information\n",
    "isomers_per_week <- nrow(all_isomers)/length(unique(Isomer))/weeks #number of each isomer analyzed per week\n",
    "    for(k in 1:length(unique(Isomer))){      #for 1- number of isomers\n",
    "        for(i in 1:weeks){                   #for number of weeks\n",
    "            start <- ((i*isomers_per_week)-(isomers_per_week-1))+((k-1)*nrow(all_isomers)/length(unique(Isomer))) #start of week\n",
    "            finish <- (i*isomers_per_week)+((k-1)*nrow(all_isomers)/length(unique(Isomer))) #end of week\n",
    "           Week[start:finish] <- rep(i, isomers_per_week)  #repeat the week number for how many samples of each isomer were analyzed each week\n",
    "        }\n",
    "}\n",
    "three <- rep(3,card/weeks/2*3) #repeat number of times volume was analyzed each week (currently 6)\n",
    "six <- rep(6,card/weeks/2*3)   #repeat number of times volume was analyzed each week (currently 6)\n",
    "Volume <- rep(rep(c(three, six),weeks),length(unique(Isomer))) #repeat each volume vector for each week\n",
    "Card<- rep(c(1,1,1,2,2,2,3,3,3,4,4,4),24)   #which card per week (4 cards each week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 18 × 15</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Isomer</th><th scope=col>Week</th><th scope=col>Card</th><th scope=col>Volume</th><th scope=col>109.050 30V</th><th scope=col>137.100 30V</th><th scope=col>154.125 30V</th><th scope=col>83.050 60V</th><th scope=col>109.050 60V</th><th scope=col>137.100 60V</th><th scope=col>154.125 60V</th><th scope=col>83.050 90V</th><th scope=col>109.050 90V</th><th scope=col>137.100 90V</th><th scope=col>154.125 90V</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>2</td><td>1</td><td>1</td><td>3</td><td>0.10377394</td><td>0.2034317</td><td>0.6927943</td><td>0.0011044946</td><td>0.5505102</td><td>0.13969553</td><td>0.3086897</td><td>0.03558691</td><td>0.8859393</td><td>0.03805406</td><td>0.04041974</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2</td><td>1</td><td>1</td><td>3</td><td>0.09282654</td><td>0.2015393</td><td>0.7056342</td><td>0.0014766927</td><td>0.6905756</td><td>0.12074936</td><td>0.1871983</td><td>0.05166330</td><td>0.8794328</td><td>0.03153022</td><td>0.03737372</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>2</td><td>1</td><td>1</td><td>3</td><td>0.06924147</td><td>0.1525347</td><td>0.7782238</td><td>0.0013871258</td><td>0.6664224</td><td>0.13034566</td><td>0.2018448</td><td>0.05076539</td><td>0.8825918</td><td>0.03015939</td><td>0.03648341</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>2</td><td>1</td><td>2</td><td>3</td><td>0.08225385</td><td>0.1783805</td><td>0.7393656</td><td>0.0008955341</td><td>0.6868864</td><td>0.12403306</td><td>0.1881850</td><td>0.05205775</td><td>0.8729540</td><td>0.03313239</td><td>0.04185584</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>2</td><td>1</td><td>2</td><td>3</td><td>0.09366285</td><td>0.1967946</td><td>0.7095425</td><td>0.0018184011</td><td>0.7527970</td><td>0.10963387</td><td>0.1357508</td><td>0.06948631</td><td>0.8737595</td><td>0.02613099</td><td>0.03062319</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>2</td><td>1</td><td>2</td><td>3</td><td>0.10254642</td><td>0.2000171</td><td>0.6974365</td><td>0.0020277601</td><td>0.7988143</td><td>0.09644624</td><td>0.1027117</td><td>0.07851480</td><td>0.8819453</td><td>0.01973995</td><td>0.01979996</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>2</td><td>1</td><td>3</td><td>6</td><td>0.11089297</td><td>0.1973157</td><td>0.6917913</td><td>0.0021335392</td><td>0.7251921</td><td>0.11052354</td><td>0.1621508</td><td>0.06872101</td><td>0.8762463</td><td>0.02636369</td><td>0.02866898</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>2</td><td>1</td><td>3</td><td>6</td><td>0.10643314</td><td>0.2018099</td><td>0.6917570</td><td>0.0019902734</td><td>0.7492397</td><td>0.10730463</td><td>0.1414654</td><td>0.06766880</td><td>0.8783433</td><td>0.02598746</td><td>0.02800041</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>2</td><td>1</td><td>3</td><td>6</td><td>0.10287388</td><td>0.1942190</td><td>0.7029071</td><td>0.0019341820</td><td>0.7646608</td><td>0.10707030</td><td>0.1263347</td><td>0.07180865</td><td>0.8804084</td><td>0.02307230</td><td>0.02471069</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>2</td><td>1</td><td>4</td><td>6</td><td>0.11798631</td><td>0.2238409</td><td>0.6581728</td><td>0.0011923270</td><td>0.6047529</td><td>0.12727670</td><td>0.2667781</td><td>0.04421735</td><td>0.8452095</td><td>0.05301763</td><td>0.05755549</td></tr>\n",
       "\t<tr><th scope=row>11</th><td>2</td><td>1</td><td>4</td><td>6</td><td>0.09297040</td><td>0.1833793</td><td>0.7236503</td><td>0.0014923410</td><td>0.7091551</td><td>0.11726804</td><td>0.1720845</td><td>0.05449621</td><td>0.8799202</td><td>0.03159382</td><td>0.03398972</td></tr>\n",
       "\t<tr><th scope=row>12</th><td>2</td><td>1</td><td>4</td><td>6</td><td>0.14354469</td><td>0.2517657</td><td>0.6046896</td><td>0.0022506465</td><td>0.7194826</td><td>0.10866904</td><td>0.1695977</td><td>0.06192256</td><td>0.8673950</td><td>0.03501266</td><td>0.03566974</td></tr>\n",
       "\t<tr><th scope=row>13</th><td>3</td><td>1</td><td>1</td><td>3</td><td>0.07941845</td><td>0.1864845</td><td>0.7340970</td><td>0.0012368111</td><td>0.4597388</td><td>0.14631192</td><td>0.3927124</td><td>0.02630643</td><td>0.8194776</td><td>0.06844605</td><td>0.08576993</td></tr>\n",
       "\t<tr><th scope=row>14</th><td>3</td><td>1</td><td>1</td><td>3</td><td>0.11302591</td><td>0.2492851</td><td>0.6376890</td><td>0.0018437514</td><td>0.6702637</td><td>0.11761600</td><td>0.2102765</td><td>0.06779491</td><td>0.8557909</td><td>0.03701363</td><td>0.03940053</td></tr>\n",
       "\t<tr><th scope=row>15</th><td>3</td><td>1</td><td>1</td><td>3</td><td>0.11113843</td><td>0.2337504</td><td>0.6551111</td><td>0.0025172414</td><td>0.7998166</td><td>0.09493105</td><td>0.1027351</td><td>0.09214071</td><td>0.8711495</td><td>0.01939035</td><td>0.01731941</td></tr>\n",
       "\t<tr><th scope=row>16</th><td>3</td><td>1</td><td>2</td><td>3</td><td>0.09769230</td><td>0.2162559</td><td>0.6860518</td><td>0.0023983380</td><td>0.7580034</td><td>0.11066717</td><td>0.1289311</td><td>0.07819416</td><td>0.8737773</td><td>0.02474616</td><td>0.02328233</td></tr>\n",
       "\t<tr><th scope=row>17</th><td>3</td><td>1</td><td>2</td><td>3</td><td>0.08362804</td><td>0.1910216</td><td>0.7253504</td><td>0.0021800810</td><td>0.7414795</td><td>0.11184333</td><td>0.1444970</td><td>0.07182886</td><td>0.8684344</td><td>0.02954998</td><td>0.03018673</td></tr>\n",
       "\t<tr><th scope=row>18</th><td>3</td><td>1</td><td>2</td><td>3</td><td>0.09774601</td><td>0.2112053</td><td>0.6910487</td><td>0.0022304312</td><td>0.6932209</td><td>0.12030503</td><td>0.1842436</td><td>0.05844647</td><td>0.8479899</td><td>0.04346218</td><td>0.05010143</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 18 × 15\n",
       "\\begin{tabular}{r|lllllllllllllll}\n",
       "  & Isomer & Week & Card & Volume & 109.050 30V & 137.100 30V & 154.125 30V & 83.050 60V & 109.050 60V & 137.100 60V & 154.125 60V & 83.050 90V & 109.050 90V & 137.100 90V & 154.125 90V\\\\\n",
       "  & <int> & <int> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 2 & 1 & 1 & 3 & 0.10377394 & 0.2034317 & 0.6927943 & 0.0011044946 & 0.5505102 & 0.13969553 & 0.3086897 & 0.03558691 & 0.8859393 & 0.03805406 & 0.04041974\\\\\n",
       "\t2 & 2 & 1 & 1 & 3 & 0.09282654 & 0.2015393 & 0.7056342 & 0.0014766927 & 0.6905756 & 0.12074936 & 0.1871983 & 0.05166330 & 0.8794328 & 0.03153022 & 0.03737372\\\\\n",
       "\t3 & 2 & 1 & 1 & 3 & 0.06924147 & 0.1525347 & 0.7782238 & 0.0013871258 & 0.6664224 & 0.13034566 & 0.2018448 & 0.05076539 & 0.8825918 & 0.03015939 & 0.03648341\\\\\n",
       "\t4 & 2 & 1 & 2 & 3 & 0.08225385 & 0.1783805 & 0.7393656 & 0.0008955341 & 0.6868864 & 0.12403306 & 0.1881850 & 0.05205775 & 0.8729540 & 0.03313239 & 0.04185584\\\\\n",
       "\t5 & 2 & 1 & 2 & 3 & 0.09366285 & 0.1967946 & 0.7095425 & 0.0018184011 & 0.7527970 & 0.10963387 & 0.1357508 & 0.06948631 & 0.8737595 & 0.02613099 & 0.03062319\\\\\n",
       "\t6 & 2 & 1 & 2 & 3 & 0.10254642 & 0.2000171 & 0.6974365 & 0.0020277601 & 0.7988143 & 0.09644624 & 0.1027117 & 0.07851480 & 0.8819453 & 0.01973995 & 0.01979996\\\\\n",
       "\t7 & 2 & 1 & 3 & 6 & 0.11089297 & 0.1973157 & 0.6917913 & 0.0021335392 & 0.7251921 & 0.11052354 & 0.1621508 & 0.06872101 & 0.8762463 & 0.02636369 & 0.02866898\\\\\n",
       "\t8 & 2 & 1 & 3 & 6 & 0.10643314 & 0.2018099 & 0.6917570 & 0.0019902734 & 0.7492397 & 0.10730463 & 0.1414654 & 0.06766880 & 0.8783433 & 0.02598746 & 0.02800041\\\\\n",
       "\t9 & 2 & 1 & 3 & 6 & 0.10287388 & 0.1942190 & 0.7029071 & 0.0019341820 & 0.7646608 & 0.10707030 & 0.1263347 & 0.07180865 & 0.8804084 & 0.02307230 & 0.02471069\\\\\n",
       "\t10 & 2 & 1 & 4 & 6 & 0.11798631 & 0.2238409 & 0.6581728 & 0.0011923270 & 0.6047529 & 0.12727670 & 0.2667781 & 0.04421735 & 0.8452095 & 0.05301763 & 0.05755549\\\\\n",
       "\t11 & 2 & 1 & 4 & 6 & 0.09297040 & 0.1833793 & 0.7236503 & 0.0014923410 & 0.7091551 & 0.11726804 & 0.1720845 & 0.05449621 & 0.8799202 & 0.03159382 & 0.03398972\\\\\n",
       "\t12 & 2 & 1 & 4 & 6 & 0.14354469 & 0.2517657 & 0.6046896 & 0.0022506465 & 0.7194826 & 0.10866904 & 0.1695977 & 0.06192256 & 0.8673950 & 0.03501266 & 0.03566974\\\\\n",
       "\t13 & 3 & 1 & 1 & 3 & 0.07941845 & 0.1864845 & 0.7340970 & 0.0012368111 & 0.4597388 & 0.14631192 & 0.3927124 & 0.02630643 & 0.8194776 & 0.06844605 & 0.08576993\\\\\n",
       "\t14 & 3 & 1 & 1 & 3 & 0.11302591 & 0.2492851 & 0.6376890 & 0.0018437514 & 0.6702637 & 0.11761600 & 0.2102765 & 0.06779491 & 0.8557909 & 0.03701363 & 0.03940053\\\\\n",
       "\t15 & 3 & 1 & 1 & 3 & 0.11113843 & 0.2337504 & 0.6551111 & 0.0025172414 & 0.7998166 & 0.09493105 & 0.1027351 & 0.09214071 & 0.8711495 & 0.01939035 & 0.01731941\\\\\n",
       "\t16 & 3 & 1 & 2 & 3 & 0.09769230 & 0.2162559 & 0.6860518 & 0.0023983380 & 0.7580034 & 0.11066717 & 0.1289311 & 0.07819416 & 0.8737773 & 0.02474616 & 0.02328233\\\\\n",
       "\t17 & 3 & 1 & 2 & 3 & 0.08362804 & 0.1910216 & 0.7253504 & 0.0021800810 & 0.7414795 & 0.11184333 & 0.1444970 & 0.07182886 & 0.8684344 & 0.02954998 & 0.03018673\\\\\n",
       "\t18 & 3 & 1 & 2 & 3 & 0.09774601 & 0.2112053 & 0.6910487 & 0.0022304312 & 0.6932209 & 0.12030503 & 0.1842436 & 0.05844647 & 0.8479899 & 0.04346218 & 0.05010143\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 18 × 15\n",
       "\n",
       "| <!--/--> | Isomer &lt;int&gt; | Week &lt;int&gt; | Card &lt;dbl&gt; | Volume &lt;dbl&gt; | 109.050 30V &lt;dbl&gt; | 137.100 30V &lt;dbl&gt; | 154.125 30V &lt;dbl&gt; | 83.050 60V &lt;dbl&gt; | 109.050 60V &lt;dbl&gt; | 137.100 60V &lt;dbl&gt; | 154.125 60V &lt;dbl&gt; | 83.050 90V &lt;dbl&gt; | 109.050 90V &lt;dbl&gt; | 137.100 90V &lt;dbl&gt; | 154.125 90V &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 2 | 1 | 1 | 3 | 0.10377394 | 0.2034317 | 0.6927943 | 0.0011044946 | 0.5505102 | 0.13969553 | 0.3086897 | 0.03558691 | 0.8859393 | 0.03805406 | 0.04041974 |\n",
       "| 2 | 2 | 1 | 1 | 3 | 0.09282654 | 0.2015393 | 0.7056342 | 0.0014766927 | 0.6905756 | 0.12074936 | 0.1871983 | 0.05166330 | 0.8794328 | 0.03153022 | 0.03737372 |\n",
       "| 3 | 2 | 1 | 1 | 3 | 0.06924147 | 0.1525347 | 0.7782238 | 0.0013871258 | 0.6664224 | 0.13034566 | 0.2018448 | 0.05076539 | 0.8825918 | 0.03015939 | 0.03648341 |\n",
       "| 4 | 2 | 1 | 2 | 3 | 0.08225385 | 0.1783805 | 0.7393656 | 0.0008955341 | 0.6868864 | 0.12403306 | 0.1881850 | 0.05205775 | 0.8729540 | 0.03313239 | 0.04185584 |\n",
       "| 5 | 2 | 1 | 2 | 3 | 0.09366285 | 0.1967946 | 0.7095425 | 0.0018184011 | 0.7527970 | 0.10963387 | 0.1357508 | 0.06948631 | 0.8737595 | 0.02613099 | 0.03062319 |\n",
       "| 6 | 2 | 1 | 2 | 3 | 0.10254642 | 0.2000171 | 0.6974365 | 0.0020277601 | 0.7988143 | 0.09644624 | 0.1027117 | 0.07851480 | 0.8819453 | 0.01973995 | 0.01979996 |\n",
       "| 7 | 2 | 1 | 3 | 6 | 0.11089297 | 0.1973157 | 0.6917913 | 0.0021335392 | 0.7251921 | 0.11052354 | 0.1621508 | 0.06872101 | 0.8762463 | 0.02636369 | 0.02866898 |\n",
       "| 8 | 2 | 1 | 3 | 6 | 0.10643314 | 0.2018099 | 0.6917570 | 0.0019902734 | 0.7492397 | 0.10730463 | 0.1414654 | 0.06766880 | 0.8783433 | 0.02598746 | 0.02800041 |\n",
       "| 9 | 2 | 1 | 3 | 6 | 0.10287388 | 0.1942190 | 0.7029071 | 0.0019341820 | 0.7646608 | 0.10707030 | 0.1263347 | 0.07180865 | 0.8804084 | 0.02307230 | 0.02471069 |\n",
       "| 10 | 2 | 1 | 4 | 6 | 0.11798631 | 0.2238409 | 0.6581728 | 0.0011923270 | 0.6047529 | 0.12727670 | 0.2667781 | 0.04421735 | 0.8452095 | 0.05301763 | 0.05755549 |\n",
       "| 11 | 2 | 1 | 4 | 6 | 0.09297040 | 0.1833793 | 0.7236503 | 0.0014923410 | 0.7091551 | 0.11726804 | 0.1720845 | 0.05449621 | 0.8799202 | 0.03159382 | 0.03398972 |\n",
       "| 12 | 2 | 1 | 4 | 6 | 0.14354469 | 0.2517657 | 0.6046896 | 0.0022506465 | 0.7194826 | 0.10866904 | 0.1695977 | 0.06192256 | 0.8673950 | 0.03501266 | 0.03566974 |\n",
       "| 13 | 3 | 1 | 1 | 3 | 0.07941845 | 0.1864845 | 0.7340970 | 0.0012368111 | 0.4597388 | 0.14631192 | 0.3927124 | 0.02630643 | 0.8194776 | 0.06844605 | 0.08576993 |\n",
       "| 14 | 3 | 1 | 1 | 3 | 0.11302591 | 0.2492851 | 0.6376890 | 0.0018437514 | 0.6702637 | 0.11761600 | 0.2102765 | 0.06779491 | 0.8557909 | 0.03701363 | 0.03940053 |\n",
       "| 15 | 3 | 1 | 1 | 3 | 0.11113843 | 0.2337504 | 0.6551111 | 0.0025172414 | 0.7998166 | 0.09493105 | 0.1027351 | 0.09214071 | 0.8711495 | 0.01939035 | 0.01731941 |\n",
       "| 16 | 3 | 1 | 2 | 3 | 0.09769230 | 0.2162559 | 0.6860518 | 0.0023983380 | 0.7580034 | 0.11066717 | 0.1289311 | 0.07819416 | 0.8737773 | 0.02474616 | 0.02328233 |\n",
       "| 17 | 3 | 1 | 2 | 3 | 0.08362804 | 0.1910216 | 0.7253504 | 0.0021800810 | 0.7414795 | 0.11184333 | 0.1444970 | 0.07182886 | 0.8684344 | 0.02954998 | 0.03018673 |\n",
       "| 18 | 3 | 1 | 2 | 3 | 0.09774601 | 0.2112053 | 0.6910487 | 0.0022304312 | 0.6932209 | 0.12030503 | 0.1842436 | 0.05844647 | 0.8479899 | 0.04346218 | 0.05010143 |\n",
       "\n"
      ],
      "text/plain": [
       "   Isomer Week Card Volume 109.050 30V 137.100 30V 154.125 30V 83.050 60V  \n",
       "1  2      1    1    3      0.10377394  0.2034317   0.6927943   0.0011044946\n",
       "2  2      1    1    3      0.09282654  0.2015393   0.7056342   0.0014766927\n",
       "3  2      1    1    3      0.06924147  0.1525347   0.7782238   0.0013871258\n",
       "4  2      1    2    3      0.08225385  0.1783805   0.7393656   0.0008955341\n",
       "5  2      1    2    3      0.09366285  0.1967946   0.7095425   0.0018184011\n",
       "6  2      1    2    3      0.10254642  0.2000171   0.6974365   0.0020277601\n",
       "7  2      1    3    6      0.11089297  0.1973157   0.6917913   0.0021335392\n",
       "8  2      1    3    6      0.10643314  0.2018099   0.6917570   0.0019902734\n",
       "9  2      1    3    6      0.10287388  0.1942190   0.7029071   0.0019341820\n",
       "10 2      1    4    6      0.11798631  0.2238409   0.6581728   0.0011923270\n",
       "11 2      1    4    6      0.09297040  0.1833793   0.7236503   0.0014923410\n",
       "12 2      1    4    6      0.14354469  0.2517657   0.6046896   0.0022506465\n",
       "13 3      1    1    3      0.07941845  0.1864845   0.7340970   0.0012368111\n",
       "14 3      1    1    3      0.11302591  0.2492851   0.6376890   0.0018437514\n",
       "15 3      1    1    3      0.11113843  0.2337504   0.6551111   0.0025172414\n",
       "16 3      1    2    3      0.09769230  0.2162559   0.6860518   0.0023983380\n",
       "17 3      1    2    3      0.08362804  0.1910216   0.7253504   0.0021800810\n",
       "18 3      1    2    3      0.09774601  0.2112053   0.6910487   0.0022304312\n",
       "   109.050 60V 137.100 60V 154.125 60V 83.050 90V 109.050 90V 137.100 90V\n",
       "1  0.5505102   0.13969553  0.3086897   0.03558691 0.8859393   0.03805406 \n",
       "2  0.6905756   0.12074936  0.1871983   0.05166330 0.8794328   0.03153022 \n",
       "3  0.6664224   0.13034566  0.2018448   0.05076539 0.8825918   0.03015939 \n",
       "4  0.6868864   0.12403306  0.1881850   0.05205775 0.8729540   0.03313239 \n",
       "5  0.7527970   0.10963387  0.1357508   0.06948631 0.8737595   0.02613099 \n",
       "6  0.7988143   0.09644624  0.1027117   0.07851480 0.8819453   0.01973995 \n",
       "7  0.7251921   0.11052354  0.1621508   0.06872101 0.8762463   0.02636369 \n",
       "8  0.7492397   0.10730463  0.1414654   0.06766880 0.8783433   0.02598746 \n",
       "9  0.7646608   0.10707030  0.1263347   0.07180865 0.8804084   0.02307230 \n",
       "10 0.6047529   0.12727670  0.2667781   0.04421735 0.8452095   0.05301763 \n",
       "11 0.7091551   0.11726804  0.1720845   0.05449621 0.8799202   0.03159382 \n",
       "12 0.7194826   0.10866904  0.1695977   0.06192256 0.8673950   0.03501266 \n",
       "13 0.4597388   0.14631192  0.3927124   0.02630643 0.8194776   0.06844605 \n",
       "14 0.6702637   0.11761600  0.2102765   0.06779491 0.8557909   0.03701363 \n",
       "15 0.7998166   0.09493105  0.1027351   0.09214071 0.8711495   0.01939035 \n",
       "16 0.7580034   0.11066717  0.1289311   0.07819416 0.8737773   0.02474616 \n",
       "17 0.7414795   0.11184333  0.1444970   0.07182886 0.8684344   0.02954998 \n",
       "18 0.6932209   0.12030503  0.1842436   0.05844647 0.8479899   0.04346218 \n",
       "   154.125 90V\n",
       "1  0.04041974 \n",
       "2  0.03737372 \n",
       "3  0.03648341 \n",
       "4  0.04185584 \n",
       "5  0.03062319 \n",
       "6  0.01979996 \n",
       "7  0.02866898 \n",
       "8  0.02800041 \n",
       "9  0.02471069 \n",
       "10 0.05755549 \n",
       "11 0.03398972 \n",
       "12 0.03566974 \n",
       "13 0.08576993 \n",
       "14 0.03940053 \n",
       "15 0.01731941 \n",
       "16 0.02328233 \n",
       "17 0.03018673 \n",
       "18 0.05010143 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_isomers <- arrange(all_isomers, Label)      #arrange by label  \n",
    "my_data <- all_isomers[,-1]                     #all data except labels\n",
    "Isomer <- all_isomers[,1]                       #labels\n",
    "my_data_normal <- normalize(my_data, normal)    #use normalization function to normalize the data\n",
    "my_data_test <- cbind(Isomer, my_data_normal)   #create a dataset with normalized data and label information\n",
    "my_data_anova <- cbind(as.factor(Isomer), as.factor(Week), as.factor(Volume), my_data_normal)  #create a dataset for ANOVA testing and week-to-week plotting\n",
    "colnames(my_data_anova) <- c(\"Isomer\", \"Week\", \"Volume\", colnames(my_data_anova[,-(1:3)])) #create column names for ANOVA dataset\n",
    "my_data_sameday_byweek <- arrange(cbind(Isomer, Week, Card, Volume, my_data_normal), Week) #create a dataset for same day testing, and arrange by week \n",
    "\n",
    "head(my_data_sameday_byweek, n=18)              #show first 18 rows of sameday testing dataset, to check order of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA testing\n",
    "- Save as excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(anova){ #perform anova if requested\n",
    " \n",
    "result<-matrix(nrow = ncol(my_data_normal), ncol = 7) #create a matrix to store results\n",
    "   \n",
    "    colnames(result) <- c(\"Isomer\", \"Week\", \"Volume\", \"Isomer:Week\", \"Isomer:Volume\", \"Week:Volume\", \"Isomer:Week:Volume\") #column names for the various ANOVA results\n",
    "    rownames(result) <- colnames(my_data_normal) #row names are the m/z variables\n",
    "    for(i in 1:ncol(my_data_normal)){ #for each m/z variables perform the anova \n",
    "        result[i,] <- summary(aov(my_data_anova[,i+3]~ Isomer * Week * Volume, data = my_data_anova))[[1]][[\"Pr(>F)\"]][1:7] #store the p-values from each ANOVA in the appropriate spot in the results matrix\n",
    "    }\n",
    "    \n",
    "name = paste0(type, \" \", ion_threshold, \"  \", normal) #give sheet names based on isomer and normalization information\n",
    "write.xlsx(result, file_anova, sheetName = name, append = TRUE, showNA = FALSE, row.names = TRUE,col.names = TRUE) #save to excel\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week to week plots\n",
    "- Color coded by Isomer\n",
    "- Standard Deviation error bars\n",
    "- Save as pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(week_plots){ #if creating week-to-week plots\n",
    "    \n",
    "list_of_weekly_plots <- vector(\"list\", length = ncol(my_data_normal)) #create a list of plots to be filled in \n",
    "plot_data <- matrix(nrow = length(unique(Isomer))*weeks, ncol = 4) #create empty plot data matrix, with four columns and 1 row per isomer per week\n",
    "colnames(plot_data) <- c(\"Isomer\", \"Week\", \"Mean\", \"sd\") #set column names\n",
    "\n",
    "for (i in 1:ncol(my_data_normal)) {             #each m/z variable will have a different plot\n",
    "  for (j in 1:length(unique(Isomer))) {         #for each isomer\n",
    "    for(k in 1:weeks){                          #for each week\n",
    "      plot_data[((weeks*j)-(weeks-1)):(weeks*j),1] <- paste0(as.integer(unique(Isomer)[j])+1,\"-\",type) #put isomer information in column 1, one per week, to be used as legend info\n",
    "      plot_data[(((weeks*j)-weeks) + k),2] <- unique(Week)[k] #put week information in column 2, sequentially 1 through number of weeks, repeated for number of isomers\n",
    "      test<-my_data_anova[,c(1:2,(i+3))] %>%    #create dataset of isomer, week, and selected m/z variable\n",
    "        filter(Isomer==unique(Isomer)[j]) %>%   #filter dataset based on only isomer of current interest                                                          \n",
    "        filter(Week==unique(Week)[k])           #filter remaining data by week\n",
    "      plot_data[(((weeks*j)-weeks) + k),3] <- mean(test[,3]) #insert mean of given m/z for that isomer for that week into the appropriate index of the plot_data matrix\n",
    "      plot_data[(((weeks*j)-weeks) + k),4] <- sd(test[,3])   #insert sd of given m/z for that isomer for that week into the appropriate index of the plot_data matrix\n",
    "    }\n",
    "  }\n",
    "    \n",
    "plot_data <- as.data.frame(plot_data) #format as data frame \n",
    "    plot_data[,3] <- as.numeric(plot_data[,3]) #format mean column as numeric\n",
    "    plot_data[,4] <- as.numeric(plot_data[,4]) #format sd column as numeric\n",
    "graphy <- ggplot(data = plot_data, aes(x = Week, y = Mean, group = Isomer, color = Isomer))  + #create ggplot, with Week as x axis and mean as y axis, group by and color by isomer\n",
    "  geom_errorbar(aes(ymin=Mean-sd, ymax=Mean+sd), width=.1, position=position_dodge(0.05)) + #add std dev as error bars, width of 0.1 and slightly off from one another to be visible\n",
    "    geom_line() + #connect points with line\n",
    "      geom_point() + #show data points as points\n",
    "        labs(title = colnames(my_data_anova)[i+3]) + #title plots with m/z information\n",
    "        theme(panel.background = element_rect(fill = \"white\")) +  #white plot background\n",
    "        theme(plot.background = element_rect(color = \"black\")) +  #separate with black borders\n",
    "        theme(text = element_text(size = 7)) +  #text size 7\n",
    "        theme(legend.key.size = unit(7,\"pt\"))   #legend text size 7\n",
    "    \n",
    "  list_of_weekly_plots[[i]] <- graphy #save plot in list of plots\n",
    "  }\n",
    "\n",
    "ggexport(plotlist = list_of_weekly_plots, ncol = 2, nrow = 3, filename = file_weekly) #export to file name set above, arrange in 2 columns, 3 rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growing dataset over time plots\n",
    "- Color coded by Isomer\n",
    "- Standard Deviation error bars\n",
    "- Save as pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(growing_plots){  #if set to create plots based on adding weeks to dataset\n",
    "\n",
    "list_of_growing_plots <- vector(\"list\", length = ncol(my_data_normal)) #create an empty list to store plots\n",
    "plot_data <- matrix(nrow = length(unique(Isomer))*length(unique(Week)), ncol = 4) #create empty plot_data matrix\n",
    "colnames(plot_data) <- c(\"Isomer\", \"Week\", \"Mean\", \"sd\")  #set column names\n",
    "\n",
    "for (i in 1:ncol(my_data_normal)) { #each m/z variable will have a different plot\n",
    "  for (j in 1:length(unique(Isomer))) {     #for each isomer\n",
    "    for(k in 1:weeks){ #for each week\n",
    "      plot_data[((weeks*j)-(weeks-1)):(weeks*j),1] <- paste0(as.integer(unique(Isomer)[j])+1,\"-\",type) #put isomer information in column 1, one per week, to be used as legend info\n",
    "      plot_data[(((weeks*j)-weeks)+ k),2] <- paste(\"1-\", unique(Week)[k]) #put week information in column 2, sequentially 1-1 through 1-# of weeks, repeated for number of isomers\n",
    "      test<-my_data_anova[,c(1:2,(i+3))] %>% #create data of only isomer info, week info, and selected m/z variable\n",
    "        filter(Isomer==unique(Isomer)[j]) %>%  #filter dataset based on only isomer of current interest\n",
    "        filter(Week %in% unique(Week)[1:k]) #filter remaining data by whether in selected combination of weeks\n",
    "      plot_data[(((weeks*j)-weeks) + k),3] <- mean(test[,3]) #insert mean of given m/z for that isomer for that set of weeks into the appropriate index of the plot_data matrix\n",
    "      plot_data[(((weeks*j)-weeks) + k),4] <- sd(test[,3]) #insert sd of given m/z for that isomer for that set of weeks into the appropriate index of the plot_data matrix\n",
    "    }\n",
    "  }\n",
    "plot_data <- as.data.frame(plot_data) #format as data frame \n",
    "    plot_data[,3] <- as.numeric(plot_data[,3]) #format mean column as numeric\n",
    "    plot_data[,4] <- as.numeric(plot_data[,4]) #format sd column as numeric\n",
    "graphy <- ggplot(data = plot_data, aes(x = Week, y = Mean, group = Isomer, color = Isomer)) + #create ggplot, with Week as x axis and mean as y axis, group by and color by isomer\n",
    "  geom_errorbar(aes(ymin=Mean-sd, ymax=Mean+sd), width=.1, position=position_dodge(0.05)) + #add std dev as error bars, width of 0.1 and slightly off from one another to be visible\n",
    "    geom_line() + #connect points with line\n",
    "      geom_point() + #show datapoints as points \n",
    "        labs(title = colnames(my_data_anova)[i+3]) + #title plots with m/z information\n",
    "        theme(panel.background = element_rect(fill = \"white\")) + #white plot background\n",
    "        theme(plot.background = element_rect(color = \"black\")) + #separate plots with black border\n",
    "        theme(text = element_text(size = 7)) + #set text size\n",
    "        theme(legend.key.size = unit(7,\"pt\"))  #set legend text size\n",
    "    \n",
    "  list_of_growing_plots[[i]] <- graphy #save plot in list of plots\n",
    "  }\n",
    "\n",
    "ggexport(plotlist = list_of_growing_plots, ncol = 2, nrow = 3, filename = file_growing) #export to file name set above, arrange in 2 columns, 3 rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up empty matrices for Plotting Comparison of different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (compare_plot){  #if plotting comparison of results for different methods \n",
    "    compare_plot_data <- matrix(0, nrow = 90, ncol = 4)   #create matrix for plotting data, rows for each method * each result type * # of threshold\n",
    "    colnames(compare_plot_data) <- c(\"Method\", \"Threshold\", \"Result\", \"Rate\")  #create column names\n",
    "    compare_plot_data[,1] <- c(rep(\"t-test\", 30), rep(\"LDA\", 30), rep(\"Random Forest\", 30))  #fill in method column\n",
    "    compare_plot_data[,2] <- rep(c(1,2,3,4,5,6,7,8,9,10), 9) #fill in threshold column (thresholds are difference in # of significant ions for t-test, posterior probability for LDA, and proportion of trees for RF)\n",
    "    compare_plot_data[,3] <- rep(c(rep(\"Success\", 10),rep(\"Inconclusive\", 10), rep(\"Error\", 10)),3) #fill in Result column\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test analysis\n",
    "- Including same day, if selected\n",
    "- Randomly selected normalization/confidence\n",
    "- Performs selected number of replicate analyses\n",
    "- Creates individual ROC curves based on number of statistically indistinguishable ions\n",
    "- Creates average ROC curve for all replicate analyses\n",
    "- Creates table of success/inconclusive/error rates based on isomer to isomer comparison (need to add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ttest){ #performs replicate t-tests\n",
    "original_normal <- normal #saves normalization technique so it can be reverted to after replicates\n",
    "  dim.names <- list(unique(card.names),c(colnames(all_isomers[,-1]),\"Sum\")) #makes list of 1 - card names, 2 - fragment names\n",
    "  result_2_2 <- matrix(nrow = card, ncol = ncol(my_data)+1, dimnames = dim.names) #creates matrix to fill in and compare 2-unk to 2-knowns\n",
    "  result_2_3 <- matrix(nrow = card, ncol = ncol(my_data)+1, dimnames = dim.names) #compare 2-unk to 3-knowns\n",
    "  result_2_4 <- matrix(nrow = card, ncol = ncol(my_data)+1, dimnames = dim.names) #2-unk to 4-knowns\n",
    "  result_3_2 <- matrix(nrow = card, ncol = ncol(my_data)+1, dimnames = dim.names) #3-unk to 2 knowns\n",
    "  result_3_3 <- matrix(nrow = card, ncol = ncol(my_data)+1, dimnames = dim.names) #3-unk to 3 knowns\n",
    "  result_3_4 <- matrix(nrow = card, ncol = ncol(my_data)+1, dimnames = dim.names) #3-unk to 4 knowns\n",
    "  result_4_2 <- matrix(nrow = card, ncol = ncol(my_data)+1, dimnames = dim.names) #4-unk to 2 knowns\n",
    "  result_4_3 <- matrix(nrow = card, ncol = ncol(my_data)+1, dimnames = dim.names) #4-unk to 3 knowns\n",
    "  result_4_4 <- matrix(nrow = card, ncol = ncol(my_data)+1, dimnames = dim.names) #4-unk to 4 knowns\n",
    "  \n",
    "index <- 1 #set starting index\n",
    "list_of_ttest_plots <- vector(\"list\", length = rep_num)  #create list of plots \n",
    "average_FPR <- matrix(nrow= rep_num, ncol = ncol(my_data)) #create empty matrix to fill in with false positive rate info for each replicate\n",
    "average_TPR <- matrix(nrow= rep_num, ncol = ncol(my_data))  #create empty matrix to fill in with true positive rate info for each replicate\n",
    "success_matrix <- matrix(nrow = nrow(my_data)/length(unique(Isomer)), ncol = 10) #empty matrix for storing success rate information\n",
    "inconclusive_matrix <- matrix(nrow = nrow(my_data)/length(unique(Isomer)), ncol = 10) #empty matrix for storing inconclusive rate information\n",
    "error_matrix <- matrix(nrow = nrow(my_data)/length(unique(Isomer)), ncol = 10)    #empty matrix for storing error rate information\n",
    "average_threshold_success <- matrix(nrow = rep_num, ncol = 10)    #empty matrix for storing success rate information per replicate to be averaged \n",
    "average_threshold_inconclusive <- matrix(nrow = rep_num, ncol = 10) #empty matrix for storing inconclusive rate information per replicate to be averaged \n",
    "average_threshold_error <- matrix(nrow = rep_num, ncol = 10) #empty matrix for storing error rate information per replicate to be averaged \n",
    "    \n",
    "if(compare_methods){ #if comparing variables importance per method\n",
    "    compare_matrix <- matrix(0, nrow = 4, ncol = ncol(my_data)) #create empty matrix with one row per \"method\" (both LD functions shown separately)\n",
    "    colnames(compare_matrix) <- colnames(my_data) #set column names to match included m/z variables\n",
    "    average_accuracy <- matrix(0, nrow = rep_num, ncol = ncol(my_data)) #create empty matrix to store variables importance to be averaged\n",
    "}\n",
    "    \n",
    "normal_vector <- vector(length = rep_num) #create empty vector for storing normalization information per replicate\n",
    "    \n",
    "while (index <= rep_num) { #perform the t-test for the selected number of replicates (20)\n",
    "  confidence <- round(runif(1,90,99.999),3) #selects random confidence limit\n",
    "  norm_int <- sample(1:2, 1)                #selects random normalization type\n",
    "  if(norm_int == 1){                        #if randomly selected 1\n",
    "    normal <- \"ioncurrent\"                  #use ion current normalization\n",
    "  }\n",
    "  if(norm_int == 2){                        #if randomly selected 2\n",
    "    normal <- \"vectorlength\"                #use vector length normalization\n",
    "  }\n",
    "\n",
    "my_data_normal <- normalize(my_data, normal) #normalize using selected method\n",
    "  \n",
    "Isomer2_normal <- my_data_normal[which(Isomer==2),] #isolate only Isomer 2\n",
    "Isomer3_normal <- my_data_normal[which(Isomer==3),] #isolate only Isomer 3\n",
    "Isomer4_normal <- my_data_normal[which(Isomer==4),] #isolate only Isomer 4\n",
    "  \n",
    "if(!sameday){ #if not performing a same day analysis \n",
    "  for (i in 1:card) { #for each card in all dataset (set above, note difference in capitilization from Card (vector of card info))\n",
    "    start <- (3*i)-2  #assumes triplicate data per card, sets starting point of card\n",
    "    finish <- 3*i     #assumes triplicate data per card, sets ending point of card\n",
    "    \n",
    "    test_2Isomer <- Isomer2_normal[start:finish,]      #take one card out at a time for isomer 2 test set\n",
    "    train_2Isomer <- Isomer2_normal[-(start:finish),]  #remaining data is training set\n",
    "    test_3Isomer <- Isomer3_normal[start:finish,]      #take one card out at a time for isomer 3 test set\n",
    "    train_3Isomer <- Isomer3_normal[-(start:finish),]  #remaining data is training set\n",
    "    test_4Isomer <- Isomer4_normal[start:finish,]      #take one card out at a time for isomer 4 test set\n",
    "    train_4Isomer <- Isomer4_normal[-(start:finish),]  #remaining data is training set\n",
    "    \n",
    "    for(l in 1:ncol(Isomer2_normal)) { #for each m/z variable\n",
    "      res_2_2 <- t.test(test_2Isomer[,l],train_2Isomer[,l])  #compare held out Isomer 2 to remaining Isomer 2 data\n",
    "      res_2_3 <- t.test(test_2Isomer[,l],train_3Isomer[,l])  #compare held out Isomer 2 to remaining Isomer 3 data \n",
    "      res_2_4 <- t.test(test_2Isomer[,l],train_4Isomer[,l])  #compare held out Isomer 2 to remaining Isomer 4 data \n",
    "      res_3_2 <- t.test(test_3Isomer[,l],train_2Isomer[,l])  #compare held out Isomer 3 to remaining Isomer 2 data \n",
    "      res_3_3 <- t.test(test_3Isomer[,l],train_3Isomer[,l])  #compare held out Isomer 3 to remaining Isomer 3 data \n",
    "      res_3_4 <- t.test(test_3Isomer[,l],train_4Isomer[,l])  #compare held out Isomer 3 to remaining Isomer 4 data \n",
    "      res_4_2 <- t.test(test_4Isomer[,l],train_2Isomer[,l])  #compare held out Isomer 4 to remaining Isomer 2 data \n",
    "      res_4_3 <- t.test(test_4Isomer[,l],train_3Isomer[,l])  #compare held out Isomer 4 to remaining Isomer 3 data \n",
    "      res_4_4 <- t.test(test_4Isomer[,l],train_4Isomer[,l])  #compare held out Isomer 4 to remaining Isomer 4 data \n",
    "      \n",
    "      if (res_2_2$p.value < (1 - confidence/100)) {         #check if p-value is less than alpha for randomly chosen confidence\n",
    "        result_2_2[i,l] <- 0                                #put 0 in result vector if statistically distinguishable\n",
    "      }\n",
    "      else {\n",
    "        result_2_2[i,l] <- 1                                #put 1 in result vector if statistically INdistinguishable\n",
    "      }\n",
    "      \n",
    "      if (res_2_3$p.value < (1 - confidence/100)){          #check if p-value is less than alpha for randomly chosen confidence\n",
    "        result_2_3[i,l] <- 0                                #put 0 in result vector if statistically distinguishable\n",
    "      }\n",
    "      else {\n",
    "        result_2_3[i,l] <- 1                                #put 1 in result vector if statistically INdistinguishable\n",
    "      }\n",
    "      \n",
    "      if (res_2_4$p.value < (1 - confidence/100)){          #check if p-value is less than alpha for randomly chosen confidence\n",
    "        result_2_4[i,l] <- 0                                #put 0 in result vector if statistically distinguishable\n",
    "      }\n",
    "      else {\n",
    "        result_2_4[i,l] <- 1                                #put 1 in result vector if statistically INdistinguishable\n",
    "      }\n",
    "      \n",
    "      if (res_3_2$p.value < (1 - confidence/100)){          #check if p-value is less than alpha for randomly chosen confidence\n",
    "        result_3_2[i,l] <- 0                                #put 0 in result vector if statistically distinguishable\n",
    "      }\n",
    "      else {\n",
    "        result_3_2[i,l] <- 1                                #put 1 in result vector if statistically INdistinguishable\n",
    "      }\n",
    "      \n",
    "      if (res_3_3$p.value < (1 - confidence/100)){          #check if p-value is less than alpha for randomly chosen confidence\n",
    "        result_3_3[i,l] <- 0                                #put 0 in result vector if statistically distinguishable\n",
    "      }\n",
    "      else {\n",
    "        result_3_3[i,l] <- 1                                #put 1 in result vector if statistically INdistinguishable\n",
    "      }\n",
    "      \n",
    "      if (res_3_4$p.value < (1 - confidence/100)){          #check if p-value is less than alpha for randomly chosen confidence\n",
    "        result_3_4[i,l] <- 0                                #put 0 in result vector if statistically distinguishable\n",
    "      }\n",
    "      else {\n",
    "        result_3_4[i,l] <- 1                                #put 1 in result vector if statistically INdistinguishable\n",
    "      }\n",
    "      \n",
    "      if (res_4_2$p.value < (1 - confidence/100)){          #check if p-value is less than alpha for randomly chosen confidence\n",
    "        result_4_2[i,l] <- 0                                #put 0 in result vector if statistically distinguishable\n",
    "      }\n",
    "      else {\n",
    "        result_4_2[i,l] <- 1                                #put 1 in result vector if statistically INdistinguishable\n",
    "      }\n",
    "      \n",
    "      if (res_4_3$p.value < (1 - confidence/100)){          #check if p-value is less than alpha for randomly chosen confidence\n",
    "        result_4_3[i,l] <- 0                                #put 0 in result vector if statistically distinguishable\n",
    "      }\n",
    "      else {\n",
    "        result_4_3[i,l] <- 1                                #put 1 in result vector if statistically INdistinguishable\n",
    "      }\n",
    "      \n",
    "      if (res_4_4$p.value < (1 - confidence/100)){          #check if p-value is less than alpha for randomly chosen confidence\n",
    "        result_4_4[i,l] <- 0                                #put 0 in result vector if statistically distinguishable\n",
    "      }\n",
    "      else {\n",
    "        result_4_4[i,l] <- 1                                #put 1 in result vector if statistically INdistinguishable\n",
    "      }     \n",
    "    }\n",
    "    result_2_2[i,ncol(Isomer2_normal)+1] <- sum(result_2_2[i,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_2_3[i,ncol(Isomer2_normal)+1] <- sum(result_2_3[i,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_2_4[i,ncol(Isomer2_normal)+1] <- sum(result_2_4[i,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_3_2[i,ncol(Isomer2_normal)+1] <- sum(result_3_2[i,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_3_3[i,ncol(Isomer2_normal)+1] <- sum(result_3_3[i,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_3_4[i,ncol(Isomer2_normal)+1] <- sum(result_3_4[i,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_4_2[i,ncol(Isomer2_normal)+1] <- sum(result_4_2[i,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_4_3[i,ncol(Isomer2_normal)+1] <- sum(result_4_3[i,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_4_4[i,ncol(Isomer2_normal)+1] <- sum(result_4_4[i,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "      \n",
    "    test_vector_2 <- c(result_2_2[i,ncol(Isomer2_normal)+1], result_2_3[i,ncol(Isomer2_normal)+1], result_2_4[i,ncol(Isomer2_normal)+1])   #create vector of results when isomer 2 is the test set, sum of indistinguishable ions for 2 vs 2, 2 vs 3, and 2 vs 4 (correct ID always in first position of vector)\n",
    "    test_vector_3 <- c(result_3_3[i,ncol(Isomer2_normal)+1], result_3_2[i,ncol(Isomer2_normal)+1], result_3_4[i,ncol(Isomer2_normal)+1])   #create vector of results when isomer 3 is the test set, sum of indistinguishable ions for 3 vs 3, 3 vs 2, and 3 vs 4 (correct ID always in first position of vector)\n",
    "    test_vector_4 <- c(result_4_4[i,ncol(Isomer2_normal)+1], result_4_3[i,ncol(Isomer2_normal)+1], result_4_2[i,ncol(Isomer2_normal)+1])   #create vector of results when isomer 4 is the test set, sum of indistinguishable ions for 4 vs 4, 4 vs 2, and 4 vs 3 (correct ID always in first position of vector)\n",
    "    \n",
    "#testing if sum of indistinguishable ions for correct ID is higher than for incorrect ID by given thresholds (up to 10 higher)      \n",
    "      for(k in 1: 10) {    #for each threshold\n",
    "         if(max(test_vector_2) >= sort(test_vector_2, partial=2)[2] + k) {  #test if the highest value in test_vector_2 is greater than or equal to the second highest value plus the k threshold\n",
    "             #row of matrices are three rows for every variable (one for each isomer), columns are the threshold\n",
    "             inconclusive_matrix[i*length(unique(Isomer))-2,k] <- 0#if the threshold is met, the result is not inconclusive and the next test is performed\n",
    "             if(test_vector_2[1]==max(test_vector_2)){             #if the correct ID is also the highest value\n",
    "                 success_matrix[i*length(unique(Isomer))-2,k] <- 1 #the result is a success\n",
    "                 error_matrix[i*length(unique(Isomer))-2,k] <- 0   #the result is not an error\n",
    "            } else {                                               #if the correct ID is not the highest value\n",
    "                success_matrix[i*length(unique(Isomer))-2,k] <- 0  #the result is not a success\n",
    "                error_matrix[i*length(unique(Isomer))-2,k] <- 1    #the result is an error\n",
    "            }\n",
    "        } else {                                                   #if the threshold is not met\n",
    "            inconclusive_matrix[i*length(unique(Isomer))-2,k] <- 1 #the result is inconclusive\n",
    "            success_matrix[i*length(unique(Isomer))-2,k] <- 0      #the result is not a success\n",
    "            error_matrix[i*length(unique(Isomer))-2,k] <- 0        #the result is not an error\n",
    "         }\n",
    "       \n",
    "        if(max(test_vector_3) >= sort(test_vector_3, partial=2)[2] + k) {  #test if the highest value in test_vector_3 is greater than or equal to the second highest value plus the k threshold\n",
    "             inconclusive_matrix[i*length(unique(Isomer))-1,k] <- 0#if the threshold is met, the result is not inconclusive and the next test is performed\n",
    "             if(test_vector_3[1]==max(test_vector_3)){             #if the correct ID is also the highest value\n",
    "                 success_matrix[i*length(unique(Isomer))-1,k] <- 1 #the result is a success\n",
    "                 error_matrix[i*length(unique(Isomer))-1,k] <- 0   #the result is not an error\n",
    "            } else {                                               #if the correct ID is not the highest value                                                   #if the threshold is not met\n",
    "                success_matrix[i*length(unique(Isomer))-1,k] <- 0  #the result is not a success\n",
    "                error_matrix[i*length(unique(Isomer))-1,k] <- 1    #the result is an error\n",
    "            }\n",
    "        } else {                                                   #if the threshold is not met\n",
    "            inconclusive_matrix[i*length(unique(Isomer))-1,k] <- 1 #the result is inconclusive \n",
    "            success_matrix[i*length(unique(Isomer))-1,k] <- 0      #the result is not a success\n",
    "            error_matrix[i*length(unique(Isomer))-1,k] <- 0        #the result is not an error \n",
    "         }\n",
    "          \n",
    "        if(max(test_vector_4) >= sort(test_vector_4, partial=2)[2] + k) {  #test if the highest value in test_vector_3 is greater than or equal to the second highest value plus the k threshold\n",
    "             inconclusive_matrix[i*length(unique(Isomer)),k] <- 0  #if the threshold is met, the result is not inconclusive and the next test is performed\n",
    "             if(test_vector_4[1]==max(test_vector_4)){             #if the correct ID is also the highest value\n",
    "                 success_matrix[i*length(unique(Isomer)),k] <- 1   #the result is a success\n",
    "                 error_matrix[i*length(unique(Isomer)),k] <- 0     #the result is not an error\n",
    "            } else {                                               #if the correct ID is not the highest value\n",
    "                success_matrix[i*length(unique(Isomer)),k] <- 0    #the result is not a success\n",
    "                error_matrix[i*length(unique(Isomer)),k] <- 1      #the result is an error\n",
    "            }\n",
    "        } else {                                                   #if the threshold is not met\n",
    "            inconclusive_matrix[i*length(unique(Isomer)),k] <- 1   #the result is inconclusive \n",
    "            success_matrix[i*length(unique(Isomer)),k] <- 0        #the result is not a success\n",
    "            error_matrix[i*length(unique(Isomer)),k] <- 0          #the result is not an error\n",
    "         } \n",
    "        }      \n",
    "    }\n",
    "} \n",
    "      \n",
    "if(sameday) {                                                      #if sameday analysis\n",
    "  for (i in 1:weeks) {                                             #for each week\n",
    "    start <- (nrow(Isomer2_normal)/weeks*i)-(nrow(Isomer2_normal)/weeks-1) #starting row of each week\n",
    "    finish <- (nrow(Isomer2_normal)/weeks*i)                       #ending row of each week\n",
    "    Isomer2_weeks <- Isomer2_normal[start:finish,]                 #isolate by week\n",
    "    Isomer3_weeks <- Isomer3_normal[start:finish,]                 #isolate by week\n",
    "    Isomer4_weeks <- Isomer4_normal[start:finish,]                 #isolate by week\n",
    "    \n",
    "    for (p in 1:(card/weeks)){                                     #for number of cards per week\n",
    "      start_week <- (3*p)-2                                        #hold out one card at a time, compare to other cards from same day\n",
    "      finish_week <- (3*p)                                         #ending row of card (assumes triplciate)\n",
    "      row <- (i*4)+p-4                                             #row in results matrices\n",
    "      test_2Isomer <- Isomer2_weeks[start_week:finish_week,]       #take one card out at a time for isomer 2 test set\n",
    "      train_2Isomer <- Isomer2_weeks[-(start_week:finish_week),]   #remaining data is training set\n",
    "      test_3Isomer <- Isomer3_weeks[start_week:finish_week,]       #take one card out at a time for isomer 3 test set \n",
    "      train_3Isomer <- Isomer3_weeks[-(start_week:finish_week),]   #remaining data is training set\n",
    "      test_4Isomer <- Isomer4_weeks[start_week:finish_week,]       #take one card out at a time for isomer 4 test set \n",
    "      train_4Isomer <- Isomer4_weeks[-(start_week:finish_week),]   #remaining data is training set\n",
    "      \n",
    "      for(l in 1:ncol(Isomer2_weeks)) {                            #for each m/z variable\n",
    "        res_2_2 <- t.test(test_2Isomer[,l],train_2Isomer[,l])      #compare held out Isomer 2 to remaining Isomer 2 data\n",
    "        res_2_3 <- t.test(test_2Isomer[,l],train_3Isomer[,l])      #compare held out Isomer 2 to remaining Isomer 3 data \n",
    "        res_2_4 <- t.test(test_2Isomer[,l],train_4Isomer[,l])      #compare held out Isomer 2 to remaining Isomer 4 data \n",
    "        res_3_2 <- t.test(test_3Isomer[,l],train_2Isomer[,l])      #compare held out Isomer 3 to remaining Isomer 2 data\n",
    "        res_3_3 <- t.test(test_3Isomer[,l],train_3Isomer[,l])      #compare held out Isomer 3 to remaining Isomer 3 data \n",
    "        res_3_4 <- t.test(test_3Isomer[,l],train_4Isomer[,l])      #compare held out Isomer 3 to remaining Isomer 4 data \n",
    "        res_4_2 <- t.test(test_4Isomer[,l],train_2Isomer[,l])      #compare held out Isomer 4 to remaining Isomer 2 data\n",
    "        res_4_3 <- t.test(test_4Isomer[,l],train_3Isomer[,l])      #compare held out Isomer 4 to remaining Isomer 3 data \n",
    "        res_4_4 <- t.test(test_4Isomer[,l],train_4Isomer[,l])      #compare held out Isomer 4 to remaining Isomer 4 data \n",
    "        \n",
    "        if (res_2_2$p.value < (1 - confidence/100)) {              #check if p-value is less than alpha for randomly chosen confidence\n",
    "          result_2_2[row,l] <- 0                                   #put 0 in result vector if statistically distinguishable\n",
    "        }\n",
    "        else {\n",
    "          result_2_2[row,l] <- 1                                   #put 1 in result vector if statistically INdistinguishable\n",
    "        }\n",
    "        \n",
    "        if (res_2_3$p.value < (1 - confidence/100)){               #check if p-value is less than alpha for randomly chosen confidence\n",
    "          result_2_3[row,l] <- 0                                   #put 0 in result vector if statistically distinguishable\n",
    "        }\n",
    "        else {\n",
    "          result_2_3[row,l] <- 1                                   #put 1 in result vector if statistically INdistinguishable 1\n",
    "        }\n",
    "        \n",
    "        if (res_2_4$p.value < (1 - confidence/100)){               #check if p-value is less than alpha for randomly chosen confidence\n",
    "          result_2_4[row,l] <- 0                                   #put 0 in result vector if statistically distinguishable\n",
    "        }\n",
    "        else {\n",
    "          result_2_4[row,l] <- 1                                   #put 1 in result vector if statistically INdistinguishable1\n",
    "        }\n",
    "        \n",
    "        if (res_3_2$p.value < (1 - confidence/100)){               #check if p-value is less than alpha for randomly chosen confidence\n",
    "          result_3_2[row,l] <- 0                                   #put 0 in result vector if statistically distinguishable\n",
    "        }\n",
    "        else {\n",
    "          result_3_2[row,l] <- 1                                   #put 1 in result vector if statistically INdistinguishable1\n",
    "        }\n",
    "        \n",
    "        if (res_3_3$p.value < (1 - confidence/100)){               #check if p-value is less than alpha for randomly chosen confidence\n",
    "          result_3_3[row,l] <- 0                                   #put 0 in result vector if statistically distinguishable\n",
    "        }\n",
    "        else {\n",
    "          result_3_3[row,l] <- 1                                   #put 1 in result vector if statistically INdistinguishable1\n",
    "        }\n",
    "        \n",
    "        if (res_3_4$p.value < (1 - confidence/100)){               #check if p-value is less than alpha for randomly chosen confidence\n",
    "          result_3_4[row,l] <- 0                                   #put 0 in result vector if statistically distinguishable\n",
    "        }\n",
    "        else {\n",
    "          result_3_4[row,l] <- 1                                   #put 1 in result vector if statistically INdistinguishable1\n",
    "        }\n",
    "        \n",
    "        if (res_4_2$p.value < (1 - confidence/100)){               #check if p-value is less than alpha for randomly chosen confidence\n",
    "          result_4_2[row,l] <- 0                                   #put 0 in result vector if statistically distinguishable\n",
    "        }\n",
    "        else {\n",
    "          result_4_2[row,l] <- 1                                   #put 1 in result vector if statistically INdistinguishable1\n",
    "        }\n",
    "        \n",
    "        if (res_4_3$p.value < (1 - confidence/100)){               #check if p-value is less than alpha for randomly chosen confidence\n",
    "          result_4_3[row,l] <- 0                                   #put 0 in result vector if statistically distinguishable\n",
    "        }\n",
    "        else {\n",
    "          result_4_3[row,l] <- 1                                   #put 1 in result vector if statistically INdistinguishable1\n",
    "        }\n",
    "        \n",
    "        if (res_4_4$p.value < (1 - confidence/100)){               #check if p-value is less than alpha for randomly chosen confidence\n",
    "          result_4_4[row,l] <- 0                                   #put 0 in result vector if statistically distinguishable\n",
    "        }\n",
    "        else {\n",
    "          result_4_4[row,l] <- 1                                   #put 1 in result vector if statistically INdistinguishable1\n",
    "        }     \n",
    "      }\n",
    "    result_2_2[row,ncol(Isomer2_normal)+1] <- sum(result_2_2[row,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_2_3[row,ncol(Isomer2_normal)+1] <- sum(result_2_3[row,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_2_4[row,ncol(Isomer2_normal)+1] <- sum(result_2_4[row,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_3_2[row,ncol(Isomer2_normal)+1] <- sum(result_3_2[row,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_3_3[row,ncol(Isomer2_normal)+1] <- sum(result_3_3[row,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_3_4[row,ncol(Isomer2_normal)+1] <- sum(result_3_4[row,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_4_2[row,ncol(Isomer2_normal)+1] <- sum(result_4_2[row,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_4_3[row,ncol(Isomer2_normal)+1] <- sum(result_4_3[row,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "    result_4_4[row,ncol(Isomer2_normal)+1] <- sum(result_4_4[row,1:ncol(Isomer2_normal)])  #sum number of statistically indistinguishable variables\n",
    "  \n",
    "    test_vector_2 <- c(result_2_2[row,ncol(Isomer2_normal)+1], result_2_3[row,ncol(Isomer2_normal)+1], result_2_4[row,ncol(Isomer2_normal)+1])   #create vector of results when isomer 2 is the test set, sum of indistinguishable ions for 2 vs 2, 2 vs 3, and 2 vs 4 (correct ID always in first position of vector)  \n",
    "    test_vector_3 <- c(result_3_3[row,ncol(Isomer2_normal)+1], result_3_2[row,ncol(Isomer2_normal)+1], result_3_4[row,ncol(Isomer2_normal)+1])   #create vector of results when isomer 3 is the test set, sum of indistinguishable ions for 3 vs 3, 3 vs 2, and 3 vs 4 (correct ID always in first position of vector)\n",
    "    test_vector_4 <- c(result_4_4[row,ncol(Isomer2_normal)+1], result_4_3[row,ncol(Isomer2_normal)+1], result_4_2[row,ncol(Isomer2_normal)+1])   #create vector of results when isomer 4 is the test set, sum of indistinguishable ions for 4 vs 4, 4 vs 2, and 4 vs 3 (correct ID always in first position of vector)\n",
    "#testing if sum of indistinguishable ions for correct ID is higher than for incorrect ID by given thresholds (up to 10 higher)            \n",
    "      for(k in 1:10) {    #for each threshold\n",
    "         if(max(test_vector_2) >= sort(test_vector_2, partial=2)[2] + k){#test if the highest value in test_vector_2 is greater than or equal to the second highest value plus the k threshold\n",
    "             inconclusive_matrix[row*length(unique(Isomer))-2,k] <- 0#if the threshold is met, the result is not inconclusive and the next test is performed\n",
    "             if(test_vector_2[1]==max(test_vector_2)){               #if the correct ID is also the highest value\n",
    "                 success_matrix[row*length(unique(Isomer))-2,k] <- 1 #the result is a success\n",
    "                 error_matrix[row*length(unique(Isomer))-2,k] <- 0   #the result is not an error\n",
    "            } else {                                                 #if the correct ID is not the highest value\n",
    "                success_matrix[row*length(unique(Isomer))-2,k] <- 0  #the result is not a success\n",
    "                error_matrix[row*length(unique(Isomer))-2,k] <- 1    #the result is an error\n",
    "            }\n",
    "        } else {                                                     #if the threshold is not met\n",
    "            inconclusive_matrix[row*length(unique(Isomer))-2,k] <- 1 #the result is inconclusive \n",
    "            success_matrix[row*length(unique(Isomer))-2,k] <- 0      #the result is not a success\n",
    "            error_matrix[row*length(unique(Isomer))-2,k] <- 0        #the result is not an error\n",
    "         }\n",
    "       \n",
    "        if(max(test_vector_3) >= sort(test_vector_3, partial=2)[2] + k) {#test if the highest value in test_vector_2 is greater than or equal to the second highest value plus the k threshold\n",
    "             inconclusive_matrix[row*length(unique(Isomer))-1,k] <- 0#if the threshold is met, the result is not inconclusive and the next test is performed\n",
    "             if(test_vector_3[1]==max(test_vector_3)){               #if the correct ID is also the highest value\n",
    "                 success_matrix[row*length(unique(Isomer))-1,k] <- 1 #the result is a success\n",
    "                 error_matrix[row*length(unique(Isomer))-1,k] <- 0   #the result is not an error\n",
    "            } else {                                                 #if the correct ID is not the highest value\n",
    "                success_matrix[row*length(unique(Isomer))-1,k] <- 0  #the result is not a success\n",
    "                error_matrix[row*length(unique(Isomer))-1,k] <- 1    #the result is an error\n",
    "            }\n",
    "        } else {                                                     #if the threshold is not met\n",
    "            inconclusive_matrix[row*length(unique(Isomer))-1,k] <- 1 #the result is inconclusive \n",
    "            success_matrix[row*length(unique(Isomer))-1,k] <- 0      #the result is not a success\n",
    "            error_matrix[row*length(unique(Isomer))-1,k] <- 0        #the result is not an error\n",
    "         }\n",
    "          \n",
    "        if(max(test_vector_4) >= sort(test_vector_4, partial=2)[2] + k) {\n",
    "             inconclusive_matrix[row*length(unique(Isomer)),k] <- 0  #if the threshold is met, the result is not inconclusive and the next test is performed\n",
    "             if(test_vector_4[1]==max(test_vector_4)){               #if the correct ID is also the highest value\n",
    "                 success_matrix[row*length(unique(Isomer)),k] <- 1   #the result is a success\n",
    "                 error_matrix[row*length(unique(Isomer)),k] <- 0     #the result is not an error\n",
    "            } else {                                                 #if the correct ID is not the highest value\n",
    "                success_matrix[row*length(unique(Isomer)),k] <- 0    #the result is not a success\n",
    "                error_matrix[row*length(unique(Isomer)),k] <- 1      #the result is an error\n",
    "            }\n",
    "        } else {                                                     #if the threshold is not met\n",
    "            inconclusive_matrix[row*length(unique(Isomer)),k] <- 1   #the result is inconclusive \n",
    "            success_matrix[row*length(unique(Isomer)),k] <- 0        #the result is not a success\n",
    "            error_matrix[row*length(unique(Isomer)),k] <- 0          #the result is not an error\n",
    "         }\n",
    "          \n",
    "         }   \n",
    "    }\n",
    "}\n",
    "}\n",
    "    \n",
    "average_threshold_success[index,] <- colMeans(success_matrix)        #for each threshold level, put the average success rate into the overall average success matrix\n",
    "average_threshold_inconclusive[index,] <- colMeans(inconclusive_matrix) #for each threshold level, put the average inconclusive rate into the overall average inconclusive matrix\n",
    "average_threshold_error[index,] <- colMeans(error_matrix)            #for each threshold level, put the average error rate into the overall average error matrix\n",
    "   \n",
    "result<-rbind(result_2_2, result_2_3, result_2_4, result_3_2, result_3_3, result_3_4, result_4_2, result_4_3, result_4_4)    #create larger overall result matrix\n",
    "\n",
    "#Generate ROC Curves, not comparing isomers to one another, using only cut-off of sum of indistinguished ions    \n",
    "threshold = 1                                    #set initial threshold for sum of indistinguishable ions\n",
    "exp_pos <- nrow(result) / length(unique(Isomer)) #expected positives, assuming equal number of each isomer\n",
    "exp_neg <- nrow(result) - exp_pos                #expected negatives, total conclusions minus expected positives\n",
    "obs_pos <- vector(length = ncol(my_data))        #observed positives, create vector\n",
    "obs_neg <- vector(length = ncol(my_data))        #observed negatives, create vector\n",
    "true_pos <- vector(length = ncol(my_data))       #true positives, create vector \n",
    "false_pos <- vector(length = ncol(my_data))      #false positives, create vector\n",
    "true_neg <- vector(length = ncol(my_data))       #true negatives, create vector\n",
    "false_neg <- vector(length = ncol(my_data))      #false negatives, create vector   \n",
    "TPR <- vector(length = ncol(my_data))            #true positive rate, create vector\n",
    "FPR <- vector(length = ncol(my_data))            #false positive rate, create vector     \n",
    "    \n",
    "    while(threshold <= ncol(my_data)){           #go through all threshold up to # of variables\n",
    "        obs_pos[threshold] <- sum(result[,ncol(result)]>=threshold) #observed positives at each threshold is the sum of how many comparisons had a sum of indistinguishable ions over the threshold\n",
    "        if(obs_pos[threshold] == nrow(result)){ #if all comparisons are positive then there are no observed negatives\n",
    "            obs_neg[threshold] <- 0\n",
    "        } else {obs_neg[threshold] <- nrow(result) - obs_pos[threshold]} #otherwise, the observed negatives are the total comparisons minus the observed positives\n",
    "        true_pos[threshold] <- sum(result_2_2[,ncol(result_2_2)]>=threshold) + sum(result_3_3[,ncol(result_3_3)]>=threshold) + sum(result_4_4[,ncol(result_4_4)]>=threshold) #true positive is the # of correct ID comparisons over the threshold\n",
    "        false_pos[threshold] <- sum(result_2_3[,ncol(result_2_3)]>=threshold) + sum(result_2_4[,ncol(result_2_4)]>=threshold) + sum(result_3_2[,ncol(result_3_2)]>=threshold) + sum(result_3_4[,ncol(result_3_4)]>=threshold) + sum(result_4_2[,ncol(result_4_2)]>=threshold) + sum(result_4_3[,ncol(result_4_3)]>=threshold) #false positive is the # of incorrect IDs over the threshold\n",
    "        true_neg[threshold] <- sum(result_2_3[,ncol(result_2_3)]<threshold) + sum(result_2_4[,ncol(result_2_4)]<threshold) + sum(result_3_2[,ncol(result_3_2)]<threshold) + sum(result_3_4[,ncol(result_3_4)]<threshold) + sum(result_4_2[,ncol(result_4_2)]<threshold) + sum(result_4_3[,ncol(result_4_3)]<threshold)  #true negative - sum of incorrect IDs under the threshold\n",
    "        false_neg[threshold] <- sum(result_2_2[,ncol(result_2_2)]<threshold) + sum(result_3_3[,ncol(result_3_3)]<threshold) + sum(result_4_4[,ncol(result_4_4)]<threshold) #false negative - sum of correct IDs under the threshold\n",
    "        TPR[threshold] <- true_pos[threshold]/exp_pos #true positive rate - true positives divided by expected positives\n",
    "        FPR[threshold] <- false_pos[threshold]/exp_neg  #false positive rate - false positives divided by expected positives   \n",
    "        threshold <- threshold + 1 #increase threshold by 1\n",
    "    }\n",
    "    \n",
    "average_FPR[index,] <- FPR #store the false positive vector in the average FPR matrix\n",
    "average_TPR[index,] <- TPR #store the true positive vector in the average TPR matrix\n",
    "    \n",
    "AUC_vector <- vector(length = ncol(my_data)) #create an area under the curve vector\n",
    "    for (i in 1:ncol(my_data)){              #for each variable\n",
    "        if(i<ncol(my_data)){                 #if i is less than the last column\n",
    "        AUC_vector[i] <- (FPR[i]-FPR[i+1])*TPR[i] #AUC is the false positive rate of the given column, minus the next FPR, multiplied by the TPR of that column\n",
    "        }\n",
    "        else {                               #if i is the last column\n",
    "            AUC_vector[i] <- FPR[i]*TPR[i]   #AUC is the FPR times the TPR\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    AUC <- signif(sum(AUC_vector),4)        #Sum up the AUC and round to 4 significant digits\n",
    "    max_diff <- signif(max(TPR-FPR),4)      #calculate the highest difference between the TPR and FPR, round to 4 sig figs.\n",
    "    title <- paste0(type, \", \", ion_threshold, \"%, \", normal, \", \", confidence, \"% confidence\") #create a plot title\n",
    "    subtitle <- paste0(\"AUC = \", AUC, \", Max TPR-FPR = \", max_diff) #Add AUC and Max TPR-FPR to the plot\n",
    "    plot_data <- cbind(FPR, TPR)            #store FPR and TPR for plotting\n",
    "\n",
    "graphy <- ggplot(data = as.data.frame(plot_data), aes(x = FPR, y = TPR)) + #plot ROC curve of FPR vs TPR\n",
    "    geom_line() +                           #connect points with a line\n",
    "    geom_point() +                          #show distinct points on line\n",
    "    ylim(0,1) +                             #set y axis limits\n",
    "    xlim(0,1) +                             #set x axis limits\n",
    "    labs(title = title, subtitle = subtitle, x = \"False Positive Rate\", y = \"True Positive Rate\") + #label title, subtitle, and axes\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    theme(text = element_text(size = 7.5))  #text size                \n",
    "    \n",
    "list_of_ttest_plots[[index]] <- graphy      #store plot in list of plots \n",
    "\n",
    "if(compare_methods){                        #if comparing variable importance by method \n",
    "obs_pos_col <- vector(length = ncol(my_data))  #create vector for observed positives by column (not threshold of total sum)\n",
    "obs_neg_col <- vector(length = ncol(my_data))  #create vector for observed negatives by column (not threshold of total sum)\n",
    "true_pos_col <- vector(length = ncol(my_data)) #create vector for true positives by column (not threshold of total sum)\n",
    "false_pos_col <- vector(length = ncol(my_data))#create vector for false positives by column (not threshold of total sum)\n",
    "true_neg_col <- vector(length = ncol(my_data)) #create vector for true negatives by column (not threshold of total sum)\n",
    "false_neg_col <- vector(length = ncol(my_data))#create vector for false negatives by column (not threshold of total sum)   \n",
    "accuracy_col <- vector(length = ncol(my_data)) #create vector to store and calcualte accuracy by variable\n",
    "    \n",
    " for(i in 1:ncol(result)-1){ #for each column (representing a variable, not the sum)\n",
    "    obs_pos_col[i] <- sum(result[,i])          #observed positives per column \n",
    "    obs_neg_col[i] <- nrow(result) - sum(result[,i]) #observed negatives per column (# rows minus sum of indistinguishable ions)\n",
    "    true_pos_col[i] <- sum(sum(result_2_2[,i]), sum(result_3_3[,i]), sum(result_4_4[,i])) #sum of correct IDs per column\n",
    "    false_pos_col[i] <- sum(sum(result_2_3[,i]), sum(result_2_4[,i]), sum(result_3_2[,i]), sum(result_3_4[,i]), sum(result_4_2[,i]), sum(result_4_3[,i])) #sum of incorrect IDs per column\n",
    "    true_neg_col[i] <- length(which(result_2_3[,i]==0)) + length(which(result_2_4[,i]==0)) + length(which(result_3_2[,i]==0)) + length(which(result_3_4[,i]==0)) + length(which(result_4_2[,i]==0)) + length(which(result_4_3[,i]==0)) #how many incorrect IDs = 0 (distinguishable)\n",
    "    false_neg_col[i] <- length(which(result_2_2[,i]==0)) + length(which(result_3_3[,i]==0)) + length(which(result_4_4[,i]==0)) #false negatives = how many correct IDs =0 (distinguishable)\n",
    "    accuracy_col[i] <- (true_pos_col[i] + true_neg_col[i])/ (true_pos_col[i] + false_pos_col[i] + true_neg_col[i] + false_neg_col[i]) #accuracy - true pos + true neg over all conclusions\n",
    " }\n",
    "average_accuracy[index,] <- accuracy_col  #store the accuracy vector for each replicate  \n",
    "}\n",
    "normal_vector[index] <- normal   #store the normalization techniques used\n",
    "index <- index + 1 #increase index, go back to start and complete next rep\n",
    "}\n",
    "\n",
    "ggexport(plotlist = list_of_ttest_plots, ncol = 2, nrow = 3, filename = roc_curves) #export all plots\n",
    "    \n",
    "#average ROC curve    \n",
    "plot_data_average <- cbind(colMeans(average_FPR), colMeans(average_TPR)) #create plot data from column Means of average FPR/TPR\n",
    "average_AUC_vector <- vector(length = ncol(my_data)) #create vector for storing AUC for average plot\n",
    "    for (i in 1:ncol(my_data)){   #for each threshold\n",
    "        if(i<ncol(my_data)){      #if not the last column\n",
    "        average_AUC_vector[i] <- (colMeans(average_FPR)[i]-colMeans(average_FPR)[i+1])*colMeans(average_TPR)[i] #AUC is average FPR minus next FPR, times TPR\n",
    "        }\n",
    "        else { #for last column\n",
    "        average_AUC_vector[i] <- colMeans(average_FPR)[i]*colMeans(average_TPR)[i] #AUC average FPR*TPR\n",
    "        }\n",
    "    }\n",
    "average_AUC <- signif(sum(average_AUC_vector),4) #sum the average AUC vector and round to 4 sig figs\n",
    "average_max_diff <- signif(max(colMeans(average_TPR) - colMeans(average_FPR)),4) #find the max TPR-FPR for the average plot\n",
    "title <- paste0(\"Average ROC Curve \", type, \", \", ion_threshold, \"%\")  #create title\n",
    "subtitle <- paste0(\"AUC = \", average_AUC, \", Max TPR-FPR = \", average_max_diff)   #create subtitle with AUC/Max TPR-FPR  \n",
    "average_plot <- ggplot(data = as.data.frame(plot_data_average), aes(x=colMeans(average_FPR), y=colMeans(average_TPR))) + #plot average FPR vs average TPR\n",
    "    geom_line() + #connect points with line\n",
    "    geom_point() + #show data points as points\n",
    "    labs(title = title, subtitle = subtitle, x = \"False Positive Rate\", y = \"True Positive Rate\") + #label title, subtitle, and axes\n",
    "    ylim(0,1) +                             #set y axis limits\n",
    "    xlim(0,1) +                             #set x axis limits\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\"))   #black border\n",
    "ggexport(average_plot, filename = average_roc_curve) #export average ROC curve   \n",
    "      \n",
    "#calculate average Success/inconclusive/error rate to save in Excel\n",
    "result_threshold <- matrix(nrow=3, ncol = 10) #create empty matrix, three rows (S/I/E), number of thresholds as columns\n",
    "rownames(result_threshold) <- c(\"Success Rate\", \"Inconclusive Rate\", \"Error Rate\") #assign row names\n",
    "colnames(result_threshold) <- 1:10     #assign thresholds as column names\n",
    "result_threshold[1,] <- signif(colMeans(average_threshold_success),3) #row 1 is average success rate across all replicates\n",
    "result_threshold[2,] <- signif(colMeans(average_threshold_inconclusive),3) #row 2 is average inconclusive rate across all replicates\n",
    "result_threshold[3,] <- signif(colMeans(average_threshold_error),3) #row 3 is average error rate across all replicates\n",
    "name <- paste0(ion_threshold, \" \", type) #create sheet name\n",
    "if(original_normal == \"ioncurrent\" && !test_set){   #only save as excel once per ionthreshold/type combination\n",
    "    write.xlsx(result_threshold, file_ttest_threshold, sheetName = name, append = TRUE, showNA = FALSE)     #create/append to excel file\n",
    "}\n",
    "    \n",
    "if(compare_plot){\n",
    "    compare_plot_data[1:10,4] <- result_threshold[1,]  #add average success rate to compare plot matrix\n",
    "    compare_plot_data[11:20,4] <- result_threshold[2,] #add average inconclusive rate to compare plot matrix\n",
    "    compare_plot_data[21:30,4] <- result_threshold[3,] #add average error rate to compare plot matrix\n",
    "}    \n",
    "    \n",
    "#calculate S/I/E for maximum success rate, and lowest error rate and save to excel\n",
    "#Max Success (if tie, lowest associated error rate)   \n",
    "mat <- which(average_threshold_success==max(average_threshold_success), arr.ind = TRUE) #save array indices of maximum success rate (row is replicate, column is threshold)\n",
    "min_mat <- vector(length = nrow(mat))   #create vector to store the error rates associated with the above maximum success rate locations\n",
    "for(i in 1:nrow(mat)){ #for however many indices had the same max success rate\n",
    "    min_mat[i] <- average_threshold_error[mat[i,1], mat[i,2]] #store the associated error rate\n",
    "}  \n",
    "err_ind <- which(min_mat == min(min_mat))[1] #store the first index of the minimum associated error rates\n",
    "max_success <- vector(length = 4) #create vector to store the S/I/E/normal data for the maximum success\n",
    "    max_success[1] <- signif(average_threshold_success[mat[err_ind,1],mat[err_ind,2]],3) #to 3 sig figs, store the maximum success rate (with lowest error rate)\n",
    "    max_success[2] <- signif(average_threshold_inconclusive[mat[err_ind,1],mat[err_ind,2]],3) #to 3 sig figs, store the inc. rate associated with the max success rate\n",
    "    max_success[3] <- signif(average_threshold_error[mat[err_ind,1],mat[err_ind,2]],3) #to 3 sig figs, store the error rate associated with the max success rate\n",
    "    max_success[4] <- normal_vector[mat[err_ind,1]] #store the normalization used to reach the maximum success rate\n",
    "\n",
    "#Minimum error (if tie, highest associated success rate)    \n",
    "mat2 <- which(average_threshold_error==min(average_threshold_error), arr.ind = TRUE) #save array indices of minimum error rates (row is replicate, column is threshold)\n",
    "max_mat <- vector(length = nrow(mat2))  #create vector to store the success rates associated with the above manimum error rate locations\n",
    "for(i in 1:nrow(mat2)){ #for however many indices had the same minimum error rate rate\n",
    "    max_mat[i] <- average_threshold_success[mat2[i,1], mat2[i,2]] #store the associated success rate\n",
    "}\n",
    "err_ind <- which(max_mat == max(max_mat))[1] #store the first index of the maximum associated success rates\n",
    "min_error <- vector(length = 4) #create vector to store the S/I/E/normal data for the minimum error   \n",
    "    min_error[1] <- signif(average_threshold_success[mat2[err_ind,1],mat2[err_ind,2]],3)#to 3 sig figs, store the success rate associated with the lowest error rate\n",
    "    min_error[2] <- signif(average_threshold_inconclusive[mat2[err_ind,1],mat2[err_ind,2]],3) #to 3 sig figs, store the inc. rate associated with the lowest error rate\n",
    "    min_error[3] <- signif(average_threshold_error[mat2[err_ind,1],mat2[err_ind,2]],3)  #to 3 sig figs, store the error rate associated with the max success rate\n",
    "    min_error[4] <- normal_vector[mat2[err_ind,1]] #store the normalization used to reach the minimum error rate\n",
    "\n",
    "name1 <- paste0(ion_threshold, \"_\", type) #create sheet name\n",
    "    if(original_normal == \"ioncurrent\" && !test_set){   #only save as excel once per ionthreshold/type combination\n",
    "write.xlsx(rbind(max_success, min_error), max_ttest, sheetName = name1, append = TRUE, showNA = FALSE)     #store max success and min error vectors in excel sheet\n",
    " }  \n",
    "if(compare_methods){   #if comparing variable importance based on methods\n",
    "    compare_matrix[1,] <-colMeans(average_accuracy) #store column averages of variable accuracy\n",
    "    #len_tt <- sqrt(compare_matrix[1,]%*%compare_matrix[1,])  #if desired to normalize variable importance by vector length\n",
    "    #compare_matrix[1,] <- compare_matrix[1,]/rep(len_tt, ncol(compare_matrix))\n",
    "}\n",
    "    \n",
    "normal <- original_normal           #in case other methods are being used, restore originally assigned method of normalizations\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA/LDA Analysis\n",
    "- Full Dataset\n",
    "- Leave-one-out-Cross-Validation\n",
    "- Max LOOCV and with thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(PCA_LDA && !sameday){  #If conducting PCA for variable reduction before LDA\n",
    "pca.train <- prcomp(my_data_test[,-1], center = TRUE, scale = TRUE) #calculate PCA on centered/scaled data\n",
    "    \n",
    "if(nchar(as.character(my_data_test[1,1]))==1){    #if label vector entries iare only one character\n",
    "my_data_test[,1] <- paste0(my_data_test[,1], \"-\", type) #add\"-type\"\n",
    "}   \n",
    "   \n",
    "#plot just PCA    \n",
    "title = paste0(\"PCA \", ion_threshold, \" \", type, \" \", normal) #assign title\n",
    "xlab = paste0('PC1 ', signif(pca.train$sdev[1]*pca.train$sdev[1]/sum(pca.train$sdev%*%pca.train$sdev)*100,4), \"% variance\") #assign x label including proportion of variance\n",
    "ylab = paste0('PC2 ', signif(pca.train$sdev[2]*pca.train$sdev[2]/sum(pca.train$sdev%*%pca.train$sdev)*100,4), \"% variance\") #assign y label including proportion of variance\n",
    "plot_data <- as.data.frame(cbind(Isomer = my_data_test[,1], PC1 = as.numeric(predict(pca.train)[,1]), PC2 = as.numeric(predict(pca.train)[,2]))) #create dataframe for plotting including PC scores and label information\n",
    "plot_data[,1] <- as.factor(plot_data[,1]) #make label information as factor\n",
    "\n",
    "pca_plot <- ggplot(data = plot_data, aes(x = as.numeric(PC1), y = as.numeric(PC2), color = Isomer)) + #plot PC1 vs PC2, colored as Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) #black border\n",
    "    \n",
    "ggexport(pca_plot, filename = pca_file) #export pca scores plot\n",
    " \n",
    "PCs = 1    #start as PCs = 1\n",
    "for (i in 1: length(pca.train$sdev)){ #determine how many PCs to retain for LDA\n",
    "    if(sum(pca.train$sdev[1:i]%*%pca.train$sdev[1:i])/sum(pca.train$sdev%*%pca.train$sdev) < 0.95){ #go up until cumulative proportion of variance is >= 0.95\n",
    "        PCs <- PCs + 1\n",
    "    }\n",
    "} \n",
    "    \n",
    "lda.model <- lda(predict(pca.train)[,1:PCs],Isomer, prior = rep(1,length(unique(Isomer)))/length(unique(Isomer))) #perform LDA on PCA up to number of PCs determined, equal prior probability per isomer\n",
    "Zcv.train <- predict(lda.model)$x #store rotated LD functions\n",
    "title = paste0(\"PCA_LDA \", ion_threshold, \" \", type, \" \", normal) #assign title\n",
    "xlab = paste0(\"LD1 \",signif(lda.model$svd[1]*lda.model$svd[1]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign x axis label, including Between:within variance\n",
    "ylab = paste0(\"LD2 \",signif(lda.model$svd[2]*lda.model$svd[2]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign y axis label, including Between:within variance\n",
    "plot_data <- as.data.frame(cbind(Isomer = my_data_test[,1], LD1 = Zcv.train[,1], LD2 = Zcv.train[,2])) #create plot data data frame\n",
    "plot_data[,1] <- as.factor(plot_data[,1]) #make label information as factor\n",
    "\n",
    "if(!lda_threshold){    #if using default loocv error rate (highest posterior probabilty only)\n",
    "error <- 0 #start error at 0\n",
    "for (i in 1:nrow(my_data_test)) {  #leave-one-out, for each sample (row)\n",
    "  pca.train<-prcomp(my_data_test[-i,-1],center=TRUE,scale=TRUE) #perform PCA on entire dataset minus held out sample\n",
    "  Zpc.train<-predict(pca.train) #rotate onto PC axes\n",
    "  Zpc.verify<-predict(pca.train, newdata=my_data_test[i,-1]) #rotate held out sample onto PC axes\n",
    "  lda.model<-lda(Zpc.train[,1:PCs],my_data_test[-i,1], prior = rep(1,length(unique(Isomer)))/length(unique(Isomer))) #perform LDA on previously detemrined # of PCs\n",
    "  Zcv.verify<- predict(lda.model, newdata = Zpc.verify[,1:PCs])$class #rotate held out samples PC scores into LD space, and classify\n",
    "  if(Zcv.verify != my_data_test[i,1]) { #if the classification does not equal the sample's label\n",
    "    error <- error + 1                  #add 1 to error\n",
    "  }\n",
    "}\n",
    "    \n",
    "loocv <- signif(error/nrow(my_data_test)*100,4)    #leave one out error rate is error divived by # of samples, multiplied by 100, rounded to 4 significant figures\n",
    "xlim = mean(as.numeric(plot_data[,2])) #for x location of loocv on plot\n",
    "ylim = min(as.numeric(plot_data[,3]))  #for y location of loocv on plot\n",
    "    \n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #create lda plot, LD1 vs LD2, colored as Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    annotate(\"label\", x = xlim, y = ylim, label = paste0(\"LOOCV error = \", loocv,\"%\"))  #add LOOCV error rate\n",
    "    ggexport(lda_plot, filename = pcalda_loocv_plot) #export plot\n",
    "}\n",
    "    \n",
    "if(lda_threshold){ #if thresholds used for error rate, plot without error rate for cleaner figure\n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #create LDA plot, LD1 vs LD2, colored as Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) +  #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) #black border \n",
    "    ggexport(lda_plot, filename = pcalda_threshold_plot)  #export plot\n",
    "       \n",
    "posterior <- matrix(0, nrow = nrow(my_data_test), ncol = length(unique(Isomer))) #create empty matrix for posterior probabilities during loocv\n",
    "for (i in 1:nrow(posterior)) { #for every sample\n",
    "  pca.train<-prcomp(my_data_test[-i,-1],center=TRUE,scale=TRUE) #hold each sample out, perform PCA\n",
    "  Zpc.train<-predict(pca.train) #rotate training set into PC space\n",
    "  Zpc.verify<-predict(pca.train, newdata=my_data_test[i,-1]) #rotate held out sample into PC space\n",
    "  lda.model<-lda(Zpc.train[,1:PCs],my_data_test[-i,1], prior = rep(1,length(unique(Isomer)))/length(unique(Isomer))) #calculate LDA on pre-determined # of PCs\n",
    "  posterior[i,]<- as.numeric(predict(lda.model, newdata = Zpc.verify[,1:PCs])$posterior) #rotate held out sample's PC scores into LD space, store posterior probabilities in matrix\n",
    "}   \n",
    "\n",
    "confusion <- matrix(NA,nrow = 3, ncol = 10) #create empty confusion matrix\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\") #assign row names of confusion matrix\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #Assign thresholds as column names\n",
    "\n",
    "threshold = 0.50    #set starting posterior probability threshold\n",
    "    for(t in 1:ncol(confusion)){ #for every posterior probability threshold\n",
    "          threshold_result <- matrix(0,nrow = nrow(posterior), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(posterior)){ #for every sample\n",
    "                 if(max(posterior[z,]) < threshold){ #if the highest posterior probability is under the threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, \"1\" in column 2\n",
    "                } else { #if highest posterior probability is >= the threshold\n",
    "                     threshold_result[z,1] <- paste0(which.max(posterior[z,]) +1,\"-\", type) #determine which isomer has the highest posterior probability and store as \"-type\" in column 1\n",
    "                }\n",
    "              }\n",
    "\n",
    "        confusion[1,t] <- mean(threshold_result[,1]==my_data_test[,1]) #success rate\n",
    "        confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate   \n",
    "\n",
    "        if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "          confusion[3,t] <- mean(threshold_result[,1]!=my_data_test[,1]) #error rate is mean of column 1 not equal to label vector\n",
    "        } else {  #if there are inconclusives\n",
    "          check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "          confusion[3,t] <- sum(threshold_result[-check,1]!=my_data_test[-check,1])/nrow(posterior) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "        threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "        }\n",
    "\n",
    "confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    "print(confusion)   #print confusion matrix\n",
    "name1 <- paste0(\"Loocv, \", ion_threshold, \" \", type, \" \",normal ) #assign sheet name\n",
    "    if(!test_set){\n",
    "write.xlsx(confusion, pcalda_file, sheetName = name1, append = TRUE, showNA = FALSE) #write to excel sheet\n",
    "        }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Analysis\n",
    "- Full Dataset\n",
    "- Leave-one-out-Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lda.default(x, grouping, ...):\n",
      "“variables are collinear”\n",
      "Warning message in lda.default(x, grouping, ...):\n",
      "“variables are collinear”\n",
      "file saved to /Users/jenniferbonetti/Documents/UvA/Pub1_plots/LDA/loocv/FA-10-ioncurrent_threshold.pdf\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0.50   0.55   0.60   0.65   0.70   0.75   0.80   0.85\n",
      "Success rate      0.9100 0.8890 0.8720 0.8650 0.8440 0.8230 0.8020 0.7570\n",
      "Inconclusive rate 0.0000 0.0278 0.0486 0.0556 0.0868 0.1320 0.1670 0.2290\n",
      "Error rate        0.0903 0.0833 0.0799 0.0799 0.0694 0.0451 0.0312 0.0139\n",
      "                    0.90    0.95\n",
      "Success rate      0.6700 0.57600\n",
      "Inconclusive rate 0.3190 0.42000\n",
      "Error rate        0.0104 0.00347\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in .jcall(wb, \"Lorg/apache/poi/ss/usermodel/Sheet;\", \"createSheet\", : java.lang.IllegalArgumentException: The workbook already contains a sheet of this name\n",
     "output_type": "error",
     "traceback": [
      "Error in .jcall(wb, \"Lorg/apache/poi/ss/usermodel/Sheet;\", \"createSheet\", : java.lang.IllegalArgumentException: The workbook already contains a sheet of this name\nTraceback:\n",
      "1. write.xlsx(confusion, lda_file, sheetName = name1, append = TRUE, \n .     showNA = FALSE)   # at line 88 of file <text>",
      "2. createSheet(wb, sheetName)",
      "3. .jcall(wb, \"Lorg/apache/poi/ss/usermodel/Sheet;\", \"createSheet\", \n .     sheetName)",
      "4. .jcheck(silent = FALSE)"
     ]
    }
   ],
   "source": [
    "if(LDA && !sameday && !test_set){ #If LDA analysis (directly on normalized data, no PCA variable reduction), and not same day analysis\n",
    "  \n",
    "if(nchar(as.character(my_data_test[1,1]))==1){   #if label vector entries iare only one character \n",
    "    my_data_test[,1] <- paste0(my_data_test[,1], \"-\", type)  #add \"-type\"\n",
    "    }   \n",
    "       \n",
    "lda.model <- lda(my_data_test[,-1], my_data_test[,1] , prior = rep(1,length(unique(Isomer)))/length(unique(Isomer))) #perform LDA, equal prior probabilities per isomer\n",
    "    \n",
    "if(compare_methods){ #if comparing variable importance per method\n",
    "    len_lda <- sqrt(lda.model$scaling[,1]%*%lda.model$scaling[,1]) #calculate length of the lda scaling vector\n",
    "    len_lda2 <- sqrt(lda.model$scaling[,2]%*%lda.model$scaling[,2]) #calculate length of the lda scaling vector\n",
    "    compare_matrix[2,] <- abs(lda.model$scaling[,1]/rep(len_lda, length(lda.model$scaling[,1]))) #second row in compare matrix in unit vector of lda scaling (LD1)\n",
    "    compare_matrix[3,] <- abs(lda.model$scaling[,2]/rep(len_lda2, length(lda.model$scaling[,2])))#third row in compare matrix in unit vector of lda scaling (LD2)\n",
    "}    \n",
    "    \n",
    "Zcv.train <- predict(lda.model)$x #store rotated LD matrix\n",
    "lda_cv <- lda(my_data_test[,-1],Isomer, prior = rep(1,length(unique(Isomer)))/length(unique(Isomer)),CV=TRUE)   #perform LOOCV (highest posterior probability, no thresholds)\n",
    "predictions <- paste0(lda_cv$class, \"-\", type) #save predictions as \"-type\" in vector\n",
    "title = paste0(\"LDA \", ion_threshold, \" \", type, \" \", normal) #assign plot title\n",
    "xlab = paste0(\"LD1 \",signif(lda.model$svd[1]*lda.model$svd[1]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign x axis label, including Between:within variance\n",
    "ylab = paste0(\"LD2 \",signif(lda.model$svd[2]*lda.model$svd[2]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign y axis label, including Between:within variance\n",
    "plot_data <- as.data.frame(cbind(Isomer = my_data_test[,1], LD1 = Zcv.train[,1], LD2 = Zcv.train[,2])) #save plot data data frame\n",
    "plot_data[,1] <- as.factor(plot_data[,1]) #make isomer label into factor \n",
    "    \n",
    "if(!lda_threshold){    #if not using thresholds\n",
    "loocv <- signif(sum(predictions != my_data_test[,1])/nrow(my_data_test)*100,4)    #calculate loocv to 4 significant figures, sum of predictions != label vector divided by # of samples, times 100\n",
    "xlim = mean(as.numeric(plot_data[,2])) #set x location for LOOCV error rate\n",
    "ylim = min(as.numeric(plot_data[,3]))  #set y location for LOOCV error rate\n",
    "    \n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #plot LD functions, colored as Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    annotate(\"label\", x = xlim, y = ylim, label = paste0(\"LOOCV error = \", loocv,\"%\")) #add LOOCV error rate\n",
    "    ggexport(lda_plot, filename = lda_loocv_plot) #export plot\n",
    "}\n",
    "    \n",
    "if(lda_threshold){ #if calculating error rate based on threshold, clean plot without LOOCV\n",
    "    \n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) +#plot LD functions, colored as Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\"))  #black border\n",
    "    ggexport(lda_plot, filename = lda_threshold_plot)  #export plot \n",
    "\n",
    "confusion <- matrix(0,nrow = 3, ncol = 10) #create empty confusion matrix\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\") #set row names\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #set column names of posterior probability thresholds\n",
    "threshold = 0.50    #set initial threshold\n",
    "    for(t in 1:ncol(confusion)){ #for every posterior probability threshold\n",
    "          threshold_result <- matrix(0,nrow = nrow(lda_cv$posterior), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(lda_cv$posterior)){#for every sample\n",
    "                  if(is.nan(lda_cv$posterior[z,1])){ #if the posterior probability is \"NaN\"\n",
    "                      threshold_result[z,2] <- 1 #result is inconclusive, \"1\" in column 2\n",
    "                  } else {\n",
    "                 if(max(lda_cv$posterior[z,]) < threshold){ #if the highest posterior probability is under the threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, \"1\" in column 2\n",
    "                } else { #if highest posterior probability is >= the threshold\n",
    "                     threshold_result[z,1] <- paste0(which.max(lda_cv$posterior[z,]) +1,\"-\", type) #determine which isomer has the highest posterior probability and store as \"-type\" in column 1\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "\n",
    "        confusion[1,t] <- mean(threshold_result[,1]==my_data_test[,1]) #success rate\n",
    "        confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "    \n",
    "        if(sum(as.numeric(threshold_result[,2]))==0){#if there are no inconclusives\n",
    "          confusion[3,t] <- mean(threshold_result[,1]!=my_data_test[,1]) #error rate is mean of column 1 not equal to label vector\n",
    "        } else { #if there are inconclusives\n",
    "          check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate  \n",
    "          confusion[3,t] <- sum(threshold_result[-check,1]!=my_data_test[-check,1])/nrow(lda_cv$posterior) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "        threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "    }\n",
    "\n",
    "confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    "    \n",
    "    if(compare_plot){ #if creating comparison plot\n",
    "      compare_plot_data[31:40, 4] <- confusion[1,]   #add success rate to compare plot data\n",
    "      compare_plot_data[41:50, 4] <- confusion[2,]   #add inconclusive rate to compare plot data\n",
    "      compare_plot_data[51:60, 4] <- confusion[3,]   #add error rate to compare plot data\n",
    "    }\n",
    "    \n",
    "print(confusion)  #print confusion matrix\n",
    "name1 <- paste0(\"Loocv, \", ion_threshold, \" \", type, \" \",normal ) #assign sheet name\n",
    "write.xlsx(confusion, lda_file, sheetName = name1, append = TRUE, showNA = FALSE)\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA/LDA with Test Set\n",
    "- 80/20 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(PCA_LDA & test_set && !sameday){ #If conducting PCA for variable reduction before LDA, and separating into test/training set\n",
    "\n",
    "set.seed(123) #set seed for reproducible random test set selection\n",
    "if(nchar(as.character(my_data_test[1,1]))==1){  #if label vector entries are only one character   \n",
    "my_data_test[,1] <- paste0(as.numeric(my_data_test[,1]), \"-\", type) #add \"-type\"\n",
    "}\n",
    "  \n",
    "sample <- sample(1:nrow(my_data_test), round(0.20*(nrow(my_data_test)))) #randomly sample 20% of the dataset indices\n",
    "my_data_train <- my_data_test[-sample,] #all but randomly chosen indices become training set\n",
    "my_data_testset <- my_data_test[sample,]#randomly chosen indices become test set \n",
    "    \n",
    "pca.model <- prcomp(my_data_train[,-1], center = TRUE, scale = TRUE) #perform PCA on the training set\n",
    "pca.train <- predict(pca.model) #save PC scores for training set\n",
    "pca.test <- predict(pca.model, newdata = my_data_testset[,-1])  #rotate test set into PC space\n",
    "    \n",
    "title = paste0(\"PCA \", ion_threshold, \" \", type, \" \", normal) #assign title for PCA scores plot\n",
    "xlab = paste0('PC1 ', signif(pca.model$sdev[1]*pca.model$sdev[1]/sum(pca.model$sdev%*%pca.model$sdev)*100,4), \"% variance\")  #assign x label including proportion of variance\n",
    "ylab = paste0('PC2 ', signif(pca.model$sdev[2]*pca.model$sdev[2]/sum(pca.model$sdev%*%pca.model$sdev)*100,4), \"% variance\")  #assign y label including proportion of variance\n",
    "plot_data <- as.data.frame(cbind(Isomer = my_data_train[,1], PC1 = pca.train[,1], PC2 = pca.train[,2]))  #create dataframe for plotting including PC scores and label information\n",
    "plot_data[,1] <- as.factor(plot_data[,1]) #make label information as factor\n",
    "plot_test <- as.data.frame(cbind(PC1 = pca.test[,1], PC2 = pca.test[,2])) #make data frame for plotting test set\n",
    "\n",
    "pca_plot <- ggplot(data = plot_data, aes(x = as.numeric(PC1), y = as.numeric(PC2), color = Isomer)) + #plot PC1 vs PC2, colored as Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    geom_point(data = plot_test, aes(x = as.numeric(PC1), y = as.numeric(PC2), color = \"Test Set\")) + #add test set points to plot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\"))   #black border   \n",
    "ggexport(pca_plot, filename = pca_test_file)                 #export PCA scores plot\n",
    " \n",
    "PCs = 1    #start as PCs = 1\n",
    "for (i in 1: length(pca.model$sdev)){ #determine how many PCs to retain for LDA\n",
    "    if(sum(pca.model$sdev[1:i]%*%pca.model$sdev[1:i])/sum(pca.model$sdev%*%pca.model$sdev) < 0.95){ #go up until cumulative proportion of variance is >= 0.95\n",
    "        PCs <- PCs + 1\n",
    "    }\n",
    "} \n",
    "    \n",
    "lda.model <- lda(predict(pca.model)[,1:PCs],my_data_train[,1], prior = rep(1,length(unique(Isomer)))/length(unique(Isomer))) #perform LDA on PCA up to number of PCs determined for training set, equal prior probability per isomer\n",
    "    Zcv.train <- predict(lda.model)$x #rotate training set PC scores into LD space\n",
    "    Zcv.test <- predict(lda.model, newdata = pca.test[,1:PCs])$x #rotate test set PC scores into LD space\n",
    "title = paste0(\"PCA_LDA \", ion_threshold, \" \", type, \" \", normal) #assign title\n",
    "xlab = paste0(\"LD1 \",signif(lda.model$svd[1]*lda.model$svd[1]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign x axis label, including Between:within variance\n",
    "ylab = paste0(\"LD2 \",signif(lda.model$svd[2]*lda.model$svd[2]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign y axis label, including Between:within variance\n",
    "plot_data <- as.data.frame(cbind(Isomer = my_data_train[,1], LD1 = Zcv.train[,1], LD2 = Zcv.train[,2])) #create plot data data frame\n",
    "plot_data[,1] <- as.factor(plot_data[,1]) #save label info as factors\n",
    "plot_test <- as.data.frame(cbind(LD1 = Zcv.test[,1], LD2 = Zcv.test[,2])) #create data frame of test data for plotting\n",
    "\n",
    "if(!lda_threshold){    #if not using threshold\n",
    "predictions <- predict(lda.model, newdata = pca.test[,1:PCs])$class   #attempt to classify test set using highest posterior probability\n",
    "error <- signif(sum(predictions != my_data_testset[,1])/length(my_data_testset[,1]) *100,4)  #error is number of times predictions didn't match labels, rounded to 4 sig figs\n",
    "xlim = mean(as.numeric(plot_data[,2])) #for x location of test set error on plot\n",
    "ylim = min(as.numeric(plot_data[,3]))  #for y location of test set error on plot\n",
    "    \n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #create lda plot, LD1 vs LD2, colored as Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    geom_point(data = plot_test, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = \"Test Set\")) + #add test set points to plot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    annotate(\"label\", x = xlim, y = ylim, label = paste0(\"Test set error = \", error,\"%\")) #annotate with test set error rate\n",
    "    ggexport(lda_plot, filename = pcalda_test_plot) #export plot\n",
    "}\n",
    "    \n",
    "if(lda_threshold){ #if using thresholds, create clean plot without error rate\n",
    "posterior <- predict(lda.model, newdata = pca.test[,1:PCs])$posterior  #save psoterior probabilities for test set \n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #create lda plot, LD1 vs LD2, colored as Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    geom_point(data = plot_test, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = \"Test Set\")) + #add test set points to plot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) #black border  \n",
    "ggexport(lda_plot, filename = pcalda_threshold_plot_test) #export plot\n",
    "    \n",
    "confusion <- matrix(0,nrow = 3, ncol = 10) #create empty confusion matrix\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\") #assign row names of confusion matrix\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #Assign thresholds as column names\n",
    "threshold = 0.50 #set starting posterior probability threshold\n",
    "    for(t in 1:ncol(confusion)){ #for every posterior probability threshold\n",
    "          threshold_result <- matrix(0,nrow = nrow(posterior), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(posterior)){ #for every sample\n",
    "                 if(max(posterior[z,])< threshold){ #if the highest posterior probability is under the threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, \"1\" in column 2\n",
    "                } else { #if highest posterior probability is >= the threshold\n",
    "                     threshold_result[z,1] <- paste0(which.max(posterior[z,]) +1,\"-\", type) #determine which isomer has the highest posterior probability and store as \"-type\" in column 1\n",
    "                }\n",
    "              }\n",
    "\n",
    "        confusion[1,t] <- mean(threshold_result[,1]==my_data_testset[,1]) #success rate\n",
    "        confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "        \n",
    "        if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "          confusion[3,t] <- mean(threshold_result[,1]!=my_data_testset[,1]) #error rate is mean of column 1 not equal to label vector\n",
    "        } else { #if there are inconclusives\n",
    "            check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "            confusion[3,t] <- sum(threshold_result[-check,1]!=my_data_testset[-check,1])/nrow(posterior) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "        threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "    }\n",
    "\n",
    "confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    "name1 <- paste0(\"Test Set, \", ion_threshold, \" \", type, \" \",normal ) #assign sheet name\n",
    "write.xlsx(confusion, pcalda_file, sheetName = name1, append = TRUE, showNA = FALSE) #print to excel\n",
    "}\n",
    "}     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA with Test Set\n",
    "- 80/20 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LDA && test_set && !sameday){ #If conducting LDA directly (without PCA variable reduction), on randomly selected test set \n",
    "\n",
    "set.seed(123) #set seed for reproducible random test set selection\n",
    "if(nchar(as.character(my_data_test[1,1]))==1){    #if label vector entries are only one character   \n",
    "my_data_test[,1] <- paste0(as.numeric(my_data_test[,1]), \"-\", type) #add \"-type\"\n",
    "}\n",
    "  \n",
    "sample <- sample(1:nrow(my_data_test), round(0.20*(nrow(my_data_test))))  #randomly sample 20% of the dataset indices\n",
    "my_data_train <- my_data_test[-sample,] #all but randomly chosen indices become training set\n",
    "my_data_testset <- my_data_test[sample,]#randomly chosen indices become test set  \n",
    "    \n",
    "lda.model <- lda(my_data_train[,-1],my_data_train[,1], prior = rep(1,length(unique(Isomer)))/length(unique(Isomer))) #perform LDA on training set, equal prior probability per isomer\n",
    "Zcv.train <- predict(lda.model)$x  #rotate training set into LD space\n",
    "Zcv.test <- predict(lda.model, newdata = my_data_testset[,-1])$x #rotate test set into LD space\n",
    "title = paste0(\"LDA \", ion_threshold, \" \", type, \" \", normal) #assign title\n",
    "xlab = paste0(\"LD1 \",signif(lda.model$svd[1]*lda.model$svd[1]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign x axis label, including Between:within variance\n",
    "ylab = paste0(\"LD2 \",signif(lda.model$svd[2]*lda.model$svd[2]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign y axis label, including Between:within variance\n",
    "plot_data <- as.data.frame(cbind(Isomer = my_data_train[,1], LD1 = Zcv.train[,1], LD2 = Zcv.train[,2]))  #create plot data data frame\n",
    "plot_data[,1] <- as.factor(plot_data[,1]) #save label info as factors\n",
    "plot_test <- as.data.frame(cbind(LD1 = Zcv.test[,1], LD2 = Zcv.test[,2]))  #create data frame of test data for plotting\n",
    "  \n",
    "if(!lda_threshold){   #if not using threshold \n",
    "predictions <- predict(lda.model, newdata = my_data_testset[,-1])$class #attempt to classify test set using highest posterior probability\n",
    "error <- signif(sum(as.numeric(predictions) != as.numeric(my_data_testset[,1]))/length(my_data_testset[,1]) *100,4) #error is number of times predictions didn't match labels, rounded to 4 sig figs\n",
    "xlim = mean(as.numeric(plot_data[,2]))  #for x location of test set error on plot\n",
    "ylim = min(as.numeric(plot_data[,3]))   #for y location of test set error on plot\n",
    "    \n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #create lda plot, LD1 vs LD2, colored as Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    geom_point(data = plot_test, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = \"Test Set\")) + #add test set points to plot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    annotate(\"label\", x = xlim, y = ylim, label = paste0(\"Test set error = \", error,\"%\")) #annotate with test set error rate\n",
    "    print(lda_plot)  #print plot to view\n",
    "ggexport(lda_plot, filename = lda_test_plot) #export plot\n",
    "}\n",
    "    \n",
    "if(lda_threshold){ #if using thresholds, create clean plot without error rate\n",
    "posterior <- predict(lda.model, newdata = my_data_testset[,-1])$posterior  #save posterior probabilities for test set\n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #create lda plot, LD1 vs LD2, colored as Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    geom_point(data = plot_test, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = \"Test Set\")) + #add test set points to plot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\"))   #black border\n",
    "    print(lda_plot)  #print plot to view\n",
    "ggexport(lda_plot, filename = lda_threshold_plot_test) #export plot\n",
    "    \n",
    "confusion <- matrix(0,nrow = 3, ncol = 10) #create empty confusion matrix\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\") #assign row names of confusion matrix\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #Assign thresholds as column names\n",
    "threshold = 0.50  #set starting posterior probability threshold\n",
    "    for(t in 1:ncol(confusion)){ #for every posterior probability threshold\n",
    "          threshold_result <- matrix(0,nrow = nrow(posterior), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(posterior)){ #for every sample\n",
    "                 if(max(posterior[z,])< threshold){ #if the highest posterior probability is under the threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, \"1\" in column 2\n",
    "                } else {   #if highest posterior probability is >= the threshold\n",
    "                     threshold_result[z,1] <- paste0(which.max(posterior[z,]) +1,\"-\", type) #determine which isomer has the highest posterior probability and store as \"-type\" in column 1\n",
    "                }\n",
    "              }\n",
    "\n",
    "        confusion[1,t] <- mean(threshold_result[,1]==my_data_testset[,1]) #success rate\n",
    "        confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "        \n",
    "        if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "          confusion[3,t] <- mean(threshold_result[,1]!=my_data_testset[,1]) #error rate is mean of column 1 not equal to label vector\n",
    "        } else { #if there are inconclusives\n",
    "            check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "            confusion[3,t] <- sum(threshold_result[-check,1]!=my_data_testset[-check,1])/nrow(posterior) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "        threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "    }\n",
    "\n",
    "confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    "    if(compare_plot){ #if creating comparison plot\n",
    "      compare_plot_data[31:40, 4] <- confusion[1,]   #add success rate to compare plot data\n",
    "      compare_plot_data[41:50, 4] <- confusion[2,]   #add inconclusive rate to compare plot data\n",
    "      compare_plot_data[51:60, 4] <- confusion[3,]   #add error rate to compare plot data\n",
    "    }\n",
    "print(confusion) #print confusion matrix    \n",
    "name1 <- paste0(\"Test Set, \", ion_threshold, \" \", type, \" \",normal) #assign sheet name\n",
    "write.xlsx(confusion, lda_file, sheetName = name1, append = TRUE, showNA = FALSE, )\n",
    "}\n",
    "}     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Using Same day\n",
    "- LOOCV using only data from the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(LDA && sameday && !test_set){   #if performing LDA with leave-one-out on cards analyzed on the same day\n",
    "  \n",
    "if(nchar(as.character(my_data_sameday_byweek[1,1]))==1){   #if label vector entries are only one character \n",
    "my_data_sameday_byweek[,1] <- paste0(my_data_sameday_byweek[,1], \"-\", type) #add \"-type\"\n",
    "    }     \n",
    "\n",
    "predictions <- vector(length = nrow(my_data_sameday_byweek)) #create vector to store predictions\n",
    "posterior <- matrix(nrow = nrow(my_data_sameday_byweek), ncol = length(unique(Isomer))) #create matrix to store posterior probabilities  \n",
    "list_of_lda_plots <- vector(\"list\", length = weeks)          #create empty list for plots to live in (without error rate)\n",
    "list_of_lda_plots_loocv <- vector(\"list\", length = weeks)    #create empty list for plots to live in\n",
    "    \n",
    "for (i in 1:weeks) {                                         #for every week\n",
    "  start <- (nrow(my_data_sameday_byweek)/weeks*i)-(nrow(my_data_sameday_byweek)/weeks-1) #starting row of each week\n",
    "  finish <- (nrow(my_data_sameday_byweek)/weeks*i)           #ending row of each week\n",
    "  data <- my_data_sameday_byweek[start:finish,]              #isolate just week of interest\n",
    "  data_bycard <- arrange(data, Card)                         #arrange by Card\n",
    "\n",
    "    trainData <- data_bycard[,5:ncol(data_bycard)]           #training data is all columns of the same week, except label, week, card, volume info\n",
    "    trainlbl <- data_bycard[,1]                              #assigns labels\n",
    "    \n",
    "lda.model <- lda(trainData,trainlbl, prior = rep(1,length(unique(Isomer)))/length(unique(Isomer))) #create LDA model, equal prior probability for each Isomer\n",
    "Zcv.train <- predict(lda.model)$x                            #rotate training set into LDa space\n",
    "lda_cv <- lda(trainData, trainlbl, prior = rep(1,length(unique(Isomer)))/length(unique(Isomer)),CV=TRUE) #perform LOOCV\n",
    "   \n",
    "for (j in 1:length(lda_cv$class)) {                          #for every sample in the dataset\n",
    "    row <- ((i*nrow(my_data_sameday_byweek)/weeks)-nrow(my_data_sameday_byweek)/weeks)+j  #determine row to store results in   \n",
    "    predictions[row] <- paste(lda_cv$class[j])               #save predictions (paste so as to not save as \"level\")\n",
    "    posterior[row,] <- lda_cv$posterior[j,]                  #save posterior probabilities\n",
    "    }\n",
    "\n",
    "loocv <- signif(sum(predictions[start:finish] != trainlbl)/nrow(trainData)*100,4)  #store LOOCV error rate, rounded to 4 sig figs    \n",
    "title = paste0(\"Sameday - LDA \", ion_threshold, \" \", type, \" \", normal) #store plot title\n",
    "xlab = paste0(\"LD1 \",signif(lda.model$svd[1]*lda.model$svd[1]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign x axis label, including Between:within variance\n",
    "ylab = paste0(\"LD2 \",signif(lda.model$svd[2]*lda.model$svd[2]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign y axis label, including Between:within variance\n",
    "plot_data <- as.data.frame(cbind(Isomer = trainlbl, LD1 = Zcv.train[,1], LD2 = Zcv.train[,2])) #store plot data as data frame\n",
    "plot_data[,1] <- as.factor(plot_data[,1]) #make labels into factors\n",
    "    \n",
    "if(!lda_threshold){    #if using default LOOCV, not thresholds\n",
    "    xlim = mean(as.numeric(plot_data[,2]))   #create x location for loocv on plot\n",
    "    ylim = min(as.numeric(plot_data[,3]))    #create y location for loocv on plot\n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #plot rotated LD scores, color by Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    annotate(\"label\", x = xlim, y = ylim, label = paste0(\"LOOCV error = \", loocv,\"%\")) #annotate with LOOCV error rate\n",
    "    list_of_lda_plots_loocv[[i]] <- lda_plot  #store plot in vector list\n",
    "  }\n",
    "    \n",
    "if(lda_threshold){    #if using thresholds (clean plot)\n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #plot rotated LD scores, color by Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) #black border\n",
    "    list_of_lda_plots[[i]] <- lda_plot   #store plot in vector list  \n",
    "}\n",
    "}\n",
    "\n",
    "if(!lda_threshold){  #if using default LOOCV, not thresholds (has to be separate from above so list can be filled in)\n",
    "ggexport(plotlist = list_of_lda_plots_loocv, nrow = 2, ncol = 2, filename = lda_loocv_plot) #export plots with LOOCV error rate\n",
    "    }\n",
    "if(lda_threshold){  #is using thresholds\n",
    "ggexport(plotlist = list_of_lda_plots, nrow = 2, ncol = 2, filename = lda_threshold_plot)   #export clean plots\n",
    "lda_train_results <- vector(length = nrow(posterior)) #make vector to store classifications\n",
    "    for(i in 1:nrow(posterior)){ #for every sample\n",
    "        lda_train_results[i] <- paste0(which.max(posterior[i,])+1, \"-\", type) #store which had the highest posterior probability as \"-type\"\n",
    "    } \n",
    "lda_result_inthirds <- rep(c(paste0(\"2-\", type),paste0(\"3-\", type), paste0(\"4-\", type)), card) #create vector of suspected results per card\n",
    "    \n",
    "confusion <- matrix(NA,nrow = 15, ncol = 10) #create empty confusion matrix\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Average: All 3 match\", \"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Average: 2 match\", \"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Max per card\", \"Success rate\", \"Inconclusive rate\", \"Error rate\") #set row names for confusion matrix\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #Assign thresholds as column names\n",
    "threshold <- 0.50 #set initial threshold\n",
    "for(t in 1:ncol(confusion)){  #for every threshold\n",
    "#each spectrum individually\n",
    "          threshold_result <- matrix(0, nrow = nrow(posterior), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(posterior)){ #for every sample\n",
    "                  if(is.nan(posterior[z,1])){ #if the posterior probability is \"NaN\"\n",
    "                      threshold_result[z,2] <- 1 #result is inconclusive, \"1\" in column 2\n",
    "                  } else {\n",
    "                 if(max(posterior[z,]) < threshold){ #if the highest posterior probability is under the threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, \"1\" in column 2\n",
    "                } else { #if highest posterior probability is >= the threshold\n",
    "                     threshold_result[z,1] <- lda_train_results[z] #store classification in column 1\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "        confusion[1,t] <- mean(threshold_result[,1]==rep(trainlbl, weeks)) #success rate\n",
    "        confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "        \n",
    "        if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "          confusion[3,t] <- mean(threshold_result[,1]!=rep(trainlbl, weeks)) #error rate is mean of column 1 not equal to label vector\n",
    "        } else { #if there are inconclusives\n",
    "          check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "          confusion[3,t] <- sum(threshold_result[-check,1]!=rep(trainlbl, weeks)[-check])/nrow(posterior) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "\n",
    "#average, all 3 match\n",
    "threshold_result <- matrix(0, nrow = nrow(posterior)/3, ncol = 2)    #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2 #beginning of triplicate\n",
    "    finish <- (3*i)  #end of triplicate\n",
    "   if(length(unique(lda_train_results[start:finish])) > 1) {  #test if more than 1 compound was ID'd (all 3 don't match)\n",
    "       threshold_result[i,2] <-1 #inconclusive result\n",
    "      } else {  #if all three give the same conclusion\n",
    "       if(length(is.nan(posterior[start:finish,]))==9){ #if all samples in triplicate give NaN results\n",
    "           threshold_result[i,2] <-1  #result is inconclusive\n",
    "       } else {\n",
    "       if(mean(max(posterior[start:finish,]), na.rom = TRUE) < threshold){ #test if the average of all three is under the threshold\n",
    "        threshold_result[i,2] <-1  #if under the threshold, result is inconclusive\n",
    "       } else { #if average is over the threshold\n",
    "        threshold_result[i,1] <- lda_train_results[start] #store first result of triplicate (since they all match) in matrix\n",
    "       }\n",
    "   }\n",
    "}\n",
    "}\n",
    "     \n",
    "confusion[5,t] <- mean(threshold_result[,1]==lda_result_inthirds) #success rate #check how often the classified result matches the true result of the triplicate\n",
    "confusion[6,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[7,t] <- mean(threshold_result[,1]!=lda_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusives\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[7,t] <- sum(threshold_result[-check,1]!=lda_result_inthirds[-check])/length(lda_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        } \n",
    "    \n",
    "#average, 2-3 match\n",
    "threshold_result <- matrix(0, nrow = nrow(posterior)/3, ncol = 2)    #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2 #beginning of triplicate\n",
    "    finish <- (3*i)  #end of triplicate\n",
    "   if(length(unique(lda_train_results[start:finish])) > 2) { #test if all samples in the triplicate gave different results\n",
    "       threshold_result[i,2] <-1 #if so, result is inconclusive, 1 in column 2\n",
    "    } else {\n",
    "       check <- as.data.frame(lda_train_results[start:finish]) %>%  #count how many unique responses are in the triplicate (2 or less) and put the most common at the top \n",
    "                  add_count(lda_train_results[start:finish], sort = TRUE)\n",
    "       selected_rows <- which(lda_train_results[start:finish]==check[1]) #save the ID of the isomer identified at least twice\n",
    "       average_vector <- vector(length = length(selected_rows))     #make a vector to save the posterior for whichever samples in the triplicate were Id'd most frequently \n",
    "       for(j in 1:length(selected_rows)){ #for however many samples in the triplicate match the most common ID\n",
    "           average_vector[j] <- max(posterior[start-1+selected_rows[j],1:3]) #save the posterior probability in the average vector\n",
    "       }\n",
    "       if(is.nan(average_vector[1])){\n",
    "           threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else{\n",
    "       if(mean(average_vector, na.rm = TRUE) < threshold){ #if the average of the samples that match the most common ID is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the average meets or exceeds the threshold\n",
    "        threshold_result[i,1] <- lda_train_results[(start-1+selected_rows[1])] #save the result of the most common ID\n",
    "       }\n",
    "   }\n",
    "}\n",
    "}\n",
    "    \n",
    "confusion[9,t] <- mean(threshold_result[,1]==lda_result_inthirds) #success rate\n",
    "confusion[10,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[11,t] <- mean(threshold_result[,1]!=lda_result_inthirds)#error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusives\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[11,t] <- sum(threshold_result[-check,1]!=lda_result_inthirds[-check])/length(lda_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }\n",
    "    \n",
    "#Max of three\n",
    "threshold_result <- matrix(0, nrow = nrow(posterior)/3, ncol = 2)   #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2 #beginning of triplicate\n",
    "    finish <- (3*i)  #end of triplicate\n",
    "           if(length(is.nan(posterior[start:finish,]))==9){ #if all samples in triplicate give NaN results\n",
    "           threshold_result[i,2] <-1  #result is inconclusive\n",
    "       } else {\n",
    "       if(max(max(posterior[start:finish,], na.rm = TRUE)) < threshold){ #If the highest posterior probability of the triplicate is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the max meets or exceeds the threshold\n",
    "        check <- which.max(max(posterior[start:finish,])) #determine which sample of the triplicate is highest\n",
    "        threshold_result[i,1] <- paste0(which.max(posterior[(start-1+check),]) +1, \"-\", type) #save that sample's result as the result for the triplicate\n",
    "       }\n",
    "   }\n",
    "    }\n",
    "    \n",
    "confusion[13,t] <- mean(threshold_result[,1]==lda_result_inthirds) #success rate\n",
    "confusion[14,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){\n",
    "    confusion[15,t] <- mean(threshold_result[,1]!=lda_result_inthirds)#error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusives\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[15,t] <- sum(threshold_result[-check,1]!=lda_result_inthirds[-check])/length(lda_result_inthirds)#count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }\n",
    "        \n",
    "threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "}\n",
    "        \n",
    "confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    "    \n",
    "    if(compare_plot){ #if creating comparison plot\n",
    "      compare_plot_data[31:40, 4] <- confusion[1,]   #add success rate to compare plot data\n",
    "      compare_plot_data[41:50, 4] <- confusion[2,]   #add inconclusive rate to compare plot data\n",
    "      compare_plot_data[51:60, 4] <- confusion[3,]   #add error rate to compare plot data\n",
    "    }\n",
    "print(confusion)               #print confusion matrix\n",
    "name1 <- paste0(\"Loocv, \", ion_threshold, \" \", type, \" \",normal ) #assign sheet name\n",
    "write.xlsx(confusion, lda_file, sheetName = name1, append = TRUE, showNA = FALSE) #export to excel sheet\n",
    "}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "- Sameday as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(LDA && test_set && sameday){ #if doing LDA only, on sameday data, using each card as a tesset\n",
    "\n",
    "if(nchar(as.character(my_data_sameday_byweek[1,1]))==1){    #check if label vector is made up of scalar values\n",
    "my_data_sameday_byweek[,1] <- paste0(my_data_sameday_byweek[,1], \"-\", type) #add \"-type\" info\n",
    "}\n",
    "\n",
    "data_inthirds <- my_data_sameday_byweek[seq(1, nrow(my_data_sameday_byweek), 3),]     #Since data was run in triplicate, divided into thirds to test making only one conclusion per card\n",
    "\n",
    "list_of_lda_plots <- vector(\"list\", length = card)           #make empty list of plots\n",
    "posterior <- matrix(nrow = nrow(my_data_sameday_byweek), ncol = 3) #make empty matrix for posterior probabilities\n",
    "    \n",
    "for (i in 1:weeks) {                                         #for every week\n",
    "  start <- (nrow(my_data_sameday_byweek)/weeks*i)-(nrow(my_data_sameday_byweek)/weeks-1) #starting row of each week\n",
    "  finish <- (nrow(my_data_sameday_byweek)/weeks*i)           #ending row of each week\n",
    "  data <- my_data_sameday_byweek[start:finish,]              #isolate just week of interest\n",
    "  data_bycard <- arrange(data, Card)                         #arrange by Card\n",
    "\n",
    "for (p in 1:length(unique(Card))) {                          #for every card\n",
    "    testData <- data_bycard[((p*9)-8):(p*9),5:ncol(data_bycard)] #set each card as a test set\n",
    "    testlbl <- data_bycard[((p*9)-8):(p*9),1]                #assign labels for test set\n",
    "    trainData <- data_bycard[-(((p*9)-8):(p*9)),5:ncol(data_bycard)] #assign remaining cards of the week as training set\n",
    "    trainlbl <- data_bycard[-(((p*9)-8):(p*9)),1]            #assign labels for training set\n",
    "    plot_list_index <- (i*4)-4 + p                           #set location for plot in list \n",
    "    \n",
    "lda.model <- lda(trainData, trainlbl, prior = rep(1,length(unique(Isomer)))/length(unique(Isomer))) #perform LDA on training data\n",
    "Zcv.train <- predict(lda.model)$x                            #rotate training set into LD space\n",
    "Zcv.test <- predict(lda.model, newdata = testData)$x         #rotate test set into LD space  \n",
    "title = paste0(\"Sameday LDA \", ion_threshold, \" \", type, \" \", normal) #assign plot title\n",
    "xlab = paste0(\"LD1 \",signif(lda.model$svd[1]*lda.model$svd[1]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign x axis label, including Between:within variance\n",
    "ylab = paste0(\"LD2 \",signif(lda.model$svd[2]*lda.model$svd[2]/sum(lda.model$svd%*%lda.model$svd)*100,4), \"% of between:within variance\") #assign y axis label, including Between:within variance\n",
    "plot_data <- as.data.frame(cbind(Isomer = trainlbl, LD1 = Zcv.train[,1], LD2 = Zcv.train[,2])) #create plot data as data frame\n",
    "plot_data[,1] <- as.factor(plot_data[,1])                    #make labels into factors\n",
    "plot_test <- as.data.frame(cbind(LD1 = as.numeric(Zcv.test[,1]), LD2 = as.numeric(Zcv.test[,2]))) #create data frame for test set data\n",
    " \n",
    "if(!lda_threshold){ #if not using threshold, put error on plot\n",
    "xlim = mean(as.numeric(plot_data[,2])) #set x location for error\n",
    "ylim = min(as.numeric(plot_data[,3]))  #set y location for error\n",
    "predictions <- predict(lda.model, newdata = testData)$class #predict test set\n",
    "error <- signif(sum(predictions != testlbl)/length(testlbl) * 100,4) #calculate test set error\n",
    "    \n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #plot LD scores of training set, color by Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    geom_point(data = plot_test, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = \"Test Set\")) + #add LD scores of test set to plot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    annotate(\"label\", x = xlim, y = ylim, label = paste0(\"Test set error = \", error,\"%\"))  #add test set error as label\n",
    "  \n",
    "list_of_lda_plots[[plot_list_index]] <- lda_plot #add plot to list of plots\n",
    "}\n",
    "    \n",
    "if(lda_threshold){ #is using threshold, clean plot without error rate\n",
    "    \n",
    "lda_plot <- ggplot(data = plot_data, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = Isomer)) + #plot LD scores of training set, color by Isomer\n",
    "    geom_point() + #scatterplot\n",
    "    geom_point(data = plot_test, aes(x = as.numeric(LD1), y = as.numeric(LD2), color = \"Test Set\")) + #add LD scores of test set to plot\n",
    "    labs(x = xlab, y = ylab, title = title) + #label axes and title\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\"))   #black border\n",
    "\n",
    "list_of_lda_plots[[plot_list_index]] <- lda_plot #add plot to list of plots\n",
    "  for (j in 1:nrow(testData)){ #for each test sample\n",
    "    row <- ((i*nrow(my_data_test)/weeks)-nrow(my_data_test)/weeks)+(p*length(unique(Isomer))*3)-(length(unique(Isomer))*3)+j #row in posterior probability matrix (which week, plus which card, plus which sample)\n",
    "    posterior[row,] <- predict(lda.model, newdata = testData)$posterior[j,] #store posterior probabilities\n",
    "   }\n",
    "  }\n",
    " }\n",
    "}\n",
    "    \n",
    "if(!lda_threshold){ #if putting test set error on plot\n",
    "    ggexport(ggarrange(plotlist = list_of_lda_plots, nrow = 2, ncol = 2), filename = lda_test_plot)  #export plots\n",
    "    }\n",
    "    \n",
    "if(lda_threshold){  #if using thresholds and clean plots\n",
    "    ggexport(ggarrange(plotlist = list_of_lda_plots, nrow = 2, ncol = 2), filename = lda_threshold_plot_test) #export plots\n",
    "    lda_train_results <- vector(length = nrow(posterior)) #make vector to store classifications\n",
    "    for(i in 1:nrow(posterior)){ #for every sample\n",
    "        lda_train_results[i] <- paste0(which.max(posterior[i,])+1, \"-\", type) #store which had the highest posterior probability as \"-type\"\n",
    "    }\n",
    "\n",
    "lda_result_inthirds <- rep(c(paste0(\"2-\", type),paste0(\"3-\", type), paste0(\"4-\", type)), card) #create vector of suspected results per card\n",
    "    \n",
    "confusion <- matrix(NA,nrow = 15, ncol = 10) #create empty confusion matrix\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Average: All 3 match\", \"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Average: 2 match\", \"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Max per card\", \"Success rate\", \"Inconclusive rate\", \"Error rate\") #set row names for confusion matrix\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #set columns names of threshold levels\n",
    "threshold <- 0.50 #set initial threshold\n",
    "for(t in 1:ncol(confusion)){  #for every threshold\n",
    "#each spectrum individually\n",
    "          threshold_result <- matrix(0, nrow = nrow(posterior), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(posterior)){ #for every sample\n",
    "                 if(max(posterior[z,]) < threshold){ #if the highest posterior probability is under the threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, \"1\" in column 2\n",
    "                } else { #if highest posterior probability is >= the threshold\n",
    "                     threshold_result[z,1] <- lda_train_results[z] #store classification in column 1\n",
    "                }\n",
    "              }\n",
    "        confusion[1,t] <- mean(threshold_result[,1]==rep(testlbl, card)) #success rate\n",
    "        confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "        \n",
    "        if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "          confusion[3,t] <- mean(threshold_result[,1]!=rep(testlbl, card)) #error rate is mean of column 1 not equal to label vector\n",
    "        } else { #if there are inconclusives\n",
    "          check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "          confusion[3,t] <- sum(threshold_result[-check,1]!=rep(testlbl, card)[-check])/nrow(posterior) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "\n",
    "#average, all 3 match\n",
    "threshold_result <- matrix(0, nrow = nrow(posterior)/3, ncol = 2)    #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2 #beginning of triplicate\n",
    "    finish <- (3*i)  #end of triplicate\n",
    "   if(length(unique(lda_train_results[start:finish])) > 1) {  #test if more than 1 compound was ID'd (all 3 don't match)\n",
    "       threshold_result[i,2] <-1 #inconclusive result\n",
    "      } else {  #if all three give the same conclusion\n",
    "       if(mean(max(posterior[start:finish,])) < threshold){ #test if the average of all three is under the threshold\n",
    "        threshold_result[i,2] <-1  #if under the threshold, result is inconclusive\n",
    "       } else { #if average is over the threshold\n",
    "        threshold_result[i,1] <- lda_train_results[start] #store first result of triplicate (since they all match) in matrix\n",
    "       }\n",
    "   }\n",
    "}\n",
    "     \n",
    "confusion[5,t] <- mean(threshold_result[,1]==lda_result_inthirds) #success rate #check how often the classified result matches the true result of the triplicate\n",
    "confusion[6,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[7,t] <- mean(threshold_result[,1]!=lda_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusives\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[7,t] <- sum(threshold_result[-check,1]!=lda_result_inthirds[-check])/length(lda_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        } \n",
    "    \n",
    "#average, 2-3 match\n",
    "threshold_result <- matrix(0, nrow = nrow(posterior)/3, ncol = 2)    #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2 #beginning of triplicate\n",
    "    finish <- (3*i)  #end of triplicate\n",
    "   if(length(unique(lda_train_results[start:finish])) > 2) { #test if all samples in the triplicate gave different results\n",
    "       threshold_result[i,2] <-1 #if so, result is inconclusive, 1 in column 2\n",
    "    } else {\n",
    "       check <- as.data.frame(lda_train_results[start:finish]) %>%  #count how many unique responses are in the triplicate (2 or less) and put the most common at the top \n",
    "                  add_count(lda_train_results[start:finish], sort = TRUE)\n",
    "       selected_rows <- which(lda_train_results[start:finish]==check[1]) #save the ID of the isomer identified at least twice\n",
    "       average_vector <- vector(length = length(selected_rows))     #make a vector to save the posterior for whichever samples in the triplicate were Id'd most frequently \n",
    "       for(j in 1:length(selected_rows)){ #for however many samples in the triplicate match the most common ID\n",
    "           average_vector[j] <- max(posterior[start-1+selected_rows[j],1:3]) #save the posterior probability in the average vector\n",
    "       }\n",
    "       if(mean(average_vector) < threshold){ #if the average of the samples that match the most common ID is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the average meets or exceeds the threshold\n",
    "        threshold_result[i,1] <- lda_train_results[(start-1+selected_rows[1])] #save the result of the most common ID\n",
    "       }\n",
    "   }\n",
    "}\n",
    "    \n",
    "confusion[9,t] <- mean(threshold_result[,1]==lda_result_inthirds) #success rate\n",
    "confusion[10,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[11,t] <- mean(threshold_result[,1]!=lda_result_inthirds)#error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusives\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[11,t] <- sum(threshold_result[-check,1]!=lda_result_inthirds[-check])/length(lda_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }\n",
    "    \n",
    "#Max of three\n",
    "threshold_result <- matrix(0, nrow = nrow(posterior)/3, ncol = 2)   #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2 #beginning of triplicate\n",
    "    finish <- (3*i)  #end of triplicate\n",
    "       if(max(max(posterior[start:finish,])) < threshold){ #If the highest posterior probability of the triplicate is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the max meets or exceeds the threshold\n",
    "        check <- which.max(max(posterior[start:finish,])) #determine which sample of the triplicate is highest\n",
    "        threshold_result[i,1] <- lda_train_results[(start-1+check)] #save that sample's result as the result for the triplicate\n",
    "       }\n",
    "   }\n",
    "    \n",
    "confusion[13,t] <- mean(threshold_result[,1]==lda_result_inthirds) #success rate\n",
    "confusion[14,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){\n",
    "    confusion[15,t] <- mean(threshold_result[,1]!=lda_result_inthirds)#error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusives\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[15,t] <- sum(threshold_result[-check,1]!=lda_result_inthirds[-check])/length(lda_result_inthirds)#count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }\n",
    "        \n",
    "threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "}\n",
    "\n",
    "confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    "    if(compare_plot){ #if creating comparison plot\n",
    "      compare_plot_data[31:40, 4] <- confusion[1,]   #add success rate to compare plot data\n",
    "      compare_plot_data[41:50, 4] <- confusion[2,]   #add inconclusive rate to compare plot data\n",
    "      compare_plot_data[51:60, 4] <- confusion[3,]   #add error rate to compare plot data\n",
    "    }\n",
    "    print(confusion)\n",
    "    \n",
    "name1 <- paste0(\"Test Set, \", ion_threshold, \" \", type, \" \",normal ) #assign sheet name\n",
    "write.xlsx(confusion, lda_file, sheetName = name1, append = TRUE, showNA = FALSE) #write to excel sheet\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Analysis Using Full Dataset\n",
    "- Plot Importance of Variables\n",
    "- Plot top 6 most important variables as density plots\n",
    "- Include default confusion matrix or success/inconclusive/error rate table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(random_forest && !test_set && !sameday) {   #if conducting Random Forest analysis, not restricted\n",
    "    list_of_plots <- vector(\"list\", length = 6) #create empty vector to store plots (of six most important variables)\n",
    "    \n",
    "Isomer_rf <- randomForest(my_data_test[,-1], y=as.factor(my_data_test[,1]), keep.forest = TRUE, classwt = c(rep(1/length(unique(Isomer)),length(unique(Isomer)))), importance = TRUE) #conduct RF analysis, default trees (500), default mtry (sqrt(# of variables)), store \"votes\" for OOB samples as proportion of trees\n",
    "\n",
    "if(nchar(as.character(my_data_test[1,1]))==1){      #if number of character in first index of label vector is one\n",
    "my_data_test[,1] <- paste0(my_data_test[,1], \"-\", type) #add \"-type\" information\n",
    "}\n",
    "    \n",
    "if(compare_methods){ #if comparing variable importance by method\n",
    "    compare_matrix[4,] <- Isomer_rf$importance[,(length(unique(Isomer))+1)] #store MeanDecreaseAccuracy values for each variable\n",
    "    #len_rf <- sqrt(compare_matrix[3,]%*%compare_matrix[3,])   #if normalizing to unit vector\n",
    "    #compare_matrix[3,] <- compare_matrix[3,]/ rep(len_rf, ncol(compare_matrix))\n",
    "    print(compare_matrix)\n",
    "}        \n",
    "        \n",
    "imp <- as.data.frame(varImpPlot(Isomer_rf, sort = TRUE, scale = FALSE, type = 1, main = paste0(type, \" \", ion_threshold, \"%, \", normal, \", Variable Importance\"), pch = 16))  #store the default style importance plot of RF variables in data frame\n",
    "imp$varnames <- rownames(imp)  #make clumn of variable names\n",
    "imp$Voltage <- as.factor(c(rep(\"30V\",n_30),rep(\"60V\",n_60), rep(\"90V\",n_90))) #make column of voltages\n",
    "imp <- arrange(imp, desc(MeanDecreaseAccuracy)) #arrange in decreasing order of mean decrease accuracy \n",
    "     \n",
    "if(threshold_rf_confusion){ #if making confusion matrix based on threshold, instead of default RF style\n",
    "confusion <- matrix(0,nrow = 3, ncol = 10) #create empty confusion matrix\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\") #assign row names of confusion matrix\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #Assign thresholds as column names\n",
    "\n",
    "threshold <- 0.50 #set starting proportion of decision trees threshold\n",
    "for(t in 1:ncol(confusion)){ #for every proportion of trees threshold\n",
    "          threshold_result <- matrix(0,nrow = nrow(Isomer_rf$votes), ncol = 2)  #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(Isomer_rf$votes)){   #for every sample\n",
    "                 if(max(Isomer_rf$votes[z,])< threshold){  #check if the isomer with the highest proportion of classifying OOB decision trees is lower than threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, 1 in column 2\n",
    "                } else { #if highest proportion of classifying OOB decision trees is higher than threshold\n",
    "                     threshold_result[z,1] <- paste0(which.max(Isomer_rf$votes[z,]) +1,\"-\", type) #store classification in column 1 as \"-type\"\n",
    "                }\n",
    "              }\n",
    "\n",
    "        confusion[1,t] <- mean(threshold_result[,1]==my_data_test[,1]) #success rate\n",
    "        confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "    \n",
    "        if(sum(as.numeric(threshold_result[,2]))==0){#if there are no inconclusives\n",
    "          confusion[3,t] <- mean(threshold_result[,1]!=my_data_test[,1]) #error rate is mean of column 1 not equal to label vector\n",
    "        } else { #if there are inconclusives\n",
    "          check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate  \n",
    "          confusion[3,t] <- sum(threshold_result[-check,1]!=my_data_test[-check,1])/nrow(Isomer_rf$votes) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "        threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "    }\n",
    "\n",
    "        confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    "        min <- 0.5 * max(imp$MeanDecreaseAccuracy)     #set x starting location for table on plot\n",
    "        max <- 0.6 * max(imp$MeanDecreaseAccuracy)     #set x ending location for table on plot\n",
    "        min2 <- as.integer(0.85* length(imp$varnames)) #set y starting location for table on plot\n",
    "        max2 <- as.integer(0.9* length(imp$varnames))  #set y ending location for table on plot\n",
    "        table_title <- \"Threshold Success/ Error Rates\"#set table title\n",
    "    }\n",
    "    \n",
    "    if(default_rf_confusion){  #if using RF default confusion matrix\n",
    "        confusion <- data.frame(Isomer_rf$confusion) #save default confusion matrix as data frame\n",
    "        colnames(confusion) <- c(rownames(confusion), \"Class Error\") #make the column name match the row names plus \"Class Error\"\n",
    "        confusion[,4] <- signif(confusion[,4],2) #round to 2 significant figures\n",
    "        min <- 0.7 * max(imp$MeanDecreaseAccuracy)     #set x starting location for table on plot\n",
    "        max <- 0.8 * max(imp$MeanDecreaseAccuracy)     #set x ending location for table on plot\n",
    "        min2 <- as.integer(0.8* length(imp$varnames))  #set y starting location for table on plot\n",
    "        max2 <- as.integer(0.9* length(imp$varnames))  #set y ending location for table on plot\n",
    "        table_title <- \"Confusion Matrix\"              #set table title\n",
    "    }\n",
    "    \n",
    "print(confusion) #print confusion matrix\n",
    "    if(compare_plot){ #if creating comparison plot\n",
    "      compare_plot_data[61:70, 4] <- confusion[1,]   #add success rate to compare plot data\n",
    "      compare_plot_data[71:80, 4] <- confusion[2,]   #add inconclusive rate to compare plot data\n",
    "      compare_plot_data[81:90, 4] <- confusion[3,]   #add error rate to compare plot data\n",
    "    }\n",
    "    \n",
    "rf_plot <- ggplot(imp, aes(x=reorder(varnames,MeanDecreaseAccuracy),y=MeanDecreaseAccuracy, color = Voltage)) + #plot Importance plot as ggplot style, color variables by voltage\n",
    "  geom_point() + #scatterplot\n",
    "  geom_segment(aes(x=varnames, xend=varnames,y=0,yend=MeanDecreaseAccuracy)) + #points on ends of lines \n",
    "  labs(y= \"MeanDecreaseAccuracy\", title = paste0(type, \" Random Forest Permutated Variable Importance \", normal, \" \",ion_threshold), x = \"m/z + Voltage\") + #label axes and title\n",
    "  theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "  theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "  theme(text = element_text(size = 8)) + #font size\n",
    "  theme(legend.position=\"bottom\", legend.key.size = unit(8,\"pt\"))+ #set legend beneath plot, set text size\n",
    "  coord_flip() #flip the axes (to match default style)\n",
    "    \n",
    "ggexport(rf_plot, filename = rf_plot_name) #export plot\n",
    "    \n",
    "for (i in 1:length(list_of_plots)){ #for each of the top six most important variables\n",
    "  data_plot <- cbind(my_data_test[1],my_data_test[imp$varnames[i]]) #start a data frame for plotting, first column is isomer info, second column is ith variable (ranked by decreasing importance)\n",
    "  colnames(data_plot) <- c(\"Isomer\",imp$varnames[i]) #set column names as Isomer and the ith most important variables\n",
    "  title <- imp$varnames[i] #title of the plot is the ith most important variable\n",
    "  test.aov <- aov(data_plot[,-1] ~ data_plot[,1]) #in order to include the ANOVA result on the plot, perform ANOVA based on isomer on the ith variable\n",
    "  text <- paste0(\"ANOVA p-value = \", signif(summary(test.aov)[[1]][[\"Pr(>F)\"]][1], digits = 3)) #store the ANOVA p-value round to 3 significant figures\n",
    "graphy <- ggplot(data_plot, aes_string(x = data_plot[,2], color = colnames(data_plot[1]), fill = colnames(data_plot[1]))) + #create distribution plot for ith most important variable\n",
    "    geom_density(alpha = 0.4, aes(y= ..scaled..)) + labs(x = text, title = title) + #density plot style, scaled to 1 \n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    theme(text = element_text(size = 7)) + #text size\n",
    "    theme(legend.key.size = unit(7,\"pt\")) #legend text size\n",
    "  list_of_plots[[i]] <- graphy #store plot in ith position of plot list\n",
    "} \n",
    "confusion<-signif(confusion,2) #round confusion matrix to 2 significant figures    \n",
    "cm_table <- tableGrob(confusion,  #create a table grob with the confusion matrix in order to add to the RF Importance plot\n",
    "                      theme = ttheme_default(base_size = 9, padding = unit(c(2,2), \"mm\"))) #default theme, set text size and white space within cells\n",
    "cm_table <- gtable_add_grob(cm_table, #set location of border lines of table \n",
    "                            grobs = rectGrob(gp = gpar(fill = NA, lwd = 1.3)), #lwd is width of line, thinner line than outside border\n",
    "                            t=2, b=nrow(cm_table), l=1, r = ncol(cm_table))    #top of rectangle is under the header line, other lines are full border\n",
    "cm_table <- gtable_add_grob(cm_table, #set location of border lines of table \n",
    "                            grobs = rectGrob(gp = gpar(fill = NA, lwd = 1.3)), #lwd is width of line, thinner line than outside border\n",
    "                            t=1, b=nrow(cm_table), l=2, r = ncol(cm_table))    #left of rectangle is after row names column\n",
    "cm_table <- gtable_add_grob(cm_table, #set location of border lines of table \n",
    "                            grobs = rectGrob(gp = gpar(fill = NA, lwd = 2)),   #thicker line width\n",
    "                            t=1, b=nrow(cm_table), l=1,r=ncol(cm_table))       #outside edge of table\n",
    "h <- grobHeight(cm_table) #store table height\n",
    "w <- grobWidth(cm_table)  #store table width\n",
    "\n",
    "cm_title <- textGrob(table_title, y = unit(0.5, \"npc\") + 1.1*h, vjust = -0.5, hjust = 0.5, gp = gpar(fontsize = 11)) #create a textGrob for table title, set location based on height/width of table\n",
    "cm <- gTree(children = gList(cm_table, cm_title)) #combine table and title into one unit\n",
    "\n",
    "ggexport(ggarrange(plotlist = list_of_plots, ncol = 3, nrow = 2), #export and arrange the 6 most important variable density plots and the \n",
    "         rf_plot +  #RF importance plot\n",
    "           annotation_custom(cm, xmin = imp$varnames[min2], xmax = imp$varnames[max2], ymin = min, ymax = max), #annotate rf_plot with confusion matrix at previously specified location\n",
    "         nrow = 2,filename = rf_plot_name_combined) #export\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "- Same day Analysis\n",
    "- Each sample out-of-bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(random_forest && sameday && !test_set){ #if performing random forest analysis on sameday OOB samples\n",
    "\n",
    "if(nchar(as.character(my_data_sameday_byweek[1,1]))==1){      #if number of character in first index of label vector is one\n",
    "my_data_sameday_byweek[,1] <- paste0(my_data_sameday_byweek[,1], \"-\", type) #add \"-type\" information\n",
    "}    \n",
    "    \n",
    "rf_train_results <- matrix(nrow = nrow(my_data_sameday_byweek), ncol = 6)    #create matrix for storing results\n",
    "colnames(rf_train_results) <- c(\"2\",\"3\",\"4\",\"Assigned\", \"Actual\",\"Max\")      #assign column names of result matrix\n",
    "data_inthirds <- my_data_sameday_byweek[seq(1, nrow(my_data_sameday_byweek), 3),]  #in order to make conclusions based on each triplicate, store every third row    \n",
    "    \n",
    "for (i in 1:weeks) {\n",
    "  start <- (nrow(my_data_sameday_byweek)/weeks*i)-(nrow(my_data_sameday_byweek)/weeks-1) #starting row of each week\n",
    "  finish <- (nrow(my_data_sameday_byweek)/weeks*i)           #ending row of each week\n",
    "  data <- my_data_sameday_byweek[start:finish,]              #isolate just week of interest\n",
    "  data_bycard <- arrange(data, Card)                         #arrange by Card\n",
    "\n",
    "    trainData <- data_bycard[,5:ncol(data_bycard)]           #training data is all columns of the same week, except label, week, card, volume info\n",
    "    trainlbl <- data_bycard[,1]                              #assigns labels\n",
    "    \n",
    "Isomer_rf <- randomForest::randomForest(trainData, y=as.factor(trainlbl), keep.forest = TRUE, classwt = c(rep(1/length(unique(Isomer)),length(unique(Isomer)))), importance = TRUE) #conduct RF analysis, default trees (500), default mtry (sqrt(# of variables)), store \"votes\" for OOB samples as proportion of trees\n",
    "    for (j in 1:nrow(Isomer_rf$votes)) { #for each sample\n",
    "        \n",
    "    row <- ((finish-(nrow(my_data_sameday_byweek))/weeks))+j  #set row for sample results information \n",
    "      print(row)\n",
    "        rf_train_results[row,1:3] <- Isomer_rf$votes[j,1:3]   #store proportion of decision trees\n",
    "      if(length(which(Isomer_rf$votes[j,]==max(Isomer_rf$votes[j,])))==1){ #if there is one clear most commonly identified isomer\n",
    "        rf_train_results[row,4] <- paste0(which(Isomer_rf$votes[j,]==max(Isomer_rf$votes[j,]))+1, \"-\", type) #save assigned isomer as\"-type\"\n",
    "      }\n",
    "      rf_train_results[row,5] <- trainlbl[j] #store correct isomer ID\n",
    "      rf_train_results[row,6] <- max(Isomer_rf$votes[j,]) #store higher proportion of trees\n",
    "    }\n",
    "  }\n",
    "  \n",
    "rf_train_results <- as.data.frame(rf_train_results)      #convert to dataframe\n",
    "rf_train_results[,6] <- as.numeric(rf_train_results[,6]) #store maximum proportion as numeric\n",
    "rf_train_results[,1] <- as.numeric(rf_train_results[,1]) #store proportions as numeric\n",
    "rf_train_results[,2] <- as.numeric(rf_train_results[,2]) #store proportions as numeric\n",
    "rf_train_results[,3] <- as.numeric(rf_train_results[,3]) #store proportions as numeric\n",
    "rf_result_inthirds <- rf_train_results[seq(1,nrow(rf_train_results),3),5]    #store correct isomer ID per triplicate\n",
    "threshold <- 0.50 #set starting proportion of classifying trees threshold\n",
    "confusion <- matrix(NA, nrow = 15, ncol = 10) #create empty matrix for results\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Average: All 3 match\", \"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Average: 2 match\", \"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Max per card\", \"Success rate\", \"Inconclusive rate\", \"Error rate\") #set row names for confusion matrix\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #set columns names of threshold levels\n",
    "\n",
    "for(t in 1:ncol(confusion)){ #for every threshold\n",
    "#each spectrum individually\n",
    "          threshold_result <- matrix(0, nrow = nrow(rf_train_results), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(rf_train_results)){ #for every sample\n",
    "                  if(length(which(rf_train_results[z,1:3]==max(rf_train_results[z,1:3])))>1){\n",
    "                     threshold_result[z,2] <- 1 #result is inconclusive, 1 in column 2 \n",
    "                  } else {\n",
    "                 if(rf_train_results[z,6] < threshold){ #if the max proportion of classifying trees is under the threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, 1 in column 2\n",
    "                } else { #if proportion is over threshold\n",
    "                     threshold_result[z,1] <- rf_train_results[z,4] #store classification in column 1\n",
    "                }\n",
    "              }\n",
    "                  }\n",
    "confusion[1,t] <- mean(threshold_result[,1]==rf_train_results[,5]) #success rate\n",
    "confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[3,t] <- mean(threshold_result[,1]!=rf_train_results[,5]) #error rate is mean of assigned class not equal to correct class\n",
    "    } else {\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[3,t] <- sum(threshold_result[-check,1]!=rf_train_results[-check,5])/nrow(rf_train_results) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "\n",
    "#average, all 3 match\n",
    "threshold_result <- matrix(0,nrow = nrow(rf_train_results)/3, ncol = 2) #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2   #beginning of triplicate\n",
    "    finish <- (3*i)    #end of triplicate\n",
    "   if(length(unique(rf_train_results[start:finish,4])) > 1) { #test if more than 1 compound was ID'd (all 3 don't match)\n",
    "       threshold_result[i,2] <-1 #inconclusive result\n",
    "    } else {  #if all three give the same conclusion\n",
    "       if(mean(rf_train_results[start:finish,6]) < threshold){ #test if the average of all three is under the threshold\n",
    "        threshold_result[i,2] <-1   #if under the threshold, result is inconclusive\n",
    "       } else {    #if average is over the threshold\n",
    "        threshold_result[i,1] <- rf_train_results[start,4] #store first result of triplicate (since they all match) in matrix\n",
    "       }\n",
    "   }\n",
    "}\n",
    "      \n",
    "confusion[5,t] <- mean(threshold_result[,1]==rf_result_inthirds) #success rate, check how often the classified result matches the true result of the triplicate\n",
    "confusion[6,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusive\n",
    "    confusion[7,t] <- mean(threshold_result[,1]!=rf_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusive\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[7,t] <- sum(threshold_result[-check,1]!=rf_result_inthirds[-check])/length(rf_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        } \n",
    "    \n",
    "#average, 2-3 match\n",
    "threshold_result <- matrix(0,nrow = nrow(rf_train_results)/3, ncol = 2) #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2   #beginning of triplicate\n",
    "    finish <- (3*i)    #end of triplicate\n",
    "   if(length(unique(rf_train_results[start:finish,4])) > 2) { #test if all samples in the triplicate gave different results\n",
    "       threshold_result[i,2] <-1 #if so, result is inconclusive, 1 in column 2\n",
    "    } else {\n",
    "        check <- as.data.frame(rf_train_results[start:finish,]) %>% #count how many unique responses are in the triplicate (2 or less) and put the most common at the top \n",
    "                  add_count(Assigned, sort = TRUE)\n",
    "        selected_rows <- which(rf_train_results[start:finish,4]==check[1,4]) #save the ID of the isomer identified at least twice\n",
    "        average_vector <- vector(length = length(selected_rows)) #make a vector to save the proportion of trees for whichever samples in the triplicate were Id'd most frequently \n",
    "       for(j in 1:length(selected_rows)){ #for however many samples in the triplicate match the most common ID\n",
    "           average_vector[j] <- rf_train_results[start-1+selected_rows[j],6] #save the proportion of trees in the average vector\n",
    "       }\n",
    "       if(mean(average_vector) < threshold){ #if the average of the samples that match the most common ID is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the average meets or exceeds the threshold\n",
    "        threshold_result[i,1] <- rf_train_results[(start-1+selected_rows[1]),5]#save the result of the most common ID\n",
    "       }\n",
    "   }\n",
    "}\n",
    "   \n",
    "confusion[9,t] <- mean(threshold_result[,1]==rf_result_inthirds) #success rate\n",
    "confusion[10,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[11,t] <- mean(threshold_result[,1]!=rf_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusive\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[11,t] <- sum(threshold_result[-check,1]!=rf_result_inthirds[-check])/length(rf_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }\n",
    "    \n",
    "#Max of three\n",
    "threshold_result <- matrix(0,nrow = nrow(rf_train_results)/3, ncol = 2)  #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2   #beginning of triplicate\n",
    "    finish <- (3*i)    #end of triplicate\n",
    "       if(max(rf_train_results[start:finish,6]) < threshold){#If the highest proportion of trees of the triplicate is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the max meets or exceeds the threshold\n",
    "        check <- which.max(rf_train_results[start:finish,6]) #determine which sample of the triplicate is highest\n",
    "        threshold_result[i,1] <- rf_train_results[(start-1+check),5] #save the result of the ID with the highest proportion of trees\n",
    "       }\n",
    "   }\n",
    "    \n",
    "confusion[13,t] <- mean(threshold_result[,1]==rf_result_inthirds) #success rate\n",
    "confusion[14,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[15,t] <- mean(threshold_result[,1]!=rf_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusive\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[15,t] <- sum(threshold_result[-check,1]!=rf_result_inthirds[-check])/length(rf_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }    \n",
    "        \n",
    "threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "}\n",
    "\n",
    "confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    " if(compare_plot){ #if creating comparison plot\n",
    "      compare_plot_data[61:70, 4] <- confusion[1,]   #add success rate to compare plot data\n",
    "      compare_plot_data[71:80, 4] <- confusion[2,]   #add inconclusive rate to compare plot data\n",
    "      compare_plot_data[81:90, 4] <- confusion[3,]   #add error rate to compare plot data\n",
    "    }\n",
    "print(confusion) #print confusion matrix\n",
    "    \n",
    "name1 <- paste0(\"Sameday, OOB, \", ion_threshold, \" \", type, \" \",normal ) #assign sheet name\n",
    "write.xlsx(confusion, rf_sameday_file, sheetName = name1, append = TRUE, showNA = FALSE) #write to excel\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Analysis using test vs training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(random_forest && test_set && !sameday){  #if conducting random forest using test and training sets\n",
    "  \n",
    "set.seed(123) #set seed for reproducible random test set selection\n",
    "if(nchar(as.character(my_data_test[1,1]))==1){    #if label vector entries are only one character   \n",
    "my_data_test[,1] <- paste0(as.numeric(my_data_test[,1]), \"-\", type) #add \"-type\"\n",
    "}\n",
    "  \n",
    "sample <- sample(1:nrow(my_data_test), round(0.20*(nrow(my_data_test))))  #randomly sample 20% of the dataset indices\n",
    "my_data_train <- my_data_test[-sample,] #all but randomly chosen indices become training set\n",
    "my_data_testset <- my_data_test[sample,]#randomly chosen indices become test set  \n",
    "   \n",
    "list_of_plots <- vector(\"list\", length = 6) #create empty vector to store plots (of six most important variables)\n",
    "    \n",
    "Isomer_rf <- randomForest(my_data_train[,-1], y=as.factor(my_data_train[,1]), xtest = my_data_testset[,-1], ytest = as.factor(my_data_testset[,1]), keep.forest = TRUE, classwt = c(rep(1/length(unique(Isomer)),length(unique(Isomer)))), importance = TRUE) #conduct RF analysis, default trees (500), default mtry (sqrt(# of variables)), store \"votes\" for OOB samples as proportion of trees, training and test set\n",
    "    \n",
    "imp <- as.data.frame(varImpPlot(Isomer_rf, sort = TRUE, scale = FALSE, type = 1, main = paste0(type, \" \", ion_threshold, \"%, \", normal, \", Variable Importance\"), pch = 16))  #store the default style importance plot of RF variables in data frame\n",
    "imp$varnames <- rownames(imp)  #make clumn of variable names\n",
    "imp$Voltage <- as.factor(c(rep(\"30V\",n_30),rep(\"60V\",n_60), rep(\"90V\",n_90))) #make column of voltages\n",
    "imp <- arrange(imp, desc(MeanDecreaseAccuracy)) #arrange in decreasing order of mean decrease accuracy \n",
    "      \n",
    "if(threshold_rf_confusion){ #if making confusion matrix based on threshold, instead of default RF style\n",
    "confusion <- matrix(0,nrow = 3, ncol = 10) #create empty confusion matrix\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\") #assign row names of confusion matrix\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #Assign thresholds as column names\n",
    "\n",
    "threshold <- 0.50 #set starting proportion of decision trees threshold\n",
    "for(t in 1:ncol(confusion)){ #for every proportion of trees threshold\n",
    "          threshold_result <- matrix(0,nrow = nrow(Isomer_rf$votes), ncol = 2)  #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(Isomer_rf$votes)){   #for every sample\n",
    "                 if(max(Isomer_rf$votes[z,])< threshold){  #check if the isomer with the highest proportion of classifying OOB decision trees is lower than threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, 1 in column 2\n",
    "                } else { #if highest proportion of classifying OOB decision trees is higher than threshold\n",
    "                     threshold_result[z,1] <- paste0(which.max(Isomer_rf$votes[z,]) +1,\"-\", type) #store classification in column 1 as \"-type\"\n",
    "                }\n",
    "              }\n",
    "\n",
    "        confusion[1,t] <- mean(threshold_result[,1]==my_data_train[,1]) #success rate\n",
    "        confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "    \n",
    "        if(sum(as.numeric(threshold_result[,2]))==0){#if there are no inconclusives\n",
    "          confusion[3,t] <- mean(threshold_result[,1]!=my_data_train[,1]) #error rate is mean of column 1 not equal to label vector\n",
    "        } else { #if there are inconclusives\n",
    "          check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate  \n",
    "          confusion[3,t] <- sum(threshold_result[-check,1]!=my_data_train[-check,1])/nrow(Isomer_rf$votes) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "        threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "    }\n",
    "        \n",
    "threshold <- 0.50 #set starting proportion of decision trees threshold for test set\n",
    "confusion_test <- matrix(0,nrow = 3, ncol = 10)\n",
    "row.names(confusion_test) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\") #assign row names of confusion matrix\n",
    "colnames(confusion_test) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #Assign thresholds as column names\n",
    "         \n",
    "        for(t in 1:ncol(confusion_test)){ #for every proportion of trees threshold\n",
    "          threshold_result <- matrix(0,nrow = nrow(Isomer_rf$test$votes), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(Isomer_rf$test$votes)){ #for every sample\n",
    "                 if(max(Isomer_rf$test$votes[z,])< threshold){ #check if the isomer with the highest proportion of classifying decision trees is lower than threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, 1 in column 2\n",
    "                    } else { #if highest proportion of classifying OOB decision trees is higher than threshold\n",
    "                     threshold_result[z,1] <- paste0(which.max(Isomer_rf$test$votes[z,]) +1,\"-\", type) #store classification in column 1 as \"-type\"\n",
    "                    }\n",
    "                  }\n",
    "\n",
    "        confusion_test[1,t] <- mean(threshold_result[,1]==my_data_testset[,1]) #success rate\n",
    "        confusion_test[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "        if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "          confusion_test[3,t] <- mean(threshold_result[,1]!=my_data_testset[,1])#error rate is mean of column 1 not equal to label vector\n",
    "        } else { #if there are inconclusives\n",
    "          check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate  \n",
    "          confusion_test[3,t] <- sum(threshold_result[-check,1]!=my_data_testset[-check,1])/nrow(Isomer_rf$test$votes)#count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "        threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "        }\n",
    "\n",
    "        confusion<-signif(confusion,2) #round confusion matrix to 2 significant figures\n",
    "        confusion_test<-signif(confusion_test,3) #round confusion matrix for test set to 3 significant figures  \n",
    "if(compare_plot){ #if creating comparison plot\n",
    "      compare_plot_data[61:70, 4] <- confusion_test[1,]   #add success rate to compare plot data\n",
    "      compare_plot_data[71:80, 4] <- confusion_test[2,]   #add inconclusive rate to compare plot data\n",
    "      compare_plot_data[81:90, 4] <- confusion_test[3,]   #add error rate to compare plot data\n",
    "    }\n",
    "        min <- 0.6 * max(imp$MeanDecreaseAccuracy) #set x starting location for both tables on plot\n",
    "        max <- 0.7 * max(imp$MeanDecreaseAccuracy) #set x ending location for both tables on plot\n",
    "        min2 <- as.integer(0.85* length(imp$varnames)) #set y starting location for training table on plot\n",
    "        max2 <- as.integer(0.95* length(imp$varnames)) #set y ending location for training table on plot\n",
    "        min2_test <- round(0.45* length(imp$varnames)) #set y starting location for test table on plot\n",
    "        max2_test <- round(0.5* length(imp$varnames))  #set y ending location for trest table on plot\n",
    "        table_title <- \"Threshold Success/ Error Rates - Training\" #assign title for training table\n",
    "        table_title_test <- \"Threshold Success/ Error Rates - Test\"#assign title for test table\n",
    "    }\n",
    "    \n",
    "    if(default_rf_confusion){ #if using default style confusion matrices\n",
    "        confusion <- data.frame(Isomer_rf$confusion) #save default confusion matrix for OOB training samples\n",
    "        colnames(confusion) <- c(rownames(confusion), \"Class Error\") #set column names\n",
    "        confusion[,4] <- signif(confusion[,4],2) #round class error to 2 significant figures\n",
    "        confusion_test <- data.frame(Isomer_rf$test$confusion) #save default confusion matrix for test set samples\n",
    "        colnames(confusion_test) <- c(rownames(confusion_test), \"Class Error\") #set column names\n",
    "        confusion_test[,4] <- signif(confusion_test[,4],2) #round class error to 2 significant figures\n",
    "        min <- 0.6 * max(imp$MeanDecreaseAccuracy) #set x starting location for both tables on plot\n",
    "        max <- 0.7 * max(imp$MeanDecreaseAccuracy) #set x ending location for both tables on plot\n",
    "        min2 <- as.integer(0.8* length(imp$varnames)) #set y starting location for training table on plot\n",
    "        max2 <- as.integer(0.9* length(imp$varnames)) #set y ending location for training table on plot\n",
    "        min2_test <- round(0.4* length(imp$varnames)) #set y starting location for test table on plot\n",
    "        max2_test <- round(0.5* length(imp$varnames))  #set y ending location for trest table on plot\n",
    "        table_title <- \"Threshold Success/ Error Rates - Training\" #assign title for training table\n",
    "        table_title_test <- \"Threshold Success/ Error Rates - Test\"#assign title for test table\n",
    "    }\n",
    "    \n",
    "print(confusion) #print training confusion matrix\n",
    "print(confusion_test) #print test confusion matrix\n",
    "\n",
    "rf_plot <- ggplot(imp, aes(x=reorder(varnames,MeanDecreaseAccuracy),y=MeanDecreaseAccuracy, color = Voltage)) + #plot Importance plot as ggplot style, color variables by voltage\n",
    "  geom_point() + #scatterplot\n",
    "  geom_segment(aes(x=varnames, xend=varnames,y=0,yend=MeanDecreaseAccuracy)) + #points on ends of lines \n",
    "  labs(y= \"MeanDecreaseAccuracy\", title = paste0(type, \" Random Forest Permutated Variable Importance \", normal, \" \",ion_threshold), x = \"m/z + Voltage\") + #label axes and title\n",
    "  theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "  theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "  theme(text = element_text(size = 8)) + #font size\n",
    "  theme(legend.position=\"bottom\", legend.key.size = unit(8,\"pt\"))+ #set legend beneath plot, set text size\n",
    "  coord_flip() #flip the axes (to match default style) \n",
    "    \n",
    "for (i in 1:length(list_of_plots)){ #for each of the top six most important variables\n",
    "  data_plot <- cbind(my_data_train[1],my_data_train[imp$varnames[i]]) #start a data frame for plotting, first column is isomer info, second column is ith variable (ranked by decreasing importance)\n",
    "  colnames(data_plot) <- c(\"Isomer\",imp$varnames[i]) #set column names as Isomer and the ith most important variable\n",
    "  title <- imp$varnames[i] #title of the plot is the ith most important variable\n",
    "  test.aov <- aov(data_plot[,-1] ~ data_plot[,1]) #in order to include the ANOVA result on the plot, perform ANOVA based on isomer on the ith variable (only training set)\n",
    "  text <- paste0(\"ANOVA p-value = \", signif(summary(test.aov)[[1]][[\"Pr(>F)\"]][1], digits = 3)) #store the ANOVA p-value round to 3 significant figures\n",
    "graphy <- ggplot(data_plot, aes_string(x = data_plot[,2], color = colnames(data_plot[1]), fill = colnames(data_plot[1]))) + #create distribution plot for ith most important variable\n",
    "    geom_density(alpha = 0.4, aes(y= ..scaled..)) + labs(x = text, title = title) + #density plot style, scaled to 1\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    theme(text = element_text(size = 7)) + #text size\n",
    "    theme(legend.key.size = unit(7,\"pt\"))  #legend text size\n",
    "  list_of_plots[[i]] <- graphy #store plot in ith position of plot list\n",
    "} \n",
    "    \n",
    "cm_table <- tableGrob(confusion,  #create a table grob with the training confusion matrix in order to add to the RF Importance plot\n",
    "                      theme = ttheme_default(base_size = 8, padding = unit(c(2,2), \"mm\"))) #default theme, set text size and white space within cells\n",
    "cm_table <- gtable_add_grob(cm_table, #set location of border lines of table \n",
    "                            grobs = rectGrob(gp = gpar(fill = NA, lwd = 1.3)), #lwd is width of line, thinner line than outside border\n",
    "                            t=2, b=nrow(cm_table), l=1, r = ncol(cm_table))    #top of rectangle is under the header line, other lines are full border\n",
    "cm_table <- gtable_add_grob(cm_table, #set location of border lines of table \n",
    "                            grobs = rectGrob(gp = gpar(fill = NA, lwd = 1.3)), #lwd is width of line, thinner line than outside border\n",
    "                            t=1, b=nrow(cm_table), l=2, r = ncol(cm_table))    #left of rectangle is after row names column\n",
    "cm_table <- gtable_add_grob(cm_table, #set location of border lines of table \n",
    "                            grobs = rectGrob(gp = gpar(fill = NA, lwd = 2)),   #thicker line width\n",
    "                            t=1, b=nrow(cm_table), l=1,r=ncol(cm_table))       #outside edge of table\n",
    "h <- grobHeight(cm_table) #store table height\n",
    "w <- grobWidth(cm_table)  #store table width\n",
    "\n",
    "cm_title <- textGrob(table_title, y = unit(0.5, \"npc\") + 1.1*h, vjust = -0.5, hjust = 0.5, gp = gpar(fontsize = 10)) #create a textGrob for table title, set location based on height/width of table\n",
    "cm <- gTree(children = gList(cm_table, cm_title)) #combine table and title into one unit\n",
    "    \n",
    "confusion_test<-signif(confusion_test,2) #round confusion matrix for test set to 2 significant figures  \n",
    "   \n",
    "cm_table_test <- tableGrob(confusion_test, #create a table grob with the training confusion matrix in order to add to the RF Importance plot\n",
    "                      theme = ttheme_default(base_size = 8, padding = unit(c(2,2), \"mm\"))) #default theme, set text size and white space within cells\n",
    "cm_table_test <- gtable_add_grob(cm_table_test, #set location of border lines of table\n",
    "                            grobs = rectGrob(gp = gpar(fill = NA, lwd = 1.3)), #lwd is width of line, thinner line than outside border\n",
    "                            t=2, b=nrow(cm_table_test), l=1, r = ncol(cm_table_test)) #top of rectangle is under the header line, other lines are full border\n",
    "cm_table_test <- gtable_add_grob(cm_table_test, #set location of border lines of table \n",
    "                            grobs = rectGrob(gp = gpar(fill = NA, lwd = 1.3)), #lwd is width of line, thinner line than outside border\n",
    "                            t=1, b=nrow(cm_table_test), l=2, r = ncol(cm_table_test)) #left of rectangle is after row names column\n",
    "cm_table_test <- gtable_add_grob(cm_table_test, #set location of border lines of table \n",
    "                            grobs = rectGrob(gp = gpar(fill = NA, lwd = 2)),   #thicker line width\n",
    "                            t=1, b=nrow(cm_table_test), l=1,r=ncol(cm_table_test))    #outside edge of table \n",
    "h_test <- grobHeight(cm_table_test) #store table height\n",
    "w_test <- grobWidth(cm_table_test)  #store table width\n",
    "\n",
    "cm_title_test <- textGrob(table_title_test, y = unit(0.5, \"npc\") + 1.1*h_test, vjust = -0.5, hjust = 0.5, gp = gpar(fontsize = 10))#create a textGrob for table title, set location based on height/width of table\n",
    "cm_test <- gTree(children = gList(cm_table_test, cm_title_test)) #combine table and title into one unit\n",
    "\n",
    "ggexport(ggarrange(plotlist = list_of_plots, ncol = 3, nrow = 2), #export and arrange the 6 most important variable density plots and the \n",
    "         rf_plot + #RF importance plot\n",
    "        annotation_custom(cm, xmin = imp$varnames[min2], xmax = imp$varnames[max2], ymin = min, ymax = max) + #annotate rf_plot with training confusion matrix at previously specified location\n",
    "         annotation_custom(cm_test, xmin = imp$varnames[min2_test], xmax = imp$varnames[max2_test], ymin = min, ymax = max), #annotate rf_plot with test confusion matrix at previously specified location\n",
    "         nrow = 2,filename = rf_plot_name_combined_test) #export\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "- Sameday\n",
    "- Use held out card as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(sameday && random_forest && test_set){ #prints to excel\n",
    "    \n",
    "if(nchar(as.character(my_data_sameday_byweek[1,1]))==1){      #if number of character in first index of label vector is one\n",
    "my_data_sameday_byweek[,1] <- paste0(my_data_sameday_byweek[,1], \"-\", type) #add \"-type\" information\n",
    "}    \n",
    "    \n",
    "rf_test_results <- matrix(nrow = nrow(my_data_sameday_byweek), ncol = 6)    #create matrix for storing test set results\n",
    "colnames(rf_test_results) <- c(\"2\",\"3\",\"4\",\"Assigned\", \"Actual\",\"Max\")       #assign column names of result matrix\n",
    "data_inthirds <- my_data_sameday_byweek[seq(1, nrow(my_data_sameday_byweek), 3),]  #in order to make conclusions based on each triplicate, store every third row    \n",
    "    \n",
    "for (i in 1:weeks) {\n",
    "  start <- (nrow(my_data_sameday_byweek)/weeks*i)-(nrow(my_data_sameday_byweek)/weeks-1) #starting row of each week\n",
    "  finish <- (nrow(my_data_sameday_byweek)/weeks*i)           #ending row of each week\n",
    "  data <- my_data_sameday_byweek[start:finish,]              #isolate just week of interest\n",
    "  data_bycard <- arrange(data, Card)                         #arrange by Card\n",
    "\n",
    "for (p in 1:length(unique(Card))) {                             #for every week\n",
    "    testData <- data_bycard[((p*9)-8):(p*9),5:ncol(data_bycard)]#set each card as a test set\n",
    "    testlbl <- data_bycard[((p*9)-8):(p*9),1]                   #assign labels for test set\n",
    "    trainData <- data_bycard[-(((p*9)-8):(p*9)),5:ncol(data_bycard)]#assign remaining cards of the week as training set\n",
    "    trainlbl <- data_bycard[-(((p*9)-8):(p*9)),1]               #assign labels for training set\n",
    "   \n",
    "Isomer_rf <- randomForest::randomForest(trainData, y=as.factor(trainlbl), xtest = testData, ytest = as.factor(testlbl), keep.forest = TRUE, classwt = c(rep(1/length(unique(Isomer)),length(unique(Isomer)))), importance = TRUE) #conduct RF analysis, default trees (500), default mtry (sqrt(# of variables)), store \"votes\" for OOB samples as proportion of trees\n",
    "    for (j in 1:nrow(Isomer_rf$test$votes)) { #for each sample\n",
    "    row <- ((finish-(nrow(my_data_sameday_byweek))/weeks))+((p*9)-9)+j  #set row for sample results information\n",
    "      rf_test_results[row,1:3] <- Isomer_rf$test$votes[j,1:3]   #store proportion of decision trees\n",
    "      if(length(which(Isomer_rf$test$votes[j,]==max(Isomer_rf$test$votes[j,])))==1){ #if there is one clear most commonly identified isomer\n",
    "        rf_test_results[row,4] <- paste0(which(Isomer_rf$test$votes[j,]==max(Isomer_rf$test$votes[j,]))+1, \"-\", type) #save assigned isomer as\"-type\"\n",
    "      }\n",
    "      rf_test_results[row,5] <- testlbl[j] #store correct isomer ID\n",
    "      rf_test_results[row,6] <- max(Isomer_rf$test$votes[j,]) #store higher proportion of trees\n",
    "    }\n",
    "    \n",
    "  }\n",
    "}\n",
    "\n",
    "    \n",
    "rf_test_results <- as.data.frame(rf_test_results)      #convert to dataframe\n",
    "rf_test_results[,6] <- as.numeric(rf_test_results[,6]) #store maximum proportion as numeric\n",
    "rf_result_inthirds <- rf_test_results[seq(1,nrow(rf_test_results),3),5]    #store correct isomer ID per triplicate\n",
    "    \n",
    "threshold <- 0.50 #set starting proportion of classifying trees threshold\n",
    "confusion <- matrix(NA, nrow = 15, ncol = 10) #create empty matrix for results\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Average: All 3 match\", \"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Average: 2 match\", \"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Max per card\", \"Success rate\", \"Inconclusive rate\", \"Error rate\") #set row names for confusion matrix\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #set columns names of threshold levels\n",
    "\n",
    "for(t in 1:ncol(confusion)){ #for every threshold\n",
    "#each spectrum individually\n",
    "          threshold_result <- matrix(0,nrow = nrow(rf_test_results), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(rf_test_results)){ #for every sample\n",
    "                 if(rf_test_results[z,6] < threshold){ #if the max proportion of classifying trees is under the threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, 1 in column 2\n",
    "                } else {#if proportion is over threshold\n",
    "                     threshold_result[z,1] <- rf_test_results[z,4] #store classification in column 1\n",
    "                }\n",
    "              }\n",
    "confusion[1,t] <- mean(threshold_result[,1]==rf_test_results[,5]) #success rate\n",
    "confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){  #if there are no inconclusives\n",
    "    confusion[3,t] <- mean(threshold_result[,1]!=rf_test_results[,5]) #error rate is mean of assigned class not equal to correct class\n",
    "    } else {\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[3,t] <- sum(threshold_result[-check,1]!=rf_test_results[-check,5])/nrow(rf_test_results) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "\n",
    "#average, all 3 match\n",
    "threshold_result <- matrix(0,nrow = nrow(rf_test_results)/3, ncol = 2)  #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2   #beginning of triplicate\n",
    "    finish <- (3*i)    #end of triplicate\n",
    "   if(length(unique(rf_test_results[start:finish,4])) > 1) { #test if more than 1 compound was ID'd (all 3 don't match)\n",
    "       threshold_result[i,2] <-1 #result is inconclusive, 1 in column 2\n",
    "    } else { #if all three give the same conclusion\n",
    "       if(mean(rf_test_results[start:finish,6]) < threshold){ #test if the average of all three is under the threshold\n",
    "        threshold_result[i,2] <-1#result is inconclusive, 1 in column 2 \n",
    "       } else { #if average is over the threshold\n",
    "        threshold_result[i,1] <- rf_test_results[start,4] #store first result of triplicate (since they all match) in matrix\n",
    "       }\n",
    "   }\n",
    "}\n",
    "     \n",
    "confusion[5,t] <- mean(threshold_result[,1]==rf_result_inthirds) #success rate, check how often the classified result matches the true result of the triplicate\n",
    "confusion[6,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[7,t] <- mean(threshold_result[,1]!=rf_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusive\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[7,t] <- sum(threshold_result[-check,1]!=rf_result_inthirds[-check])/length(rf_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        } \n",
    "    \n",
    "    \n",
    "#average, 2-3 match\n",
    "threshold_result <- matrix(0,nrow = nrow(rf_test_results)/3, ncol = 2) #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2   #beginning of triplicate\n",
    "    finish <- (3*i)    #end of triplicate\n",
    "   if(length(unique(rf_test_results[start:finish,4])) > 2) { #test if all samples in the triplicate gave different results\n",
    "       threshold_result[i,2] <-1 #result is inconclusive, 1 in column 2\n",
    "    } else {\n",
    "       check <-   as.data.frame(rf_test_results[start:finish,]) %>%  #count how many unique responses are in the triplicate (2 or less) and put the most common at the top \n",
    "                  add_count(Assigned, sort = TRUE) \n",
    "       selected_rows <- which(rf_test_results[start:finish,4]==check[1,4]) #save the ID of the isomer identified at least twice\n",
    "       average_vector <- vector(length = length(selected_rows)) #make a vector to save the proportion of trees for whichever samples in the triplicate were Id'd most frequently\n",
    "       for(j in 1:length(selected_rows)){ #for however many samples in the triplicate match the most common ID\n",
    "           average_vector[j] <- rf_test_results[start-1+selected_rows[j],6] #save the posterior probability in the average vector\n",
    "       }\n",
    "       \n",
    "       if(mean(average_vector) < threshold){ #if the average of the samples that match the most common ID is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the average meets or exceeds the threshold\n",
    "        threshold_result[i,1] <- rf_test_results[(start-1+selected_rows[1]),5]#save the result of the most common ID\n",
    "       }\n",
    "   }\n",
    "}\n",
    "    \n",
    "confusion[9,t] <- mean(threshold_result[,1]==rf_result_inthirds) #success rate\n",
    "confusion[10,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[11,t] <- mean(threshold_result[,1]!=rf_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else {#if there are inconclusive\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[11,t] <- sum(threshold_result[-check,1]!=rf_result_inthirds[-check])/length(rf_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }\n",
    "    \n",
    "#Max of three\n",
    "threshold_result <- matrix(0,nrow = nrow(rf_test_results)/3, ncol = 2) #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(card*3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2   #beginning of triplicate\n",
    "    finish <- (3*i)    #end of triplicate\n",
    "       if(max(rf_test_results[start:finish,6]) < threshold){ #If the highest proportion of trees of the triplicate is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the max meets or exceeds the threshold\n",
    "        check <- which.max(rf_test_results[start:finish,6]) #determine which sample of the triplicate is highest\n",
    "        threshold_result[i,1] <- rf_test_results[(start-1+selected_rows[1]),5]#save the result of the ID with the highest proportion of trees\n",
    "       }\n",
    "   }\n",
    "    \n",
    "confusion[13,t] <- mean(threshold_result[,1]==rf_result_inthirds) #success rate\n",
    "confusion[14,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[15,t] <- mean(threshold_result[,1]!=rf_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else {#if there are inconclusive\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[15,t] <- sum(threshold_result[-check,1]!=rf_result_inthirds[-check])/length(rf_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }    \n",
    "        \n",
    "threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "}\n",
    "\n",
    "confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    "    if(compare_plot){ #if creating comparison plot\n",
    "      compare_plot_data[61:70, 4] <- confusion[1,]   #add success rate to compare plot data\n",
    "      compare_plot_data[71:80, 4] <- confusion[2,]   #add inconclusive rate to compare plot data\n",
    "      compare_plot_data[81:90, 4] <- confusion[3,]   #add error rate to compare plot data\n",
    "    }\n",
    "print(confusion) #print confusion matrix\n",
    "    \n",
    "name1 <- paste0(\"Sameday, test set,\", ion_threshold, \" \", type, \" \",normal ) #assign sheet name\n",
    "write.xlsx(confusion, rf_sameday_file, sheetName = name1, append = TRUE, showNA = FALSE)\n",
    "}   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test old FMA dataset against full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(test_old_data & type == \"FMA\" & random_forest & !test_set & !sameday){  #if set to test second exploratory FMA data (RF and FMA must be set)\n",
    "    colnames(old_data) <- str_replace(colnames(old_data),\"X.\", \"\") #remove X. from Colnames (intriduced by < in header)\n",
    "    old_data[is.na(old_data)] <- 0 #replace any NAs with 0\n",
    "    old_data_30 <- old_data[which(old_data[,2]==30),] #isolate based on voltage\n",
    "    old_data_60 <- old_data[which(old_data[,2]==60),] #isolate based on voltage\n",
    "    old_data_90 <- old_data[which(old_data[,2]==90),] #isolate based on voltage\n",
    "\n",
    "    old_data_30_final <- old_data_30[,which(colnames(old_data) %in% colnames(isomer_30_final))]  #find bins which match final 30V ions\n",
    "    old_data_60_final <- old_data_60[,which(colnames(old_data) %in% colnames(isomer_60_final))]  #find bins which match final 60V ions\n",
    "    old_data_90_final <- old_data_90[,which(colnames(old_data) %in% colnames(isomer_90_final))]  #find bins which match final 90V ions\n",
    "\n",
    "    all_old_data <- cbind(old_data_30_final[,-1], old_data_60_final[,-(1:2)], old_data_90_final[,-(1:2)]) #combine into final old data dataset with label columns\n",
    "\n",
    "if(length(unique(colnames(all_old_data))) < n_30+n_60+n_90+1){ #if there are duplicate column names (voltage info, not yet added)\n",
    "for (i in 1:length(colnames(all_old_data))){#add voltage information to column names\n",
    "    if(i>1 & i<= (1+n_30)){     #if column number is between 1 and (1+ number of 30V variables)-inclusive\n",
    "    colnames(all_old_data)[i] <- paste(colnames(all_old_data)[i],\"30V\")#add \" 30V\" to column name\n",
    "  }\n",
    "  if (i>(n_30+1) & i<= (1+ n_30 + n_60)){#if column number is between (1+ number of 30V variables) and (1+ number of 30 and 60V variables)\n",
    "    colnames(all_old_data)[i] <- paste(colnames(all_old_data)[i],\"60V\")#add \" 60V\" to column name\n",
    "  }\n",
    "  if (i> (1 + n_30 + n_60)){   #if column is greater than 1+ number of 30 and 60V variables\n",
    "    colnames(all_old_data)[i] <- paste(colnames(all_old_data)[i],\"90V\")#add \" 90V\" to column name\n",
    "      }\n",
    "    }\n",
    "}\n",
    "    \n",
    "all_old_data[all_old_data==0] <- replace_zero   #replace zero with centroiding threshold from instrument \n",
    "print(colnames(all_old_data) == colnames(all_isomers)) #check that variables match\n",
    "    \n",
    "all_old_data <- arrange(all_old_data, Label) #arrange by label  \n",
    "my_olddata <- all_old_data[,-1]              #all data except labels\n",
    "Isomer_old <- all_old_data[,1]    #labels\n",
    "my_olddata_normal <- normalize(my_olddata, normal) #normalization\n",
    "    \n",
    "if(nchar(as.character(my_data_test[1,1]))==1){    #if label vector entries are only one character   \n",
    "my_data_test[,1] <- paste0(as.numeric(my_data_test[,1]), \"-\", type) #add \"-type\"\n",
    "}\n",
    "    \n",
    "if(nchar(as.character(Isomer_old[1]))==1){    #if label vector entries are only one character   \n",
    "Isomer_old <- paste0(as.numeric(Isomer_old), \"-\", type) #add \"-type\"\n",
    "}\n",
    "\n",
    "Isomer_rf_old <- randomForest(my_data_test[,-1], y=as.factor(my_data_test[,1]), xtest = my_olddata_normal, ytest = as.factor(Isomer_old), keep.forest = TRUE, classwt = c(rep(1/length(unique(Isomer)),length(unique(Isomer)))), importance = TRUE) #perform RF on full dataset, using old data as test set\n",
    "rf_test_results <- matrix(nrow = nrow(all_old_data), ncol = 6)    #create matrix for storing test set results\n",
    "colnames(rf_test_results) <- c(\"2\",\"3\",\"4\",\"Assigned\", \"Actual\",\"Max\")       #assign column names of result matrix\n",
    "                       \n",
    "for (j in 1:nrow(Isomer_rf_old$test$votes)) {#for each old FMA sample\n",
    "      rf_test_results[j,1:3] <- Isomer_rf_old$test$votes[j,1:3]   #store proportion of decision trees\n",
    "      if(length(which(Isomer_rf_old$test$votes[j,]==max(Isomer_rf_old$test$votes[j,])))==1){ #if there is one clear most commonly identified isomer\n",
    "        rf_test_results[j,4] <- paste0(which(Isomer_rf_old$test$votes[j,]==max(Isomer_rf_old$test$votes[j,]))+1, \"-\", type) #save assigned isomer as\"-type\"\n",
    "      }\n",
    "      rf_test_results[j,5] <- Isomer_old[j] #store correct isomer ID\n",
    "      rf_test_results[j,6] <- max(Isomer_rf_old$test$votes[j,]) #store higher proportion of trees\n",
    "    }\n",
    "   \n",
    "rf_test_results <- as.data.frame(rf_test_results)      #convert to dataframe\n",
    "rf_test_results[,6] <- as.numeric(rf_test_results[,6]) #store maximum proportion as numeric\n",
    "rf_result_inthirds <- rf_test_results[seq(1,nrow(rf_test_results),3),5]    #store correct isomer ID per triplicate\n",
    "  \n",
    "threshold <- 0.50 #set starting proportion of classifying trees threshold\n",
    "confusion <- matrix(NA, nrow = 15, ncol = 10) #create empty matrix for results\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Average: All 3 match\", \"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Average: 2 match\", \"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Max per card\", \"Success rate\", \"Inconclusive rate\", \"Error rate\") #set row names for confusion matrix\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #set columns names of threshold levels\n",
    "\n",
    "for(t in 1:ncol(confusion)){ #for every threshold\n",
    "#each spectrum individually\n",
    "          threshold_result <- matrix(0,nrow = nrow(rf_test_results), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(rf_test_results)){ #for every sample\n",
    "                 if(rf_test_results[z,6] < threshold){ #if the max proportion of classifying trees is under the threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, 1 in column 2\n",
    "                } else {#if proportion is over threshold\n",
    "                     threshold_result[z,1] <- rf_test_results[z,4] #store classification in column 1\n",
    "                }\n",
    "              }\n",
    "confusion[1,t] <- mean(threshold_result[,1]==rf_test_results[,5]) #success rate\n",
    "confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){  #if there are no inconclusives\n",
    "    confusion[3,t] <- mean(threshold_result[,1]!=rf_test_results[,5]) #error rate is mean of assigned class not equal to correct class\n",
    "    } else {\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[3,t] <- sum(threshold_result[-check,1]!=rf_test_results[-check,5])/nrow(rf_test_results) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "\n",
    "#average, all 3 match\n",
    "threshold_result <- matrix(0,nrow = nrow(rf_test_results)/3, ncol = 2)  #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(nrow(rf_test_results)/3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2   #beginning of triplicate\n",
    "    finish <- (3*i)    #end of triplicate\n",
    "   if(length(unique(rf_test_results[start:finish,4])) > 1) { #test if more than 1 compound was ID'd (all 3 don't match)\n",
    "       threshold_result[i,2] <-1 #result is inconclusive, 1 in column 2\n",
    "    } else { #if all three give the same conclusion\n",
    "       if(mean(rf_test_results[start:finish,6]) < threshold){ #test if the average of all three is under the threshold\n",
    "        threshold_result[i,2] <-1#result is inconclusive, 1 in column 2 \n",
    "       } else { #if average is over the threshold\n",
    "        threshold_result[i,1] <- rf_test_results[start,4] #store first result of triplicate (since they all match) in matrix\n",
    "       }\n",
    "   }\n",
    "}\n",
    "     \n",
    "confusion[5,t] <- mean(threshold_result[,1]==rf_result_inthirds) #success rate, check how often the classified result matches the true result of the triplicate\n",
    "confusion[6,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[7,t] <- mean(threshold_result[,1]!=rf_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else { #if there are inconclusive\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[7,t] <- sum(threshold_result[-check,1]!=rf_result_inthirds[-check])/length(rf_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        } \n",
    "    \n",
    "    \n",
    "#average, 2-3 match\n",
    "threshold_result <- matrix(0,nrow = nrow(rf_test_results)/3, ncol = 2) #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(nrow(rf_test_results)/3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2   #beginning of triplicate\n",
    "    finish <- (3*i)    #end of triplicate\n",
    "   if(length(unique(rf_test_results[start:finish,4])) > 2) { #test if all samples in the triplicate gave different results\n",
    "       threshold_result[i,2] <-1 #result is inconclusive, 1 in column 2\n",
    "    } else {\n",
    "       check <-   as.data.frame(rf_test_results[start:finish,]) %>%  #count how many unique responses are in the triplicate (2 or less) and put the most common at the top \n",
    "                  add_count(Assigned, sort = TRUE) \n",
    "       selected_rows <- which(rf_test_results[start:finish,4]==check[1,4]) #save the ID of the isomer identified at least twice\n",
    "       average_vector <- vector(length = length(selected_rows)) #make a vector to save the proportion of trees for whichever samples in the triplicate were Id'd most frequently\n",
    "       for(j in 1:length(selected_rows)){ #for however many samples in the triplicate match the most common ID\n",
    "           average_vector[j] <- rf_test_results[start-1+selected_rows[j],6] #save the posterior probability in the average vector\n",
    "       }\n",
    "       if(mean(average_vector) < threshold){ #if the average of the samples that match the most common ID is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the average meets or exceeds the threshold\n",
    "        threshold_result[i,1] <- rf_test_results[(start-1+selected_rows[1]),5]#save the result of the most common ID\n",
    "       }\n",
    "   }\n",
    "}\n",
    "    \n",
    "confusion[9,t] <- mean(threshold_result[,1]==rf_result_inthirds) #success rate\n",
    "confusion[10,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[11,t] <- mean(threshold_result[,1]!=rf_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else {#if there are inconclusive\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[11,t] <- sum(threshold_result[-check,1]!=rf_result_inthirds[-check])/length(rf_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }\n",
    "    \n",
    "#Max of three\n",
    "threshold_result <- matrix(0,nrow = nrow(rf_test_results)/3, ncol = 2) #create threshold result matrix, 1/3 as many rows (1 conclusion per isomer per card)\n",
    "for (i in 1:(nrow(rf_test_results)/3)){ #for each set of triplicate samples\n",
    "    start <- (3*i)-2   #beginning of triplicate\n",
    "    finish <- (3*i)    #end of triplicate\n",
    "       if(max(rf_test_results[start:finish,6]) < threshold){ #If the highest proportion of trees of the triplicate is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the max meets or exceeds the threshold\n",
    "        check <- which.max(rf_test_results[start:finish,6]) #determine which sample of the triplicate is highest\n",
    "        threshold_result[i,1] <- rf_test_results[(start-1+selected_rows[1]),5]#save the result of the ID with the highest proportion of trees\n",
    "       }\n",
    "   }\n",
    "    \n",
    "confusion[13,t] <- mean(threshold_result[,1]==rf_result_inthirds) #success rate\n",
    "confusion[14,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[15,t] <- mean(threshold_result[,1]!=rf_result_inthirds) #error rate is mean of column 1 not equal to label vector\n",
    "    } else {#if there are inconclusive\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[15,t] <- sum(threshold_result[-check,1]!=rf_result_inthirds[-check])/length(rf_result_inthirds) #count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }    \n",
    "        \n",
    "threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "}\n",
    "\n",
    "confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    "print(confusion) #print confusion matrix\n",
    "    \n",
    "name1 <- paste0(ion_threshold, \", \", type, \", \", normal, \", Old data RF\") #assign sheet name\n",
    "write.xlsx(confusion, rf_olddata, sheetName = name1, append = TRUE, showNA = FALSE )     #save as excel file                   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test old old FMA data against full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(test_oldold_data & type == \"FMA\" & random_forest & !test_set & !sameday & normal != \"vectorlength\"){ #if set to test first exploratory FMA data (RF and FMA must be set)\n",
    "    colnames(old_old_data) <- str_replace(colnames(old_old_data),\"X.\", \"\") #remove X. from Colnames (introduced by < in header)\n",
    "    old_old_data[is.na(old_old_data)] <- 0 #replace any NAs with 0\n",
    "    old_data_30 <- old_old_data[which(old_old_data[,2]==30),] #isolate based on voltage\n",
    "    old_data_60 <- old_old_data[which(old_old_data[,2]==60),] #isolate based on voltage\n",
    "    old_data_90 <- old_old_data[which(old_old_data[,2]==90),] #isolate based on voltage\n",
    "\n",
    "    old_data_30_final <- old_data_30[,which(colnames(old_old_data) %in% colnames(isomer_30_final))]   #find bins which match final 30V ions\n",
    "    old_data_60_final <- old_data_60[,which(colnames(old_old_data) %in% colnames(isomer_60_final))]   #find bins which match final 30V ions\n",
    "    old_data_90_final <- old_data_90[,which(colnames(old_old_data) %in% colnames(isomer_90_final))]   #find bins which match final 30V ions\n",
    "\n",
    "    all_old_data <- cbind(old_data_30_final[,-1], old_data_60_final[,-(1:2)], old_data_90_final[,-(1:2)]) #combine into final old data dataset with label columns\n",
    "\n",
    "\n",
    "if(length(unique(colnames(all_old_data))) < n_30+n_60+n_90+1){ #if there are duplicate column names (voltage info, not yet added)\n",
    "for (i in 1:length(colnames(all_old_data))){#add voltage information to column names\n",
    "    if(i>1 & i<= (1+n_30)){     #if column number is between 1 and (1+ number of 30V variables)-inclusive\n",
    "    colnames(all_old_data)[i] <- paste(colnames(all_old_data)[i],\"30V\")#add \" 30V\" to column name\n",
    "  }\n",
    "  if (i>(n_30+1) & i<= (1+ n_30 + n_60)){#if column number is between (1+ number of 30V variables) and (1+ number of 30 and 60V variables)\n",
    "    colnames(all_old_data)[i] <- paste(colnames(all_old_data)[i],\"60V\")#add \" 60V\" to column name\n",
    "  }\n",
    "  if (i> (1 + n_30 + n_60)){   #if column is greater than 1+ number of 30 and 60V variables\n",
    "    colnames(all_old_data)[i] <- paste(colnames(all_old_data)[i],\"90V\")#add \" 90V\" to column name\n",
    "      }\n",
    "    }\n",
    "}\n",
    "    \n",
    "all_old_data[all_old_data==0] <- replace_zero   #replace zero with centroiding threshold from instrument \n",
    "print(colnames(all_old_data) == colnames(all_isomers)) #check that variables match\n",
    "    \n",
    "all_old_data <- arrange(all_old_data, Label) #arrange by label  \n",
    "my_olddata <- all_old_data[,-1]              #all data except labels\n",
    "Isomer_old <- all_old_data[,1]               #labels\n",
    "my_olddata_normal <- normalize(my_olddata, normal) #normalization\n",
    "    \n",
    "if(nchar(as.character(my_data_test[1,1]))==1){    #if label vector entries are only one character   \n",
    "my_data_test[,1] <- paste0(as.numeric(my_data_test[,1]), \"-\", type) #add \"-type\"\n",
    "}\n",
    "    \n",
    "if(nchar(as.character(Isomer_old[1]))==1){    #if label vector entries are only one character   \n",
    "Isomer_old <- paste0(as.numeric(Isomer_old), \"-\", type) #add \"-type\"\n",
    "}\n",
    " \n",
    "Isomer_rf_old <- randomForest(my_data_test[,-1], y=as.factor(my_data_test[,1]), xtest = my_olddata_normal, ytest = as.factor(Isomer_old), keep.forest = TRUE, classwt = c(rep(1/length(unique(Isomer)),length(unique(Isomer)))), importance = TRUE) #perform RF on full dataset, using old data as test set\n",
    "rf_test_results <- matrix(nrow = nrow(all_old_data), ncol = 6)    #create matrix for storing test set results\n",
    "colnames(rf_test_results) <- c(\"2\",\"3\",\"4\",\"Assigned\", \"Actual\",\"Max\")       #assign column names of result matrix\n",
    "                     \n",
    "for (j in 1:nrow(Isomer_rf_old$test$votes)) {#for each old FMA sample\n",
    "      rf_test_results[j,1:3] <- Isomer_rf_old$test$votes[j,1:3]   #store proportion of decision trees\n",
    "      if(length(which(Isomer_rf_old$test$votes[j,]==max(Isomer_rf_old$test$votes[j,])))==1){ #if there is one clear most commonly identified isomer\n",
    "        rf_test_results[j,4] <- paste0(which(Isomer_rf_old$test$votes[j,]==max(Isomer_rf_old$test$votes[j,]))+1, \"-\", type) #save assigned isomer as\"-type\"\n",
    "      }\n",
    "      rf_test_results[j,5] <- Isomer_old[j] #store correct isomer ID\n",
    "      rf_test_results[j,6] <- max(Isomer_rf_old$test$votes[j,]) #store higher proportion of trees\n",
    "    }\n",
    "    \n",
    "rf_test_results <- as.data.frame(rf_test_results)      #convert to dataframe\n",
    "rf_test_results[,6] <- as.numeric(rf_test_results[,6]) #store maximum proportion as numeric\n",
    "rf_result_insevenths <- rf_test_results[seq(1,nrow(rf_test_results),7),5]      #original FMA data was seven replicates on one card, so split in sevenths instead of thirds\n",
    "\n",
    "threshold <- 0.50   #set starting proportion of classifying trees threshold\n",
    "confusion <- matrix(NA, nrow = 7, ncol = 10) #create empty matrix for storing results\n",
    "row.names(confusion) <- c(\"Success rate\", \"Inconclusive rate\", \"Error rate\", \"Max per card\", \"Success rate\", \"Inconclusive rate\", \"Error rate\")\n",
    "colnames(confusion) <- c(\"0.50\",\"0.55\",\"0.60\",\"0.65\",\"0.70\",\"0.75\",\"0.80\",\"0.85\",\"0.90\",\"0.95\") #thresholds as column names\n",
    "\n",
    "for(t in 1:ncol(confusion)){ #for every threshold\n",
    "#each spectrum individually\n",
    "          threshold_result <- matrix(0,nrow = nrow(rf_test_results), ncol = 2) #create threshold_result matrix, column 1 is ID if made, column 2 is inconclusive or not\n",
    "              for(z in 1:nrow(rf_test_results)){ #for every sample\n",
    "                 if(rf_test_results[z,6] < threshold){ #if the max proportion of classifying trees is under the threshold\n",
    "                    threshold_result[z,2] <- 1 #result is inconclusive, 1 in column 2\n",
    "                } else {#if proportion is over threshold\n",
    "                     threshold_result[z,1] <- rf_test_results[z,4] #store classification in column 1\n",
    "                }\n",
    "              }\n",
    "confusion[1,t] <- mean(threshold_result[,1]==rf_test_results[,5]) #success rate\n",
    "confusion[2,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){  #if there are no inconclusives\n",
    "    confusion[3,t] <- mean(threshold_result[,1]!=rf_test_results[,5]) #error rate is mean of assigned class not equal to correct class\n",
    "    } else {\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[3,t] <- sum(threshold_result[-check,1]!=rf_test_results[-check,5])/nrow(rf_test_results) #count how many non-inconclusive samples don't match the label vector, divide by total # of samples\n",
    "        }\n",
    "   \n",
    "#Max of seven\n",
    "threshold_result <- matrix(0,nrow = nrow(rf_test_results)/7, ncol = 2) #create threshold matrix (1 per card)\n",
    "for (i in 1: (nrow(rf_test_results)/7)){ #for each card of data\n",
    "    start <- (7*i)-6 #beginning of card\n",
    "    finish <- (7*i)  #end of card\n",
    "       if(max(rf_test_results[start:finish,6]) < threshold){ #If the highest proportion of trees of the triplicate is less than the threshold\n",
    "        threshold_result[i,2] <-1  #result is inconclusive, 1 in column 2\n",
    "       } else { #if the max meets or exceeds the threshold\n",
    "        check <- which.max(rf_test_results[start:finish,6]) #determine which sample of the triplicate is highest\n",
    "        threshold_result[i,1] <- rf_test_results[(start-1+selected_rows[1]),4]#save the result of the ID with the highest proportion of trees\n",
    "       }\n",
    "   }\n",
    "\n",
    "confusion[5,t] <- mean(threshold_result[,1]==rf_result_insevenths) #success rate\n",
    "confusion[6,t] <- mean(as.numeric(threshold_result[,2])) #inconclusive rate\n",
    "\n",
    "if(sum(as.numeric(threshold_result[,2]))==0){ #if there are no inconclusives\n",
    "    confusion[7,t] <- mean(threshold_result[,1]!=rf_result_insevenths) #error rate is mean of column 1 not equal to label vector\n",
    "    } else {#if there are inconclusive\n",
    "    check <- which(threshold_result[,2]==1) #ID inconclusives to be removed from error rate\n",
    "    confusion[7,t] <- sum(threshold_result[-check,1]!=rf_result_insevenths[-check])/length(rf_result_insevenths)#count how many non-inconclusive results don't match the label vector, divide by total # of triplicates\n",
    "        }    \n",
    "    \n",
    "        \n",
    "threshold <- threshold + 0.05 #increase threshold by 0.05\n",
    "}\n",
    "\n",
    "confusion<-signif(confusion,3) #round confusion matrix to 3 significant figures\n",
    "print(confusion) #print confusion matrix\n",
    "    \n",
    "name1 <- paste0(ion_threshold, \", \", type, \", \", normal, \", Old old data RF\") #assign sheet name\n",
    "write.xlsx(confusion, rf_oldold_data, sheetName = name1, append = TRUE, showNA = FALSE )  #save as excel file                      \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Heat Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(correlation){ #plot correlation heat maps\n",
    "\n",
    "list_of_corr_plots <- vector(\"list\", 4) #create empty list of plots\n",
    "font_size <- (1/ncol(my_data))*50       #appropriate font size depends on # variables\n",
    "cormat <- round(cor(my_data_test[,-1]),2) #create correlation matrix\n",
    "    \n",
    "get_upper_tri <- function(cormat){   #get upper triangle of the correlation matrix\n",
    "    cormat[lower.tri(cormat)] <- NA   #put NAs in \n",
    "    return(cormat)\n",
    "}\n",
    "    \n",
    "upper_tri <- get_upper_tri(cormat)   #get upper triangle of the correlation matrix\n",
    "melted_cormat <- melt(upper_tri, na.rm = TRUE) #create melted correlation matrix for easy plotting, remove NAs\n",
    "title <- paste0(ion_threshold, \" \", type, \" \", normal, \" Correlation Heat Map, All Isomers\") #assign plot title\n",
    "\n",
    "corplot <- ggplot(data = melted_cormat, aes(x = Var2, y = Var1, fill = value)) +   #plot correlation heatmap\n",
    "    geom_tile(color = \"white\") + #white background\n",
    "    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1,1), space = \"Lab\", name = \"Pearson\\nCorrelation\") + #set color scale for heatmap \n",
    "theme_minimal() + #set theme to minimal\n",
    "labs(title = title) + #set plot title\n",
    "theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1)) + #make x-axis font angled\n",
    "coord_fixed() + #ensures that one unit on the x-axis is the same length as one unit on the y-axis\n",
    "geom_text(aes(Var2, Var1, label = value), color = \"black\", size = font_size) + #add text of correlation coefficients to tiles\n",
    "theme( #add additional theme elements\n",
    "  axis.title.x = element_blank(), #no axis title space\n",
    "  axis.title.y = element_blank(), #no axis title space\n",
    "  panel.grid.major = element_blank(), #no major panel grid\n",
    "  panel.border = element_blank(),     #no panel border\n",
    "  panel.background = element_blank(), #no panel background\n",
    "  axis.ticks = element_blank(),       #no axis ticks\n",
    "  legend.justification = c(1, 0),     #set justification for legend\n",
    "  legend.position = c(0.6, 0.7),      #set legend location\n",
    "  legend.direction = \"horizontal\")+   #make legend horizontal\n",
    "  guides(fill = guide_colorbar(barwidth = 7, barheight = 1, #adjust legend bar width and height\n",
    "                title.position = \"top\", title.hjust = 0.5))\n",
    "    \n",
    "ggexport(corplot, filename = corr_file_full) #export plot alone\n",
    "\n",
    "list_of_corr_plots[[1]] <- corplot #add to list of plots\n",
    "\n",
    "#By isomer\n",
    "if(nchar(as.character(my_data_test[1,1]))>1){ #remove\"-type\" information if present\n",
    "    my_data_test[,1] <- substr(my_data_test[,1],1,1)\n",
    "}\n",
    "\n",
    "cormat_23 <- get_upper_tri(round(cor(my_data_test[my_data_test[,1]==2,-1], my_data_test[my_data_test[,1]==3,-1]),2)) #get upper triangle of correlation matrix between Isomers 2 and 3\n",
    "\n",
    "melted_cormat <- melt(upper_tri, na.rm = TRUE)  #create melted correlation matrix for easy plotting, remove NAs\n",
    "title <- paste0(ion_threshold, \" \", type, \" \", normal, \" Correlation Heat Map, Isomer 2 vs Isomer 3\") #assign plot title\n",
    "\n",
    "corplot23 <- ggplot(data = melted_cormat, aes(x = Var2, y = Var1, fill = value)) +  #plot correlation heatmap for Isomer 2 vs Isomer 3\n",
    "    geom_tile(color = \"white\") + #white background\n",
    "    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1,1), space = \"Lab\", name = \"Pearson\\nCorrelation\") + #set color scale for heatmap \n",
    "theme_minimal() + #set theme to minimal\n",
    "labs(title = title) + #set plot title\n",
    "theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1)) + #make x-axis font angled\n",
    "coord_fixed() + #ensures that one unit on the x-axis is the same length as one unit on the y-axis\n",
    "geom_text(aes(Var2, Var1, label = value), color = \"black\", size = font_size) + #add text of correlation coefficients to tiles\n",
    "theme( #add additional theme elements\n",
    "  axis.title.x = element_blank(), #no axis title space\n",
    "  axis.title.y = element_blank(), #no axis title space\n",
    "  panel.grid.major = element_blank(), #no major panel grid\n",
    "  panel.border = element_blank(),     #no panel border\n",
    "  panel.background = element_blank(), #no panel background\n",
    "  axis.ticks = element_blank(),       #no axis ticks\n",
    "  legend.justification = c(1, 0),     #set justification for legend\n",
    "  legend.position = c(0.6, 0.7),      #set legend location\n",
    "  legend.direction = \"horizontal\")+   #make legend horizontal\n",
    "  guides(fill = guide_colorbar(barwidth = 7, barheight = 1, #adjust legend bar width and height\n",
    "                title.position = \"top\", title.hjust = 0.5))\n",
    "\n",
    "list_of_corr_plots[[2]] <- corplot23 #add to list of plots\n",
    "    \n",
    "#Isomer 3 vs Isomer 4    \n",
    "\n",
    "cormat_34 <- get_upper_tri(round(cor(my_data_test[my_data_test[,1]==3,-1], my_data_test[my_data_test[,1]==4,-1]),2))#get upper triangle of correlation matrix between Isomers 3 and 4\n",
    "\n",
    "melted_cormat <- melt(upper_tri, na.rm = TRUE)  #create melted correlation matrix for easy plotting, remove NAs\n",
    "title <- paste0(ion_threshold, \" \", type, \" \", normal, \" Correlation Heat Map, Isomer 3 vs Isomer 4\") #assign plot title\n",
    "\n",
    "corplot34 <- ggplot(data = melted_cormat, aes(x = Var2, y = Var1, fill = value)) + #plot correlation heatmap\n",
    "    geom_tile(color = \"white\") + #white background\n",
    "    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1,1), space = \"Lab\", name = \"Pearson\\nCorrelation\") + #set color scale for heatmap \n",
    "theme_minimal() + #set theme to minimal\n",
    "labs(title = title) + #set plot title\n",
    "theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1)) + #make x-axis font angled\n",
    "coord_fixed() + #ensures that one unit on the x-axis is the same length as one unit on the y-axis\n",
    "geom_text(aes(Var2, Var1, label = value), color = \"black\", size = font_size) + #add text of correlation coefficients to tiles\n",
    "theme( #add additional theme elements\n",
    "  axis.title.x = element_blank(), #no axis title space\n",
    "  axis.title.y = element_blank(), #no axis title space\n",
    "  panel.grid.major = element_blank(), #no major panel grid\n",
    "  panel.border = element_blank(),     #no panel border\n",
    "  panel.background = element_blank(), #no panel background\n",
    "  axis.ticks = element_blank(),       #no axis ticks\n",
    "  legend.justification = c(1, 0),     #set justification for legend\n",
    "  legend.position = c(0.6, 0.7),      #set legend location\n",
    "  legend.direction = \"horizontal\")+   #make legend horizontal\n",
    "  guides(fill = guide_colorbar(barwidth = 7, barheight = 1, #adjust legend bar width and height\n",
    "                title.position = \"top\", title.hjust = 0.5))\n",
    "\n",
    "list_of_corr_plots[[3]] <- corplot34 #add to list of plots\n",
    "    \n",
    "#Isomer 2 vs Isomer 4\n",
    "\n",
    "cormat_24 <- get_upper_tri(round(cor(my_data_test[my_data_test[,1]==2,-1], my_data_test[my_data_test[,1]==4,-1]),2)) #get upper triangle of correlation matrix between Isomers 2 and 4\n",
    "\n",
    "melted_cormat <- melt(upper_tri, na.rm = TRUE)             #create melted correlation matrix for easy plotting, remove NAs\n",
    "title <- paste0(ion_threshold, \" \", type, \" \", normal, \" Correlation Heat Map, Isomer 2 vs Isomer 4\")\n",
    "\n",
    "corplot24 <- ggplot(data = melted_cormat, aes(x = Var2, y = Var1, fill = value)) +   #plot correlation heatmap\n",
    "    geom_tile(color = \"white\") + #white background\n",
    "    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1,1), space = \"Lab\", name = \"Pearson\\nCorrelation\") + #set color scale for heatmap \n",
    "theme_minimal() + #set theme to minimal\n",
    "labs(title = title) + #set plot title\n",
    "theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1)) + #make x-axis font angled\n",
    "coord_fixed() + #ensures that one unit on the x-axis is the same length as one unit on the y-axis\n",
    "geom_text(aes(Var2, Var1, label = value), color = \"black\", size = font_size) + #add text of correlation coefficients to tiles\n",
    "theme( #add additional theme elements\n",
    "  axis.title.x = element_blank(), #no axis title space\n",
    "  axis.title.y = element_blank(), #no axis title space\n",
    "  panel.grid.major = element_blank(), #no major panel grid\n",
    "  panel.border = element_blank(),     #no panel border\n",
    "  panel.background = element_blank(), #no panel background\n",
    "  axis.ticks = element_blank(),       #no axis ticks\n",
    "  legend.justification = c(1, 0),     #set justification for legend\n",
    "  legend.position = c(0.6, 0.7),      #set legend location\n",
    "  legend.direction = \"horizontal\")+   #make legend horizontal\n",
    "  guides(fill = guide_colorbar(barwidth = 7, barheight = 1, #adjust legend bar width and height\n",
    "                title.position = \"top\", title.hjust = 0.5))\n",
    "\n",
    "list_of_corr_plots[[4]] <- corplot24 #add to list of plots\n",
    "print(list_of_corr_plots) #print list of plots\n",
    "ggexport(plotlist = list_of_corr_plots,filename = corr_file_all) #export list of plots\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(rf_cv) { #if performing random forest cross-validation with decreasing variables\n",
    "result <- rfcv(my_data_test[,-1], as.factor(my_data_test[,1]), cv.fold = 12) #rf cross-validation, 12-fold \n",
    "title <- paste0(ion_threshold, \" \", type, \" \", normal, \" Random Forest Cross-validation (12 Folds)\") #set plots title\n",
    "to_plot <- cbind(\"error.cv\" = result$error.cv, \"n.var\" = result$n.var) #set plot data and names\n",
    "rfcv_plot <- ggplot(data = as.data.frame(to_plot), aes(x = n.var, y = error.cv)) + #plot error rate with changing # of variables\n",
    "geom_point() + #scatterplot\n",
    "geom_line() +  #connected by lines\n",
    "labs(x = \"Number of Variables\", y = \"CV Error Rate\", title = title) + #label axes and title\n",
    "scale_x_log10() + #scale x axis on log scale\n",
    "theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "theme(text = element_text(size = 12)) #text size\n",
    "\n",
    "ggexport(rfcv_plot, filename = rfcv_file) #export plot\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Comparing method variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(compare_methods && !sameday && !test_set){  #if comparing variable importance by method\n",
    "    font_size <- (1/ncol(my_data))*126 #change font size based on number of variables\n",
    "    \n",
    "    list_of_comparison_plots <- vector(\"list\", 4) #create empty list for comparison plots\n",
    "    plot_data <- cbind(c(rep(\"t-test\",ncol(compare_matrix)),rep(\"LD1\",ncol(compare_matrix)),rep(\"LD2\",ncol(compare_matrix)),rep(\"Random_Forest\",ncol(compare_matrix))),rep(colnames(compare_matrix),4),c(compare_matrix[1,], compare_matrix[2,], compare_matrix[3,],compare_matrix[4,])) #make data frame of plotting data including one column of analysis method, and one column with results per method\n",
    "    colnames(plot_data) <- c(\"Method\", \"Variable\", \"Importance\") #column names of plotting data frame\n",
    "    title = paste0(\"Comparison of Variable Importance per method, \", ion_threshold, \" \", type, \" \", normal) #set plot title\n",
    "    \n",
    "comparison_plot <- ggplot(as.data.frame(plot_data), aes(x = Variable, y = as.numeric(Importance), fill = Method)) + #plot Variable vs Importance, filled based on Method (too busy, not used)\n",
    "    geom_bar(stat = \"identity\", position = position_dodge(), width = 0.8, color = \"darkblue\") + #bar grapht, color of outline of bars\n",
    "    scale_fill_brewer(palette = \"Set2\") + #set color palette\n",
    "    labs(title = title, y = \"Importance\") + #label y-axis and title\n",
    "    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + #change font/angle of x-axis\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    theme(text = element_text(size = 10)) +  #text size\n",
    "    theme(legend.key.size = unit(7,\"pt\"))    #legend size\n",
    "\n",
    " \n",
    "title = paste0(\"T-test Average Accuracy\") #assign title\n",
    "    ttest_comparison_plot <- ggplot(as.data.frame(plot_data[which(plot_data[,1]==\"t-test\"),]), aes(x = Variable, y = as.numeric(Importance))) + #plot Variable vs Average Accuracy for t-test\n",
    "    geom_bar(stat = \"identity\", position = position_dodge(), width = 0.8) + #bar graph\n",
    "    coord_cartesian(ylim=c(0.3,1.0)) + #adjust axis to make more comparable to other plots\n",
    "    scale_x_discrete(limits = plot_data[which(plot_data[,1]==\"t-test\"),2]) + #set x-axis scale\n",
    "    labs(title = title, y = \"t-test Average Accuracy\") + #label y-axis and title\n",
    "    theme(axis.text.x = element_text(angle = 45, vjust = 1, size = font_size, hjust = 1)) + #set x-axis labels on angle, set font size\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    theme(text = element_text(size = 9))   #legend size\n",
    "                                                        \n",
    "list_of_comparison_plots[[1]] <- ttest_comparison_plot #put plot in position 1\n",
    "    \n",
    "title = paste0(\"Random Forest Variable Importance\") #assign title\n",
    "rf_comparison_plot <- ggplot(as.data.frame(plot_data[which(plot_data[,1]==\"Random_Forest\"),]), aes(x = Variable, y = as.numeric(Importance))) + #plot Variable vs mean decrease accuracy for Random Forest\n",
    "    geom_bar(stat = \"identity\", position = position_dodge(), width = 0.8) + #bar graph\n",
    "    scale_x_discrete(limits = plot_data[which(plot_data[,1]==\"Random_Forest\"),2]) + #set x-axis scale\n",
    "    labs(title = title, y = \"Mean Decrease Accuracy\") + #label y-axis and title\n",
    "    theme(axis.text.x = element_text(angle = 45, vjust = 1, size = font_size, hjust = 1)) +#set x-axis labels on angle, set font size\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    theme(text = element_text(size = 9))   #legend size \n",
    "                                                                                                            \n",
    "list_of_comparison_plots[[2]] <- rf_comparison_plot  #put plot in position 2\n",
    "    \n",
    "#LD1 plot\n",
    "    title = paste0(\"LD1 Scalings\") #assign title\n",
    "lda_comparison_plot1 <- ggplot(as.data.frame(plot_data[which(plot_data[,1]==\"LD1\"),]), aes(x = Variable, y = as.numeric(Importance))) + #plot Variable vs importance for LD1 function\n",
    "    geom_bar(stat = \"identity\", position = position_dodge(), width = 0.8) + #bar graph\n",
    "    scale_x_discrete(limits = plot_data[which(plot_data[,1]==\"LD1\"),2]) + #set x-axis scale\n",
    "    labs(title = title, y = \"LD1 Scalings\") + #label y-axis and title\n",
    "    theme(axis.text.x = element_text(angle = 45, vjust = 1, size = font_size, hjust = 1)) + #set x-axis labels on angle, set font size\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    theme(text = element_text(size = 9))   #legend size\n",
    "                                                                                                         \n",
    "list_of_comparison_plots[[3]] <- lda_comparison_plot1 #put plot in position 3\n",
    "    \n",
    "    title = paste0(\"LD2 Scalings\")\n",
    "lda_comparison_plot2 <- ggplot(as.data.frame(plot_data[which(plot_data[,1]==\"LD2\"),]), aes(x = Variable, y = as.numeric(Importance))) + #plot Variable vs importance for LD2 function\n",
    "    geom_bar(stat = \"identity\", position = position_dodge(), width = 0.8) + #bar graph\n",
    "    scale_x_discrete(limits = plot_data[which(plot_data[,1]==\"LD2\"),2]) +  #set x-axis scale\n",
    "    labs(title = title, y = \"LD2 Scalings\") + #label y-axis and title\n",
    "    theme(axis.text.x = element_text(angle = 45, vjust = 1, size = font_size, hjust = 1)) +#set x-axis labels on angle, set font size\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    theme(text = element_text(size = 9))   #legend size\n",
    "                                                                                                                \n",
    "list_of_comparison_plots[[4]] <- lda_comparison_plot2 #put plot in position 4\n",
    "    \n",
    "ggexport(plotlist = list_of_comparison_plots, ncol = 2, nrow = 2, filename = comparison_file)    #explort plots in 2x2 grid\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Methods Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(compare_plot) { #if making comparison plot\n",
    "    \n",
    "    compare_plot_data <- as.data.frame(compare_plot_data) #make compare plot data into a data frame\n",
    "    compare_plot_data[,2] <- as.numeric(compare_plot_data[,2])#make Threshold data numeric\n",
    "    compare_plot_data[,4] <- as.numeric(compare_plot_data[,4])#make rate numeric\n",
    "    compare_plot_data[,1] <- factor(compare_plot_data[,1], levels = c(\"t-test\", \"LDA\", \"Random Forest\")) #Make methods into ordered factors\n",
    "    title <- paste0(\"Comparison of Results by Method for \", ion_threshold, \"% \", type) #assign title\n",
    "    if(sameday){ #if same day\n",
    "     title <- paste0(\"Comparison of Results by Method for \", ion_threshold, \"% \", type, \" for Same day analysis\")   #assign title\n",
    "    }\n",
    "    if(test_set){ #if test set\n",
    "     title <- paste0(\"Comparison of Results by Method for \", ion_threshold, \"% \", type, \" for Test Set analysis\")   #assign title\n",
    "    }\n",
    "    if(sameday && test_set){ #if both\n",
    "     title <- paste0(\"Comparison of Results by Method for \", ion_threshold, \"% \", type, \" for Same day/Test Set analysis\")  #assign title \n",
    "    }\n",
    "compare_plot2 <- ggplot() + #create comparison plot\n",
    "    geom_bar(data = compare_plot_data, aes(y= Rate, x = Method, fill = Result), stat = \"identity\", position = 'stack') + #stacked bar graph, separated by method\n",
    "    theme_bw() +\n",
    "    scale_fill_brewer(palette = \"Pastel1\") + #set color palette\n",
    "    labs(title = title) + #label title\n",
    "    theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 8, hjust = 1)) + #make x-axis font angled\n",
    "    theme(panel.background = element_rect(fill = \"white\")) + #white background\n",
    "    theme(plot.background = element_rect(color = \"black\")) + #black border\n",
    "    theme(legend.position=\"bottom\", legend.key.size = unit(8,\"pt\")) + #set legend beneath plot, set text size\n",
    "    facet_grid(~Threshold) #separate grid per threshold\n",
    "}\n",
    "print(compare_plot2)\n",
    "if(!test_set){\n",
    "ggexport(compare_plot2, filename = compare_plot_file) #export plot\n",
    "    }\n",
    "if(test_set){\n",
    "ggexport(compare_plot2, filename = compare_plot_file_test) #export plot\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
